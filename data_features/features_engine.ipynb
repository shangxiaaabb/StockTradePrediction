{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8cf9a-5530-472a-a927-0021efca115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dateutil.parser as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c333fce9-aabe-4132-87cc-63e3659fcc84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter):\n",
    "    '''\n",
    "    从sever中读取一只股票一天的数据\n",
    "    data_home:数据所在folder\n",
    "    data_type:类型\n",
    "    venue:交易所\n",
    "    is_filter:是否进行filter操作\n",
    "    '''\n",
    "\n",
    "    path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "        month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue)  # 读数据的路径\n",
    "\n",
    "    if (os.path.exists(path)):\n",
    "        data0 = open(path, 'r')\n",
    "        data1 = pd.read_csv(StringIO(data0.read()))\n",
    "        data = data1.loc[:, ['time', 'volume', 'current','a1_v','a1_p','b1_v','b1_p',]]\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['current'] = data['current']/10000\n",
    "        data['spread'] = (data['a1_p'] - data['b1_p'])/(data['a1_p'] + data['b1_p'])  # 计算 spread\n",
    "        data['quote_imbalance'] = (data['b1_v'] - data['a1_v']) / (data['b1_v'] + data['a1_v'])\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "    else:\n",
    "        path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "            month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue) + '.gz'\n",
    "        with gzip.open(path, 'rb') as gf:\n",
    "            data1 = pd.read_csv(gf)\n",
    "        data = data1.loc[:, ['time', 'volume','current', 'a1_v','a1_p','b1_v','b1_p',]]\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['current'] = data['current']/10000\n",
    "        data['quote_imbalance'] = (data['b1_v'] - data['a1_v']) / (data['b1_v'] + data['a1_v'])\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "def trans_date(date):  # transform date to year-month-day\n",
    "    dates = []\n",
    "    for i in range(len(date)):\n",
    "        year = str(date[i])[0:4]\n",
    "        month = str(date[i])[4:6]\n",
    "        day = str(date[i])[6:8]\n",
    "        date_std = datetime.date(int(year), int(month), int(day)).isoformat()\n",
    "        dates.append(date_std)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def trans_time(time):  # transform time to hour:minute:second\n",
    "    times = []\n",
    "    for i in range(len(time)):\n",
    "        hour = str(time[i])[8:10]\n",
    "        minute = str(time[i])[10:12]\n",
    "        second = str(time[i])[12:14]\n",
    "        time_std = datetime.time(int(hour), int(minute), int(second)).isoformat()\n",
    "        times.append(time_std)\n",
    "    return times\n",
    "\n",
    "def divide_bin(time, binnumber):  # 计算每条交易所属的bin number\n",
    "    '''\n",
    "    time:columns of time\n",
    "    '''\n",
    "    n = 237 / (binnumber - 1) * 60\n",
    "    bin_nums = []\n",
    "    for i in range(len(time)):\n",
    "        if datetime.datetime.strptime(time[i],\"%H:%M:%S\") < datetime.datetime.strptime(\"09:30:00\",\"%H:%M:%S\"):\n",
    "            bin_num = 0  # 交易发生在9：30之前，bin number为0\n",
    "        elif datetime.datetime.strptime(time[i],\"%H:%M:%S\")>datetime.datetime.strptime(\"15:00:00\",\"%H:%M:%S\"):\n",
    "            bin_num = binnumber+1\n",
    "        else:\n",
    "            starttime = parser.parse(datetime.time(9, 30, 0).isoformat())  # 开始时间设为9：30\n",
    "            endtime = parser.parse(time[i])  # 结束时间是该条数据的交易时间\n",
    "            s = (endtime - starttime).seconds  # 从开盘到现在的秒数\n",
    "            if s > -1 and s < 7201:  # 交易发生在9：30-11：30之前\n",
    "                bin_num = int((s - 0.5) // n) + 1  # 9：30之后的bin number从1开始\n",
    "            elif s > 12599 and s < 19801:  # 13:00-15:00\n",
    "                bin_num = int((s - 0.5 - 5400) // n)+1  # 去掉中间的90分钟\n",
    "            else:\n",
    "                bin_num = binnumber\n",
    "        bin_nums.append(bin_num)\n",
    "    return bin_nums\n",
    "\n",
    "####### volatility imbalance\n",
    "def cal_bin_volume(subdf, binnumber):\n",
    "    '''\n",
    "    subdf: data to be processed, DataFrame\n",
    "    return: DataFrame including one stock, ranked by bin number\n",
    "    '''\n",
    "\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber + 1])]  # Exclude rows with bin number binnumber+1\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber])]  # Exclude rows with bin number binnumber\n",
    "\n",
    "    daily_volume = subdf['volume'].groupby(subdf['date']).sum().reset_index()  # Calculate total volume for each day\n",
    "    bin_volume = subdf['volume'].groupby([subdf['date'], subdf['bin_num']]).sum().reset_index()  # Calculate volume for each bin\n",
    "    \n",
    "    # 计算标准差\n",
    "    \n",
    "    volatility = subdf['current'].groupby([subdf['date'], subdf['bin_num']]).std().reset_index().rename(columns={'current': 'volatility'})\n",
    "    df = pd.merge(daily_volume, bin_volume, how='outer', on='date')  # Merge daily_volume and bin_volume\n",
    "    subdf1 = pd.merge(df, volatility, how='outer', on=['date', 'bin_num'])\n",
    "\n",
    "    def exponential_weighted_average(numbers, alpha):\n",
    "        n = len(numbers)\n",
    "        weights = np.array([alpha ** (n - 1 - i) for i in range(n)])  # 使用倒序的权重计算\n",
    "        weighted_sum = np.sum(np.multiply(numbers, weights))\n",
    "        weight_sum = np.sum(weights)\n",
    "        ewma = weighted_sum / weight_sum\n",
    "        return ewma\n",
    "\n",
    "    \n",
    "    imbalance = subdf['quote_imbalance'].groupby([subdf['date'], subdf['bin_num']]).apply(lambda x: exponential_weighted_average(x, 0.9)).reset_index()\n",
    "    subdf1 = pd.merge(subdf1,imbalance,how='outer',on=['date','bin_num'])\n",
    "    return subdf1\n",
    "\n",
    "\n",
    "def get_df(data_home, data_type, venue, year, month, day,ticker, bin_num, is_filter=0):\n",
    "    stock_data = read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter=0)\n",
    "    stock_data = stock_data.reset_index(drop=True)\n",
    "    transdate = trans_date(stock_data['time'])\n",
    "    transtime = trans_time(stock_data['time'])\n",
    "    stock_data.loc[:, 'date'] = transdate  # replace the original data by transformed data\n",
    "    stock_data.loc[:, 'timet'] = transtime\n",
    "    bin_nums = divide_bin(time=stock_data['timet'], binnumber=bin_num)\n",
    "    stock_data.loc[:, 'bin_num'] = bin_nums\n",
    "    vol_df = cal_bin_volume(subdf=stock_data, binnumber=bin_num)\n",
    "    vol_df = vol_df.rename(columns={'volume_x': 'daily_volume'})\n",
    "    vol_df = vol_df.rename(columns={'volume_y': 'bin_volume'})\n",
    "    vol_df['bin_volume'] = vol_df['bin_volume'].fillna(1)  # 空值用1填充\n",
    "    vol_df['daily_volume'] = vol_df['daily_volume'].fillna(method = 'bfill')  # 空值用向上填充\n",
    "    return vol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "013fee44-5941-4b0f-ba68-aedae9032e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_number, is_filter):\n",
    "    '''\n",
    "    读取一只股票所有日期的数据，from start_date to end_date\n",
    "    '''\n",
    "\n",
    "    data_concat = pd.DataFrame(columns=['date', 'daily_volume', 'bin_num', 'bin_volume'])\n",
    "\n",
    "    start_date1 = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date1 = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    interval_day = (end_date1-start_date1).days\n",
    "\n",
    "    ##遍历日期\n",
    "\n",
    "    for i in range(interval_day+1):\n",
    "        date = datetime.datetime.strptime(start_date,'%Y-%m-%d') + datetime.timedelta(days=i)\n",
    "\n",
    "        date2 = datetime.datetime.strftime(date, '%Y-%m-%d')\n",
    "        year = date2[0:4]\n",
    "        month = date2[5:7]\n",
    "        day = date2[8:10]\n",
    "\n",
    "        if len(str(month)) < 2:\n",
    "            month = str(0) + str(month)\n",
    "        else:\n",
    "            month = str(month)\n",
    "        if len(str(day)) < 2:\n",
    "            day = str(0) + str(day)\n",
    "        else:\n",
    "            day = str(day)\n",
    "\n",
    "        dirs = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "                    month) + '/' + str(day) + '/'\n",
    "\n",
    "\n",
    "        if not (os.path.exists(dirs)):\n",
    "            continue\n",
    "        else:\n",
    "            print(year, month, day)\n",
    "            data = get_df(data_home, data_type, venue, year, month, day, ticker, bin_number, is_filter)\n",
    "            frames = [data_concat, data]\n",
    "            data_concat = pd.concat(frames)  # 将一只股票多天的数据合并到一个数据框里\n",
    "\n",
    "    return data_concat.reset_index(drop=True)  # 返回合并后的数据框并重新设置下标\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c821d8-88de-42af-b56b-794a6a06e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 03 18\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/volume1/sinoalgo/data/sinoalgo/JQMarketData/STOCK/XSHE/2020/03/18/002988.XSHE.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4109306/1540905032.py\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdata_generating_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvenues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbin_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4109306/1540905032.py\u001b[0m in \u001b[0;36mdata_generating_all\u001b[0;34m(data_home, data_types, venues, tickers, start_date, end_date, bin_num, is_filter)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mvenue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvenues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_stock_data_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         filename_basic = '/volume1/home/rzhu/新股票/数据/' + str(ticker) + '_' + str(venue) + '_' + str(\n\u001b[1;32m     18\u001b[0m             bin_num) +'_'+'daily.csv'\n",
      "\u001b[0;32m/tmp/ipykernel_4109306/1759212564.py\u001b[0m in \u001b[0;36mread_stock_data_all\u001b[0;34m(data_home, data_type, venue, start_date, end_date, ticker, bin_number, is_filter)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdata_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 将一只股票多天的数据合并到一个数据框里\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4109306/2297278596.py\u001b[0m in \u001b[0;36mget_df\u001b[0;34m(data_home, data_type, venue, year, month, day, ticker, bin_num, is_filter)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mstock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_stock_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mstock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mtransdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4109306/2297278596.py\u001b[0m in \u001b[0;36mread_stock_data\u001b[0;34m(data_home, data_type, venue, year, month, day, ticker, is_filter)\u001b[0m\n\u001b[1;32m     30\u001b[0m         path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n\u001b[1;32m     31\u001b[0m             month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue) + '.gz'\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'volume'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'current'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a1_v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a1_p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b1_v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b1_p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/volume1/sinoalgo/data/sinoalgo/JQMarketData/STOCK/XSHE/2020/03/18/002988.XSHE.gz'"
     ]
    }
   ],
   "source": [
    "data_home = '/volume1/sinoalgo/data/sinoalgo/JQMarketData'\n",
    "data_types = [ 'STOCK', 'STOCK']\n",
    "venues = [\n",
    "          'XSHE', 'XSHE']\n",
    "tickers = [ \n",
    "           '002988']\n",
    "start_date = \"2020-03-18\"\n",
    "end_date = \"2021-1-10\"\n",
    "bin_num = 25\n",
    "def data_generating_all(data_home, data_types, venues, tickers,start_date,end_date,bin_num, is_filter=0):\n",
    "    data_home = data_home\n",
    "    for i in range(len(data_types)):\n",
    "        data_type = data_types[i]\n",
    "        venue = venues[i]\n",
    "        ticker = tickers[i]\n",
    "        result_df = read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_num, is_filter=0)\n",
    "        filename_basic = '/volume1/home/rzhu/新股票/数据/' + str(ticker) + '_' + str(venue) + '_' + str(\n",
    "            bin_num) +'_'+'daily.csv'\n",
    "        result_df.to_csv(filename_basic, index=False)\n",
    "\n",
    "data_generating_all(data_home,data_types,venues,tickers,start_date,end_date,bin_num,is_filter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a918d86-127d-4693-9cab-1c42d1773ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['300059', '000603', '300750', '000026', '000407', '300023', '000725', '300116', '000958', '003017', '300180', '300162', '002046', '000004', '000036', '000032', '002075', '002801', '002651', '000504', '000005', '000049', '300056', '300016', '000020', '300649']\n"
     ]
    }
   ],
   "source": [
    "# 给定的股票列表\n",
    "stocks = [\n",
    "    '000725.XSHE', '300059.XSHE', '300750.XSHE', '000049.XSHE', '300016.XSHE',\n",
    "    '000032.XSHE', '002046.XSHE', '003017.XSHE', '300023.XSHE', '000005.XSHE',\n",
    "    '000725.XSHE', '300059.XSHE', '300750.XSHE', '000049.XSHE', '000026.XSHE',\n",
    "    '002075.XSHE', '300016.XSHE', '000032.XSHE', '002046.XSHE', '002651.XSHE',\n",
    "    '000004.XSHE', '300180.XSHE', '003017.XSHE', '300023.XSHE', '000005.XSHE',\n",
    "    '000407.XSHE', '000020.XSHE', '300649.XSHE', '300750.XSHE', '000049.XSHE',\n",
    "    '000032.XSHE', '003017.XSHE', '002801.XSHE', '000504.XSHE', '000725.XSHE',\n",
    "    '002046.XSHE', '000005.XSHE', '300116.XSHE', '000036.XSHE', '000958.XSHE',\n",
    "    '300059.XSHE', '300016.XSHE', '300023.XSHE', '300056.XSHE', '000603.XSHE',\n",
    "    '300162.XSHE'\n",
    "]\n",
    "\n",
    "# 整理成指定形式的股票列表，并去除重复\n",
    "formatted_stocks = list(set([stock.split('.')[0] for stock in stocks]))\n",
    "\n",
    "# 打印整理后的股票列表\n",
    "print(formatted_stocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cbe8735-8ae8-4ac1-a65e-1a1213519a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_stocks.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f26692-253d-4695-b924-5bdc96f41b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000004',\n",
       " '000005',\n",
       " '000020',\n",
       " '000026',\n",
       " '000032',\n",
       " '000036',\n",
       " '000049',\n",
       " '000407',\n",
       " '000504',\n",
       " '000603',\n",
       " '000725',\n",
       " '000958',\n",
       " '002046',\n",
       " '002075',\n",
       " '002651',\n",
       " '002801',\n",
       " '003017',\n",
       " '300016',\n",
       " '300023',\n",
       " '300056',\n",
       " '300059',\n",
       " '300116',\n",
       " '300162',\n",
       " '300180',\n",
       " '300649',\n",
       " '300750']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "737e83b6-ac69-4c94-96bd-22b6432bd4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000004',\n",
       " '000005',\n",
       " '000009',\n",
       " '000026',\n",
       " '000032',\n",
       " '000036',\n",
       " '000049',\n",
       " '000407',\n",
       " '000504',\n",
       " '000603',\n",
       " '000617',\n",
       " '000725',\n",
       " '000958',\n",
       " '002046',\n",
       " '002651',\n",
       " '002781',\n",
       " '002801',\n",
       " '300016',\n",
       " '300023',\n",
       " '300056',\n",
       " '300059',\n",
       " '300116',\n",
       " '300162',\n",
       " '300180',\n",
       " '300810',\n",
       " '300750']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['000004',\n",
    " '000005',\n",
    " '000009',\n",
    " '000026',\n",
    " '000032',\n",
    " '000036',\n",
    " '000049',\n",
    " '000407',\n",
    " '000504',\n",
    " '000603',\n",
    " '000617',\n",
    " '000725',\n",
    " '000958',\n",
    " '002046',\n",
    " '002651',\n",
    " '002781',\n",
    " '002801',\n",
    " '300016',\n",
    " '300023',\n",
    " '300056',\n",
    " '300059',\n",
    " '300116',\n",
    " '300162',\n",
    " '300180',\n",
    " '300810',\n",
    " '300750']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c4c1ad-82da-40fb-a983-4e5ae524b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tickers = ['000004',\n",
    " '000005',\n",
    " '000009',\n",
    " '000026',\n",
    " '000032',\n",
    " '000036',\n",
    " '000049',\n",
    " '000407',\n",
    " '000603',\n",
    " '000617',\n",
    " '000725',\n",
    " '000958',\n",
    " '002046',\n",
    " '002651',\n",
    " '002780',\n",
    " '002801',\n",
    " '300016',\n",
    " '300023',\n",
    " '300059',\n",
    " '300116',\n",
    " '300162',\n",
    " '300180',\n",
    " '300810',\n",
    " '300750']\n",
    "input_path='/volume1/home/rzhu/新股票/数据/处理后的/'\n",
    "for ticker in input_tickers:\n",
    "    df = pd.read_csv(input_path + ticker + '_' + 'XSHE' + '_25_daily.csv')\n",
    "    dates_to_remove = df[df['quote_imbalance'].isin([1, -1])]['date'].unique()\n",
    "    df = df[~df['date'].isin(dates_to_remove)]\n",
    "    df.to_csv('/volume1/home/rzhu/新股票/数据/去除涨跌停/' + ticker + '_' + 'XSHE' + '_25_daily.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc876cd-a68c-470e-abd3-2e3e84bd1079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e9f3e-d366-4cd6-ac45-516548e992e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c407b3-fe17-4352-ac96-0e69deee9f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed56f0-7289-4037-b083-2a64e7cb89ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e922f56-f317-4ed7-8c77-db440dbd1003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e746e7-7e6e-47d7-83b1-ee8d1d8aace3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
