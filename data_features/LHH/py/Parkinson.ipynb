{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0f9c77-31aa-464e-99e4-e323537b2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dateutil.parser as parser\n",
    "import math\n",
    "def read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter):\n",
    "    '''\n",
    "    从sever中读取一只股票一天的数据\n",
    "    data_home:数据所在folder\n",
    "    data_type:类型\n",
    "    venue:交易所\n",
    "    is_filter:是否进行filter操作\n",
    "    '''\n",
    "\n",
    "    path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "        month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue)  # 读数据的路径\n",
    "\n",
    "    if (os.path.exists(path)):\n",
    "        data0 = open(path, 'r')\n",
    "        data1 = pd.read_csv(StringIO(data0.read()))\n",
    "        data = data1.loc[:, ['time', 'volume', 'current','a1_v','a1_p','b1_v','b1_p',]]\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['current'] = data['current']/10000\n",
    "        data['spread'] = (data['a1_p'] - data['b1_p'])/(data['a1_p'] + data['b1_p'])  # 计算 spread\n",
    "        data['quote_imbalance'] = (data['b1_v'] - data['a1_v']) / (data['b1_v'] + data['a1_v'])\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "    else:\n",
    "        path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "            month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue) + '.gz'\n",
    "        with gzip.open(path, 'rb') as gf:\n",
    "            data1 = pd.read_csv(gf)\n",
    "        data = data1.loc[:, ['time', 'volume','current', 'a1_v','a1_p','b1_v','b1_p',]]\n",
    "        data['current'] = data['current']/10000\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['quote_imbalance'] = (data['b1_v'] - data['a1_v']) / (data['b1_v'] + data['a1_v'])\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "def trans_date(date): \n",
    "    dates = []\n",
    "    for i in range(len(date)):\n",
    "        year = str(date[i])[0:4]\n",
    "        month = str(date[i])[4:6]\n",
    "        day = str(date[i])[6:8]\n",
    "        date_std = datetime.date(int(year), int(month), int(day)).isoformat()\n",
    "        dates.append(date_std)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def trans_time(time):  \n",
    "    times = []\n",
    "    for i in range(len(time)):\n",
    "        hour = str(time[i])[8:10]\n",
    "        minute = str(time[i])[10:12]\n",
    "        second = str(time[i])[12:14]\n",
    "        time_std = datetime.time(int(hour), int(minute), int(second)).isoformat()\n",
    "        times.append(time_std)\n",
    "    return times\n",
    "\n",
    "\n",
    "\n",
    "def divide_bin(time, binnumber):  # 计算每条交易所属的bin number\n",
    "    '''\n",
    "    time:columns of time\n",
    "    '''\n",
    "    n = 237 / (binnumber - 1) * 60\n",
    "    bin_nums = []\n",
    "    for i in range(len(time)):\n",
    "        if datetime.datetime.strptime(time[i],\"%H:%M:%S\") < datetime.datetime.strptime(\"09:30:00\",\"%H:%M:%S\"):\n",
    "            bin_num = 0  # 交易发生在9：30之前，bin number为0\n",
    "        elif datetime.datetime.strptime(time[i],\"%H:%M:%S\")>datetime.datetime.strptime(\"15:00:00\",\"%H:%M:%S\"):\n",
    "            bin_num = binnumber+1\n",
    "        else:\n",
    "            starttime = parser.parse(datetime.time(9, 30, 0).isoformat())  # 开始时间设为9：30\n",
    "            endtime = parser.parse(time[i])  # 结束时间是该条数据的交易时间\n",
    "            s = (endtime - starttime).seconds  # 从开盘到现在的秒数\n",
    "            if s > -1 and s < 7201:  # 交易发生在9：30-11：30之前\n",
    "                bin_num = int((s - 0.5) // n) + 1  # 9：30之后的bin number从1开始\n",
    "            elif s > 12599 and s < 19801:  # 13:00-15:00\n",
    "                bin_num = int((s - 0.5 - 5400) // n)+1  # 去掉中间的90分钟\n",
    "            else:\n",
    "                bin_num = binnumber\n",
    "        bin_nums.append(bin_num)\n",
    "    return bin_nums\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def cal_volatility_1(a, b):\n",
    "    part_volatility = []\n",
    "    for i in range(len(a)):\n",
    "        result = a[i]/b[i]\n",
    "        part_volatility.append(result)\n",
    "    return part_volatility\n",
    "aggregated['bin_volatility_part1'] = cal_volatility_1(aggregated['max'], aggregated['min'])\n",
    "def cal_volatility(df,j):\n",
    "        df['volatility'] = np.nan\n",
    "        for i in range(j,len(df['bin_volatility_part1'])):\n",
    "            volatility = np.sqrt(sum(df['bin_volatility_part1'][i-j:i])/(4*j*(np.log(2))))\n",
    "            df.loc[i, 'volatility'] = volatility\n",
    "        return df  \n",
    "    subdf1 = cal_volatility(subdf1,4)\n",
    "\n",
    "####### imbalance\n",
    "def cal_bin_volume(subdf, binnumber):\n",
    "    '''\n",
    "    subdf: data to be processed, DataFrame\n",
    "    return: DataFrame including one stock, ranked by bin number\n",
    "    '''\n",
    "\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber + 1])]  # Exclude rows with bin number binnumber+1\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber])]  # Exclude rows with bin number binnumber\n",
    "    subdf['current'].replace(0, float(\"NaN\"),inplace=True)\n",
    "    grouped = subdf.groupby(['date','bin_num'])\n",
    "\n",
    "    aggregated = grouped['current'].agg(['max', 'min', 'first', 'last']).reset_index()\n",
    "    daily_volume = subdf['volume'].groupby(subdf['date']).sum().reset_index()  # Calculate total volume for each day\n",
    "    bin_volume = subdf['volume'].groupby([subdf['date'], subdf['bin_num']]).sum().reset_index()  # Calculate volume for each bin\n",
    "    aggregated['bin_volatility_part1'] = cal_volatility_1(aggregated['max'], aggregated['min'])\n",
    "   \n",
    "    '''\n",
    "    aggregated['bin_volatility_part1'] = aggregated.apply(lambda x: cal_volatility_1(x['max'], x['min']), axis=1)\n",
    "    \n",
    "    '''\n",
    "    df = pd.merge(daily_volume, bin_volume,how='outer', on='date')  # Merge daily_volume and bin_volume\n",
    "    \n",
    "    subdf1 = pd.merge(df, aggregated[['date', 'bin_num', 'bin_volatility_part1']], how='outer', on=['date','bin_num'])\n",
    "\n",
    "    \n",
    "    def exponential_weighted_average(numbers, alpha):\n",
    "        n = len(numbers)\n",
    "        weights = np.array([alpha ** (n - 1 - i) for i in range(n)])  # 使用倒序的权重计算\n",
    "        weighted_sum = np.sum(np.multiply(numbers, weights))\n",
    "        weight_sum = np.sum(weights)\n",
    "        ewma = weighted_sum / weight_sum\n",
    "        return ewma\n",
    "\n",
    "    def cal_volatility(df,j):\n",
    "        df['volatility'] = np.nan\n",
    "        for i in range(j,len(df['bin_volatility_part1'])):\n",
    "            volatility = np.sqrt(sum(df['bin_volatility_part1'][i-j:i])/(4*j*(np.log(2))))\n",
    "            df.loc[i, 'volatility'] = volatility\n",
    "        return df  \n",
    "    subdf1 = cal_volatility(subdf1,4)\n",
    "    subdf1.drop(['bin_volatility_part1'], axis=1, inplace=True)\n",
    "    imbalance = subdf['quote_imbalance'].groupby([subdf['date'], subdf['bin_num']]).apply(lambda x: exponential_weighted_average(x, 0.9)).reset_index()\n",
    "    subdf1 = pd.merge(subdf1,imbalance,how='outer',on=['date','bin_num'])\n",
    "    return subdf1\n",
    "\n",
    "\n",
    "def get_df(data_home, data_type, venue, year, month, day,ticker, bin_num, is_filter=0):\n",
    "    stock_data = read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter=0)\n",
    "    stock_data = stock_data.reset_index(drop=True)\n",
    "    transdate = trans_date(stock_data['time'])\n",
    "    transtime = trans_time(stock_data['time'])\n",
    "    stock_data.loc[:, 'date'] = transdate  \n",
    "    stock_data.loc[:, 'timet'] = transtime\n",
    "    bin_nums = divide_bin(time=stock_data['timet'], binnumber=bin_num)\n",
    "    stock_data.loc[:, 'bin_num'] = bin_nums\n",
    "    vol_df = cal_bin_volume(subdf=stock_data, binnumber=bin_num)\n",
    "    vol_df = vol_df.rename(columns={'volume_x': 'daily_volume'})\n",
    "    vol_df = vol_df.rename(columns={'volume_y': 'bin_volume'})\n",
    "    vol_df['bin_volume'] = vol_df['bin_volume'].fillna(1)  # 空值用1填充\n",
    "    vol_df['daily_volume'] = vol_df['daily_volume'].fillna(method = 'bfill')  # 空值用向上填充\n",
    "    return vol_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd87d71-90ed-42a9-9aec-28fcb1024589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_number, is_filter):\n",
    "    '''\n",
    "    读取一只股票所有日期的数据，from start_date to end_date\n",
    "    '''\n",
    "\n",
    "    data_concat = pd.DataFrame(columns=['date', 'daily_volume', 'bin_num', 'bin_volume'])\n",
    "\n",
    "    start_date1 = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date1 = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    interval_day = (end_date1-start_date1).days\n",
    "\n",
    "    ##遍历日期\n",
    "\n",
    "    for i in range(interval_day+1):\n",
    "        date = datetime.datetime.strptime(start_date,'%Y-%m-%d') + datetime.timedelta(days=i)\n",
    "\n",
    "        date2 = datetime.datetime.strftime(date, '%Y-%m-%d')\n",
    "        year = date2[0:4]\n",
    "        month = date2[5:7]\n",
    "        day = date2[8:10]\n",
    "\n",
    "        if len(str(month)) < 2:\n",
    "            month = str(0) + str(month)\n",
    "        else:\n",
    "            month = str(month)\n",
    "        if len(str(day)) < 2:\n",
    "            day = str(0) + str(day)\n",
    "        else:\n",
    "            day = str(day)\n",
    "\n",
    "        dirs = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "                    month) + '/' + str(day) + '/'\n",
    "\n",
    "\n",
    "        if not (os.path.exists(dirs)):\n",
    "            continue\n",
    "        else:\n",
    "            print(year, month, day)\n",
    "            data = get_df(data_home, data_type, venue, year, month, day, ticker, bin_number, is_filter)\n",
    "            frames = [data_concat, data]\n",
    "            data_concat = pd.concat(frames)  # 将一只股票多天的数据合并到一个数据框里\n",
    "\n",
    "    return data_concat.reset_index(drop=True)  # 返回合并后的数据框并重新设置下标\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e0d2c-ef9c-4409-96e1-e2f01175b346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 09 01\n",
      "2020 09 02\n",
      "2020 09 03\n",
      "2020 09 04\n",
      "2020 09 07\n",
      "2020 09 08\n",
      "2020 09 09\n",
      "2020 09 10\n",
      "2020 09 11\n",
      "2020 09 14\n",
      "2020 09 15\n",
      "2020 09 16\n",
      "2020 09 17\n",
      "2020 09 18\n",
      "2020 09 21\n",
      "2020 09 22\n",
      "2020 09 23\n",
      "2020 09 24\n",
      "2020 09 25\n",
      "2020 09 28\n",
      "2020 09 29\n",
      "2020 09 30\n",
      "2020 10 09\n",
      "2020 10 12\n",
      "2020 10 13\n",
      "2020 10 14\n",
      "2020 10 15\n",
      "2020 10 16\n",
      "2020 10 19\n",
      "2020 10 20\n",
      "2020 10 21\n",
      "2020 10 22\n",
      "2020 10 23\n",
      "2020 10 26\n",
      "2020 10 27\n",
      "2020 10 28\n",
      "2020 10 29\n",
      "2020 10 30\n",
      "2020 11 02\n",
      "2020 11 03\n",
      "2020 11 04\n",
      "2020 11 05\n",
      "2020 11 06\n",
      "2020 11 09\n",
      "2020 11 10\n",
      "2020 11 11\n",
      "2020 11 12\n",
      "2020 11 13\n",
      "2020 11 16\n",
      "2020 11 17\n",
      "2020 11 18\n",
      "2020 11 19\n",
      "2020 11 20\n",
      "2020 11 23\n",
      "2020 11 24\n",
      "2020 11 25\n",
      "2020 11 26\n",
      "2020 11 27\n",
      "2020 11 30\n",
      "2020 12 01\n",
      "2020 12 02\n",
      "2020 12 03\n",
      "2020 12 04\n",
      "2020 12 07\n",
      "2020 12 08\n",
      "2020 12 09\n",
      "2020 12 10\n",
      "2020 12 11\n",
      "2020 12 14\n",
      "2020 12 15\n",
      "2020 12 16\n",
      "2020 12 17\n",
      "2020 12 18\n",
      "2020 12 21\n",
      "2020 12 22\n",
      "2020 12 23\n",
      "2020 12 24\n",
      "2020 12 25\n",
      "2020 12 28\n",
      "2020 12 29\n",
      "2020 12 30\n",
      "2020 12 31\n",
      "2021 01 04\n",
      "2021 01 05\n",
      "2021 01 06\n",
      "2021 01 07\n",
      "2021 01 08\n",
      "2021 01 11\n",
      "2021 01 12\n",
      "2021 01 13\n",
      "2021 01 14\n",
      "2021 01 15\n",
      "2021 01 18\n",
      "2021 01 19\n",
      "2021 01 20\n",
      "2021 01 21\n",
      "2021 01 22\n",
      "2021 01 25\n",
      "2021 01 26\n",
      "2021 01 27\n",
      "2021 01 28\n",
      "2021 01 29\n",
      "2021 02 01\n",
      "2021 02 02\n",
      "2021 02 03\n",
      "2021 02 04\n",
      "2021 02 05\n",
      "2021 02 08\n",
      "2021 02 09\n",
      "2021 02 10\n",
      "2021 02 18\n",
      "2021 02 19\n",
      "2021 02 22\n",
      "2021 02 23\n",
      "2021 02 24\n",
      "2021 02 25\n",
      "2021 02 26\n",
      "2021 03 01\n",
      "2021 03 02\n",
      "2021 03 03\n",
      "2021 03 04\n",
      "2021 03 05\n",
      "2021 03 08\n",
      "2021 03 09\n",
      "2021 03 10\n",
      "2021 03 11\n",
      "2021 03 12\n",
      "2021 03 15\n",
      "2021 03 16\n",
      "2021 03 17\n",
      "2021 03 18\n",
      "2021 03 19\n",
      "2021 03 22\n",
      "2021 03 23\n",
      "2021 03 24\n",
      "2021 03 25\n"
     ]
    }
   ],
   "source": [
    "data_home = '/volume1/sinoalgo/data/sinoalgo/JQMarketData'\n",
    "data_types = ['STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK']\n",
    "venues = ['XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE']\n",
    "tickers = ['000725', '300015', '300185', '000002', '000807', '002340', '000001','300750','300059','000166','000009']\n",
    "start_date = \"2020-09-01\"\n",
    "end_date = \"2021-06-30\"\n",
    "bin_num = 25\n",
    "\n",
    "\n",
    "def data_generating_all(data_home, data_types, venues, tickers,start_date,end_date,bin_num, is_filter=1):\n",
    "    data_home = data_home\n",
    "    for i in range(len(data_types)):\n",
    "        data_type = data_types[i]\n",
    "        venue = venues[i]\n",
    "        ticker = tickers[i]\n",
    "        result_df = read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_num, is_filter=0)\n",
    "        \n",
    "        filename_basic = 'result_new' + str(ticker) + '_' + str(venue) + '_' + str(bin_num) + '_daily.csv'\n",
    "        result_df.to_csv(filename_basic, index=False)\n",
    "        \n",
    "data_generating_all(data_home,data_types,venues,tickers,start_date,end_date,bin_num,is_filter=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593b6f3-0c4b-4755-97b4-4e0a64536b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945378e-508c-4873-ae33-5577c424dcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd91f81-3ce3-4433-b274-862de1285456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9958c46-8dce-4516-9390-24927afbf7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb161c1a-4b0e-45ad-b0b0-d5855c5fa14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dateutil.parser as parser\n",
    "import math\n",
    "def read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter):\n",
    "    '''\n",
    "    从sever中读取一只股票一天的数据\n",
    "    data_home:数据所在folder\n",
    "    data_type:类型\n",
    "    venue:交易所\n",
    "    is_filter:是否进行filter操作\n",
    "    '''\n",
    "\n",
    "    path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "        month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue)  # 读数据的路径\n",
    "\n",
    "    if (os.path.exists(path)):\n",
    "        data0 = open(path, 'r')\n",
    "        data1 = pd.read_csv(StringIO(data0.read()))\n",
    "        data = data1.loc[:, ['time', 'volume', 'current','a1_v','a1_p','b1_v','b1_p',]]\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['current'] = data['current']/10000\n",
    "        data['spread'] = (data['a1_p'] - data['b1_p'])/(data['a1_p'] + data['b1_p'])  # 计算 spread\n",
    "        data['quote_imbalance'] = (data['b1_v'] - data['a1_v']) / (data['b1_v'] + data['a1_v'])\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "    else:\n",
    "        path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "            month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue) + '.gz'\n",
    "        with gzip.open(path, 'rb') as gf:\n",
    "            data1 = pd.read_csv(gf)\n",
    "        data = data1.loc[:, ['time', 'volume','current', 'a1_v','a1_p','b1_v','b1_p',]]\n",
    "        data['current'] = data['current']/10000\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['quote_imbalance'] = (data['b1_v'] - data['a1_v']) / (data['b1_v'] + data['a1_v'])\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "def trans_date(date): \n",
    "    dates = []\n",
    "    for i in range(len(date)):\n",
    "        year = str(date[i])[0:4]\n",
    "        month = str(date[i])[4:6]\n",
    "        day = str(date[i])[6:8]\n",
    "        date_std = datetime.date(int(year), int(month), int(day)).isoformat()\n",
    "        dates.append(date_std)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def trans_time(time):  \n",
    "    times = []\n",
    "    for i in range(len(time)):\n",
    "        hour = str(time[i])[8:10]\n",
    "        minute = str(time[i])[10:12]\n",
    "        second = str(time[i])[12:14]\n",
    "        time_std = datetime.time(int(hour), int(minute), int(second)).isoformat()\n",
    "        times.append(time_std)\n",
    "    return times\n",
    "\n",
    "\n",
    "\n",
    "def divide_bin(time, binnumber):  # 计算每条交易所属的bin number\n",
    "    '''\n",
    "    time:columns of time\n",
    "    '''\n",
    "    n = 237 / (binnumber - 1) * 60\n",
    "    bin_nums = []\n",
    "    for i in range(len(time)):\n",
    "        if datetime.datetime.strptime(time[i],\"%H:%M:%S\") < datetime.datetime.strptime(\"09:30:00\",\"%H:%M:%S\"):\n",
    "            bin_num = 0  # 交易发生在9：30之前，bin number为0\n",
    "        elif datetime.datetime.strptime(time[i],\"%H:%M:%S\")>datetime.datetime.strptime(\"15:00:00\",\"%H:%M:%S\"):\n",
    "            bin_num = binnumber+1\n",
    "        else:\n",
    "            starttime = parser.parse(datetime.time(9, 30, 0).isoformat())  # 开始时间设为9：30\n",
    "            endtime = parser.parse(time[i])  # 结束时间是该条数据的交易时间\n",
    "            s = (endtime - starttime).seconds  # 从开盘到现在的秒数\n",
    "            if s > -1 and s < 7201:  # 交易发生在9：30-11：30之前\n",
    "                bin_num = int((s - 0.5) // n) + 1  # 9：30之后的bin number从1开始\n",
    "            elif s > 12599 and s < 19801:  # 13:00-15:00\n",
    "                bin_num = int((s - 0.5 - 5400) // n)+1  # 去掉中间的90分钟\n",
    "            else:\n",
    "                bin_num = binnumber\n",
    "        bin_nums.append(bin_num)\n",
    "    return bin_nums\n",
    "\n",
    "    \n",
    "####### imbalance\n",
    "def cal_bin_volume(subdf, binnumber):\n",
    "    '''\n",
    "    subdf: data to be processed, DataFrame\n",
    "    return: DataFrame including one stock, ranked by bin number\n",
    "    '''\n",
    "\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber + 1])]  # Exclude rows with bin number binnumber+1\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber])]  # Exclude rows with bin number binnumber\n",
    "    subdf['current'].replace(0, float(\"NaN\"),inplace=True)\n",
    "    grouped = subdf.groupby(['date','bin_num'])\n",
    "    aggregated = grouped['current'].agg(['max', 'min', 'first', 'last']).reset_index()\n",
    "    \n",
    "    daily_volume = subdf['volume'].groupby(subdf['date']).sum().reset_index()  # Calculate total volume for each day\n",
    "    bin_volume = subdf['volume'].groupby([subdf['date'], subdf['bin_num']]).sum().reset_index()  # Calculate volume for each bin\n",
    "\n",
    "    df = pd.merge(daily_volume, bin_volume,how='outer', on='date')  # Merge daily_volume and bin_volume\n",
    "    \n",
    "    subdf1 = pd.merge(df, aggregated[['date', 'bin_num', 'max','min','first','last']], how='outer', on=['date','bin_num'])\n",
    "\n",
    "    \n",
    "    def exponential_weighted_average(numbers, alpha):\n",
    "        n = len(numbers)\n",
    "        weights = np.array([alpha ** (n - 1 - i) for i in range(n)])  # 使用倒序的权重计算\n",
    "        weighted_sum = np.sum(np.multiply(numbers, weights))\n",
    "        weight_sum = np.sum(weights)\n",
    "        ewma = weighted_sum / weight_sum\n",
    "        return ewma\n",
    "\n",
    "    imbalance = subdf['quote_imbalance'].groupby([subdf['date'], subdf['bin_num']]).apply(lambda x: exponential_weighted_average(x, 0.9)).reset_index()\n",
    "    subdf1 = pd.merge(subdf1,imbalance,how='outer',on=['date','bin_num'])\n",
    "    return subdf1\n",
    "\n",
    "\n",
    "def get_df(data_home, data_type, venue, year, month, day,ticker, bin_num, is_filter=0):\n",
    "    stock_data = read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter=0)\n",
    "    stock_data = stock_data.reset_index(drop=True)\n",
    "    transdate = trans_date(stock_data['time'])\n",
    "    transtime = trans_time(stock_data['time'])\n",
    "    stock_data.loc[:, 'date'] = transdate  \n",
    "    stock_data.loc[:, 'timet'] = transtime\n",
    "    bin_nums = divide_bin(time=stock_data['timet'], binnumber=bin_num)\n",
    "    stock_data.loc[:, 'bin_num'] = bin_nums\n",
    "    vol_df = cal_bin_volume(subdf=stock_data, binnumber=bin_num)\n",
    "    vol_df = vol_df.rename(columns={'volume_x': 'daily_volume'})\n",
    "    vol_df = vol_df.rename(columns={'volume_y': 'bin_volume'})\n",
    "    vol_df['bin_volume'] = vol_df['bin_volume'].fillna(1)  # 空值用1填充\n",
    "    vol_df['daily_volume'] = vol_df['daily_volume'].fillna(method = 'bfill')  # 空值用向上填充\n",
    "    return vol_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82d894b-b6ab-467a-919b-c0cdbd4a5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_number, is_filter):\n",
    "    '''\n",
    "    读取一只股票所有日期的数据，from start_date to end_date\n",
    "    '''\n",
    "\n",
    "    data_concat = pd.DataFrame(columns=['date', 'daily_volume', 'bin_num', 'bin_volume'])\n",
    "\n",
    "    start_date1 = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date1 = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    interval_day = (end_date1-start_date1).days\n",
    "\n",
    "    ##遍历日期\n",
    "\n",
    "    for i in range(interval_day+1):\n",
    "        date = datetime.datetime.strptime(start_date,'%Y-%m-%d') + datetime.timedelta(days=i)\n",
    "\n",
    "        date2 = datetime.datetime.strftime(date, '%Y-%m-%d')\n",
    "        year = date2[0:4]\n",
    "        month = date2[5:7]\n",
    "        day = date2[8:10]\n",
    "\n",
    "        if len(str(month)) < 2:\n",
    "            month = str(0) + str(month)\n",
    "        else:\n",
    "            month = str(month)\n",
    "        if len(str(day)) < 2:\n",
    "            day = str(0) + str(day)\n",
    "        else:\n",
    "            day = str(day)\n",
    "\n",
    "        dirs = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "                    month) + '/' + str(day) + '/'\n",
    "\n",
    "\n",
    "        if not (os.path.exists(dirs)):\n",
    "            continue\n",
    "        else:\n",
    "            print(year, month, day)\n",
    "            data = get_df(data_home, data_type, venue, year, month, day, ticker, bin_number, is_filter)\n",
    "            frames = [data_concat, data]\n",
    "            data_concat = pd.concat(frames)  # 将一只股票多天的数据合并到一个数据框里\n",
    "\n",
    "\n",
    "    def cal_volatility_1(a, b):\n",
    "        return (np.log(np.multiply(a,1/b)))**2\n",
    "    data_concat['part_volatility'] = cal_volatility_1(data_concat['max'], data_concat['min'])\n",
    "    def cal_volatility(df,j):\n",
    "        df['volatility'] = np.nan\n",
    "        for i in range(j,len(df['bin_nums'])):\n",
    "            volatility = np.sqrt(sum(df['part_volatility'][i-j:i])/(4*j*(np.log(2))))\n",
    "            df.loc[i, 'volatility'] = volatility\n",
    "        return df  \n",
    "    data_concat = cal_volatility_1(data_concat,4)\n",
    "\n",
    "\n",
    "    data_concat.drop(['part_volatility',  'max','min','first','last'], axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return data_concat.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56fcaf72-5427-4256-9715-4173d239da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 09 01\n",
      "2020 09 02\n",
      "2020 09 03\n",
      "2020 09 04\n",
      "2020 09 07\n",
      "2020 09 08\n",
      "2020 09 09\n",
      "2020 09 10\n",
      "2020 09 11\n",
      "2020 09 14\n",
      "2020 09 15\n",
      "2020 09 16\n",
      "2020 09 17\n",
      "2020 09 18\n",
      "2020 09 21\n",
      "2020 09 22\n",
      "2020 09 23\n",
      "2020 09 24\n",
      "2020 09 25\n",
      "2020 09 28\n",
      "2020 09 29\n",
      "2020 09 30\n",
      "2020 10 09\n",
      "2020 10 12\n",
      "2020 10 13\n",
      "2020 10 14\n",
      "2020 10 15\n",
      "2020 10 16\n",
      "2020 10 19\n",
      "2020 10 20\n",
      "2020 10 21\n",
      "2020 10 22\n",
      "2020 10 23\n",
      "2020 10 26\n",
      "2020 10 27\n",
      "2020 10 28\n",
      "2020 10 29\n",
      "2020 10 30\n",
      "2020 11 02\n",
      "2020 11 03\n",
      "2020 11 04\n",
      "2020 11 05\n",
      "2020 11 06\n",
      "2020 11 09\n",
      "2020 11 10\n",
      "2020 11 11\n",
      "2020 11 12\n",
      "2020 11 13\n",
      "2020 11 16\n",
      "2020 11 17\n",
      "2020 11 18\n",
      "2020 11 19\n",
      "2020 11 20\n",
      "2020 11 23\n",
      "2020 11 24\n",
      "2020 11 25\n",
      "2020 11 26\n",
      "2020 11 27\n",
      "2020 11 30\n",
      "2020 12 01\n",
      "2020 12 02\n",
      "2020 12 03\n",
      "2020 12 04\n",
      "2020 12 07\n",
      "2020 12 08\n",
      "2020 12 09\n",
      "2020 12 10\n",
      "2020 12 11\n",
      "2020 12 14\n",
      "2020 12 15\n",
      "2020 12 16\n",
      "2020 12 17\n",
      "2020 12 18\n",
      "2020 12 21\n",
      "2020 12 22\n",
      "2020 12 23\n",
      "2020 12 24\n",
      "2020 12 25\n",
      "2020 12 28\n",
      "2020 12 29\n",
      "2020 12 30\n",
      "2020 12 31\n",
      "2021 01 04\n",
      "2021 01 05\n",
      "2021 01 06\n",
      "2021 01 07\n",
      "2021 01 08\n",
      "2021 01 11\n",
      "2021 01 12\n",
      "2021 01 13\n",
      "2021 01 14\n",
      "2021 01 15\n",
      "2021 01 18\n",
      "2021 01 19\n",
      "2021 01 20\n",
      "2021 01 21\n",
      "2021 01 22\n",
      "2021 01 25\n",
      "2021 01 26\n",
      "2021 01 27\n",
      "2021 01 28\n",
      "2021 01 29\n",
      "2021 02 01\n",
      "2021 02 02\n",
      "2021 02 03\n",
      "2021 02 04\n",
      "2021 02 05\n",
      "2021 02 08\n",
      "2021 02 09\n",
      "2021 02 10\n",
      "2021 02 18\n",
      "2021 02 19\n",
      "2021 02 22\n",
      "2021 02 23\n",
      "2021 02 24\n",
      "2021 02 25\n",
      "2021 02 26\n",
      "2021 03 01\n",
      "2021 03 02\n",
      "2021 03 03\n",
      "2021 03 04\n",
      "2021 03 05\n",
      "2021 03 08\n",
      "2021 03 09\n",
      "2021 03 10\n",
      "2021 03 11\n",
      "2021 03 12\n",
      "2021 03 15\n",
      "2021 03 16\n",
      "2021 03 17\n",
      "2021 03 18\n",
      "2021 03 19\n",
      "2021 03 22\n",
      "2021 03 23\n",
      "2021 03 24\n",
      "2021 03 25\n",
      "2021 03 26\n",
      "2021 03 29\n",
      "2021 03 30\n",
      "2021 03 31\n",
      "2021 04 01\n",
      "2021 04 02\n",
      "2021 04 06\n",
      "2021 04 07\n",
      "2021 04 08\n",
      "2021 04 09\n",
      "2021 04 12\n",
      "2021 04 13\n",
      "2021 04 14\n",
      "2021 04 15\n",
      "2021 04 16\n",
      "2021 04 19\n",
      "2021 04 20\n",
      "2021 04 21\n",
      "2021 04 22\n",
      "2021 04 23\n",
      "2021 04 26\n",
      "2021 04 27\n",
      "2021 04 28\n",
      "2021 04 29\n",
      "2021 04 30\n",
      "2021 05 06\n",
      "2021 05 07\n",
      "2021 05 10\n",
      "2021 05 11\n",
      "2021 05 12\n",
      "2021 05 13\n",
      "2021 05 14\n",
      "2021 05 17\n",
      "2021 05 18\n",
      "2021 05 19\n",
      "2021 05 20\n",
      "2021 05 21\n",
      "2021 05 24\n",
      "2021 05 25\n",
      "2021 05 26\n",
      "2021 05 27\n",
      "2021 05 28\n",
      "2021 05 31\n",
      "2021 06 01\n",
      "2021 06 02\n",
      "2021 06 03\n",
      "2021 06 04\n",
      "2021 06 07\n",
      "2021 06 08\n",
      "2021 06 09\n",
      "2021 06 10\n",
      "2021 06 11\n",
      "2021 06 15\n",
      "2021 06 16\n",
      "2021 06 17\n",
      "2021 06 18\n",
      "2021 06 21\n",
      "2021 06 22\n",
      "2021 06 23\n",
      "2021 06 24\n",
      "2021 06 25\n",
      "2021 06 28\n",
      "2021 06 29\n",
      "2021 06 30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2181977/308862217.py\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdata_generating_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvenues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbin_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2181977/308862217.py\u001b[0m in \u001b[0;36mdata_generating_all\u001b[0;34m(data_home, data_types, venues, tickers, start_date, end_date, bin_num, is_filter)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mvenue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvenues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_stock_data_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfilename_basic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_daily.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2181977/22390512.py\u001b[0m in \u001b[0;36mread_stock_data_all\u001b[0;34m(data_home, data_type, venue, start_date, end_date, ticker, bin_number, is_filter)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcal_volatility_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdata_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'part_volatility'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_volatility_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcal_volatility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'volatility'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2181977/22390512.py\u001b[0m in \u001b[0;36mcal_volatility_1\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcal_volatility_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdata_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'part_volatility'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_volatility_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcal_volatility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot convert the series to {converter}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"__{converter.__name__}__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "data_home = '/volume1/sinoalgo/data/sinoalgo/JQMarketData'\n",
    "data_types = ['STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK']\n",
    "venues = ['XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE']\n",
    "tickers = ['000725', '300015', '300185', '000002', '000807', '002340', '000001','300750','300059','000166','000009']\n",
    "start_date = \"2020-09-01\"\n",
    "end_date = \"2021-06-30\"\n",
    "bin_num = 25\n",
    "\n",
    "\n",
    "def data_generating_all(data_home, data_types, venues, tickers,start_date,end_date,bin_num, is_filter=1):\n",
    "    data_home = data_home\n",
    "    for i in range(len(data_types)):\n",
    "        data_type = data_types[i]\n",
    "        venue = venues[i]\n",
    "        ticker = tickers[i]\n",
    "        result_df = read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_num, is_filter=0)\n",
    "        \n",
    "        filename_basic = ticker + '_daily.csv'\n",
    "        result_df.to_csv(filename_basic, index=False)\n",
    "        \n",
    "        \n",
    "data_generating_all(data_home,data_types,venues,tickers,start_date,end_date,bin_num,is_filter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2198dd-4689-4442-9ab4-4f9311ed4e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
