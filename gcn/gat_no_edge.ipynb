{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e99867e-b47b-4bd1-b701-6860aee8f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa7e249-b0e8-4daf-a41a-1a0302565d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "hyperparams = {\n",
    "    'lr': np.linspace(0.005, 0.03, 6),\n",
    "    'hidden':[6,7,8,9],\n",
    "    'nb_heads':[2,4,6,8],\n",
    "    'dropout': np.linspace(0.4, 0.8, 5),\n",
    "    'alpha':[0.01,0.05,0.1,0.15,0.5],\n",
    "    'weight_decay':[3e-4,4e-4,5e-4,6e-4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160f4062-acf2-4d2d-bb5e-8a3d4865f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "        \n",
    "        e = self.leakyrelu(torch.matmul(a_input.double(), self.a.double()).squeeze(3))\n",
    "\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        #x = F.log_softmax(x, dim=2)[:, -1, :]\n",
    "        # print(x)\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "\n",
    "    \n",
    "    def fit(self,train_loader, val_loader,num_epochs=5000,patience=500):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0001,weight_decay=5e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0 \n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs.squeeze(dim=1), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "            #     train_loss += loss.item()\n",
    "            # avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            #         val_loss += val_loss.item()\n",
    "            # avg_val_loss=val_loss / len(val_loader)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return\n",
    "                \n",
    "                \n",
    "    def test(self,test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110decbd-cfe6-4c03-934f-eab8843b749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000046_XSHE', '000753_XSHE', '000951_XSHE', '000998_XSHE', '002282_XSHE', '002679_XSHE', '002841_XSHE', '002882_XSHE', '300133_XSHE', '300174_XSHE', '300263_XSHE', '300343_XSHE', '300433_XSHE', '300540_XSHE', '600622_XSHG', '603053_XSHG', '603095_XSHG', '603359_XSHG']\n",
      ">>>>>>>>>>>>>>>>>>>>000046_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch [20/5000], Train Loss: 38332739041281.9453, Val Loss: 24927458792480.0547\n",
      "Epoch [40/5000], Train Loss: 30190592173456.3086, Val Loss: 2907731743075.7983\n",
      "Epoch [60/5000], Train Loss: 2584511957931.1855, Val Loss: 1765400941618.8091\n",
      "Epoch [80/5000], Train Loss: 875733815115.3733, Val Loss: 1129193657263.4521\n",
      "Epoch [100/5000], Train Loss: 8031606969793.1680, Val Loss: 812668645929.0121\n",
      "Epoch [120/5000], Train Loss: 818413165918.0103, Val Loss: 908011444273.6157\n",
      "Epoch [140/5000], Train Loss: 855442587267.4602, Val Loss: 806308501312.2937\n",
      "Epoch [160/5000], Train Loss: 383142110365.5735, Val Loss: 479564633180.8091\n",
      "Epoch [180/5000], Train Loss: 404362882547.6578, Val Loss: 390465661715.9664\n",
      "Epoch [200/5000], Train Loss: 183226683734.0930, Val Loss: 388113453588.3483\n",
      "Epoch [220/5000], Train Loss: 129748855045.4909, Val Loss: 405375466430.0168\n",
      "Epoch [240/5000], Train Loss: 280706126877.5519, Val Loss: 389469032658.5778\n",
      "Epoch [260/5000], Train Loss: 1610762154379.6125, Val Loss: 467791306144.3250\n",
      "Epoch [280/5000], Train Loss: 2004476896062.8103, Val Loss: 393180773913.2159\n",
      "Epoch [300/5000], Train Loss: 557260712619.1455, Val Loss: 350046074697.4863\n",
      "Epoch [320/5000], Train Loss: 304071467507.8456, Val Loss: 402287928825.2446\n",
      "Epoch [340/5000], Train Loss: 160651533418.1857, Val Loss: 430293160029.6699\n",
      "Epoch [360/5000], Train Loss: 475060094940.4849, Val Loss: 349085058272.9918\n",
      "Epoch [380/5000], Train Loss: 549639866811.8148, Val Loss: 752036310945.9014\n",
      "Epoch [400/5000], Train Loss: 275717481340.5943, Val Loss: 378054496497.7821\n",
      "Epoch [420/5000], Train Loss: 207943660076.8951, Val Loss: 348918650416.5532\n",
      "Epoch [440/5000], Train Loss: 551173702601.3469, Val Loss: 385066993099.1066\n",
      "Epoch [460/5000], Train Loss: 691624542229.5521, Val Loss: 308999232090.8544\n",
      "Epoch [480/5000], Train Loss: 474003602317.9714, Val Loss: 343238768359.2843\n",
      "Epoch [500/5000], Train Loss: 421156619292.8969, Val Loss: 411375676040.5098\n",
      "Epoch [520/5000], Train Loss: 432335164843.2052, Val Loss: 358084349453.8391\n",
      "Epoch [540/5000], Train Loss: 492124908512.2651, Val Loss: 514854640368.7675\n",
      "Epoch [560/5000], Train Loss: 473572982655.3956, Val Loss: 735655480980.5789\n",
      "Epoch [580/5000], Train Loss: 135135304784.9736, Val Loss: 349108103897.8815\n",
      "Epoch [600/5000], Train Loss: 382475105102.8773, Val Loss: 308566129868.6029\n",
      "Epoch [620/5000], Train Loss: 224633714434.2907, Val Loss: 332219339737.1564\n",
      "Epoch [640/5000], Train Loss: 310076088370.4344, Val Loss: 354839025881.0238\n",
      "Epoch [660/5000], Train Loss: 708185521638.2764, Val Loss: 736611177968.1038\n",
      "Epoch [680/5000], Train Loss: 257354957324.6103, Val Loss: 360810895342.4612\n",
      "Epoch [700/5000], Train Loss: 222896124931.2015, Val Loss: 370040232272.8407\n",
      "Epoch [720/5000], Train Loss: 237770665839.1037, Val Loss: 356690491513.3798\n",
      "Epoch [740/5000], Train Loss: 303165043551.8160, Val Loss: 304397527245.4730\n",
      "Epoch [760/5000], Train Loss: 293586794068.3162, Val Loss: 326028614862.8552\n",
      "Epoch [780/5000], Train Loss: 590054837022.1299, Val Loss: 332123411115.2548\n",
      "Epoch [800/5000], Train Loss: 167378232810.8586, Val Loss: 341802689656.2387\n",
      "Epoch [820/5000], Train Loss: 593809635803.2615, Val Loss: 351640988847.4011\n",
      "Early stopping at epoch 831\n",
      "\n",
      "Fold number: 0\n",
      "[0.5227127721107208]\n",
      "MAPE =  0.5227127721107208\n",
      "\n",
      "Epoch [20/5000], Train Loss: 36322963223003.8438, Val Loss: 9958078682488.8613\n",
      "Epoch [40/5000], Train Loss: 27181060793476.4844, Val Loss: 3898115478064.4014\n",
      "Epoch [60/5000], Train Loss: 2388570404249.6436, Val Loss: 2519912995501.6426\n",
      "Epoch [80/5000], Train Loss: 1520401265092.1987, Val Loss: 1227423777292.1719\n",
      "Epoch [100/5000], Train Loss: 9483870926159.6641, Val Loss: 826825010598.2076\n",
      "Epoch [120/5000], Train Loss: 4257012420886.7002, Val Loss: 565533605989.7743\n",
      "Epoch [140/5000], Train Loss: 580171555609.3174, Val Loss: 416378237028.6243\n",
      "Epoch [160/5000], Train Loss: 344510208852.0991, Val Loss: 291887246991.7724\n",
      "Epoch [180/5000], Train Loss: 440200567494.4302, Val Loss: 284541680016.3833\n",
      "Epoch [200/5000], Train Loss: 171399348373.9167, Val Loss: 278943392150.5402\n",
      "Epoch [220/5000], Train Loss: 197195572133.2802, Val Loss: 274258247683.9083\n",
      "Epoch [240/5000], Train Loss: 204934015281.0558, Val Loss: 274521212731.0662\n",
      "Epoch [260/5000], Train Loss: 573684242377.8690, Val Loss: 274053981158.6021\n",
      "Epoch [280/5000], Train Loss: 2059967383450.3933, Val Loss: 259568754002.3877\n",
      "Epoch [300/5000], Train Loss: 704288300745.3868, Val Loss: 245765195959.2460\n",
      "Epoch [320/5000], Train Loss: 173931950917.7588, Val Loss: 247228018560.5909\n",
      "Epoch [340/5000], Train Loss: 133419355121.1974, Val Loss: 243425071477.1716\n",
      "Epoch [360/5000], Train Loss: 471235297032.7708, Val Loss: 241783081283.4760\n",
      "Epoch [380/5000], Train Loss: 413325755268.3255, Val Loss: 243063763967.0413\n",
      "Epoch [400/5000], Train Loss: 266355664336.4955, Val Loss: 263954583158.7891\n",
      "Epoch [420/5000], Train Loss: 606318995292.6517, Val Loss: 214010850002.9753\n",
      "Epoch [440/5000], Train Loss: 532272381977.4841, Val Loss: 239464944886.1945\n",
      "Epoch [460/5000], Train Loss: 675849014655.6774, Val Loss: 262592489479.0754\n",
      "Epoch [480/5000], Train Loss: 679113482511.4874, Val Loss: 500068054876.2527\n",
      "Epoch [500/5000], Train Loss: 448680814682.5559, Val Loss: 233073462750.8940\n",
      "Epoch [520/5000], Train Loss: 484024534643.7931, Val Loss: 474019431959.9572\n",
      "Epoch [540/5000], Train Loss: 367535379514.8136, Val Loss: 187505841463.2136\n",
      "Epoch [560/5000], Train Loss: 448248439699.3566, Val Loss: 414848811254.8375\n",
      "Epoch [580/5000], Train Loss: 106085360065.4239, Val Loss: 182410246656.9336\n",
      "Epoch [600/5000], Train Loss: 591084511402.8798, Val Loss: 454254106489.0916\n",
      "Epoch [620/5000], Train Loss: 200366901189.4938, Val Loss: 208799855115.3622\n",
      "Epoch [640/5000], Train Loss: 234870933006.9449, Val Loss: 192459224445.0537\n",
      "Epoch [660/5000], Train Loss: 539561663167.1004, Val Loss: 183293859719.9406\n",
      "Epoch [680/5000], Train Loss: 229207976435.1331, Val Loss: 180354098491.8663\n",
      "Epoch [700/5000], Train Loss: 133168032015.9808, Val Loss: 171635484485.1500\n",
      "Epoch [720/5000], Train Loss: 122024421400.6445, Val Loss: 165930102867.1604\n",
      "Epoch [740/5000], Train Loss: 236032259511.0802, Val Loss: 159606458644.3144\n",
      "Epoch [760/5000], Train Loss: 571044732715.0524, Val Loss: 478889875989.1219\n",
      "Epoch [780/5000], Train Loss: 581489593850.4354, Val Loss: 154080307175.1508\n",
      "Epoch [800/5000], Train Loss: 361790677130.0328, Val Loss: 328021684786.6451\n",
      "Epoch [820/5000], Train Loss: 515241286289.7250, Val Loss: 163706598634.4128\n",
      "Epoch [840/5000], Train Loss: 407780951135.5825, Val Loss: 160288979693.6894\n",
      "Epoch [860/5000], Train Loss: 704971674963.3827, Val Loss: 171240547712.5235\n",
      "Epoch [880/5000], Train Loss: 173924847324.4977, Val Loss: 161533341238.9061\n",
      "Epoch [900/5000], Train Loss: 351633509421.1821, Val Loss: 173490398774.7599\n",
      "Epoch [920/5000], Train Loss: 1217251778342.1245, Val Loss: 154862412042.7409\n",
      "Epoch [940/5000], Train Loss: 273753667362.8685, Val Loss: 165405157449.1562\n",
      "Epoch [960/5000], Train Loss: 164064847543.9400, Val Loss: 161468413253.6109\n",
      "Epoch [980/5000], Train Loss: 352214075872.0322, Val Loss: 156363269303.5314\n",
      "Epoch [1000/5000], Train Loss: 1917758918225.9597, Val Loss: 176275270619.8272\n",
      "Epoch [1020/5000], Train Loss: 306054921507.1105, Val Loss: 159376124958.3590\n",
      "Epoch [1040/5000], Train Loss: 212287734700.1918, Val Loss: 152815301286.4317\n",
      "Epoch [1060/5000], Train Loss: 77330471331.9378, Val Loss: 180301834355.3026\n",
      "Epoch [1080/5000], Train Loss: 426467707031.6440, Val Loss: 154860228281.6862\n",
      "Epoch [1100/5000], Train Loss: 517740297281.1521, Val Loss: 163188532028.1015\n",
      "Epoch [1120/5000], Train Loss: 195701188385.4711, Val Loss: 174796637229.9879\n",
      "Epoch [1140/5000], Train Loss: 314002889224.6478, Val Loss: 489651922332.6715\n",
      "Epoch [1160/5000], Train Loss: 329288558951.1773, Val Loss: 144568451975.9800\n",
      "Epoch [1180/5000], Train Loss: 764446635755.6638, Val Loss: 154981168550.7788\n",
      "Epoch [1200/5000], Train Loss: 138490079746.9348, Val Loss: 161270004102.2428\n",
      "Epoch [1220/5000], Train Loss: 340927719875.6448, Val Loss: 158985869439.2534\n",
      "Epoch [1240/5000], Train Loss: 408402356839.7590, Val Loss: 156346423052.1809\n",
      "Epoch [1260/5000], Train Loss: 273568657466.7796, Val Loss: 153153279502.8902\n",
      "Epoch [1280/5000], Train Loss: 438618278101.2894, Val Loss: 157977541338.6911\n",
      "Epoch [1300/5000], Train Loss: 182180819831.1146, Val Loss: 335942350559.7791\n",
      "Epoch [1320/5000], Train Loss: 600079092501.0127, Val Loss: 162179524124.1472\n",
      "Epoch [1340/5000], Train Loss: 161295828092.0106, Val Loss: 164489711337.9652\n",
      "Epoch [1360/5000], Train Loss: 280236948031.3458, Val Loss: 197623157378.7950\n",
      "Epoch [1380/5000], Train Loss: 378158943549.1440, Val Loss: 239789491803.0433\n",
      "Epoch [1400/5000], Train Loss: 1511699936366.0166, Val Loss: 163236362924.5204\n",
      "Epoch [1420/5000], Train Loss: 177793676422.4673, Val Loss: 160688850697.4459\n",
      "Epoch [1440/5000], Train Loss: 470293131960.6547, Val Loss: 480261602664.7347\n",
      "Epoch [1460/5000], Train Loss: 257227467929.9232, Val Loss: 150571040524.5527\n",
      "Epoch [1480/5000], Train Loss: 327855050188.4140, Val Loss: 170095168365.0144\n",
      "Epoch [1500/5000], Train Loss: 180701047155.0259, Val Loss: 156329406571.0696\n",
      "Epoch [1520/5000], Train Loss: 249048277192.1565, Val Loss: 174232461487.2408\n",
      "Epoch [1540/5000], Train Loss: 407502510236.7866, Val Loss: 160657399182.4318\n",
      "Epoch [1560/5000], Train Loss: 435866065826.1501, Val Loss: 157327240844.2906\n",
      "Epoch [1580/5000], Train Loss: 185080003597.2584, Val Loss: 456899311134.3022\n",
      "Epoch [1600/5000], Train Loss: 186895259220.1428, Val Loss: 149919792085.5370\n",
      "Epoch [1620/5000], Train Loss: 218914110580.0076, Val Loss: 209391685516.4694\n",
      "Epoch [1640/5000], Train Loss: 173283067681.9436, Val Loss: 226866991274.6605\n",
      "Epoch [1660/5000], Train Loss: 116389398119.8762, Val Loss: 120774302485.7693\n",
      "Epoch [1680/5000], Train Loss: 255350540915.6737, Val Loss: 147325159311.0593\n",
      "Epoch [1700/5000], Train Loss: 265217647173.4481, Val Loss: 132118194757.4898\n",
      "Epoch [1720/5000], Train Loss: 348320779819.9106, Val Loss: 130570847948.0623\n",
      "Epoch [1740/5000], Train Loss: 382942087625.4263, Val Loss: 182055565578.6056\n",
      "Epoch [1760/5000], Train Loss: 369346128269.9402, Val Loss: 158466243434.6023\n",
      "Epoch [1780/5000], Train Loss: 615187643825.3008, Val Loss: 148846532167.9889\n",
      "Epoch [1800/5000], Train Loss: 235641597997.5115, Val Loss: 164898843405.3749\n",
      "Epoch [1820/5000], Train Loss: 1304861236220.0593, Val Loss: 183171415612.1494\n",
      "Epoch [1840/5000], Train Loss: 1313048587686.5400, Val Loss: 182610825534.8901\n",
      "Epoch [1860/5000], Train Loss: 265806211951.5658, Val Loss: 153770582471.8841\n",
      "Epoch [1880/5000], Train Loss: 274399119442.6274, Val Loss: 154571828393.5886\n",
      "Epoch [1900/5000], Train Loss: 170884624362.9233, Val Loss: 318200571720.1273\n",
      "Epoch [1920/5000], Train Loss: 567415686841.3429, Val Loss: 474490560097.6917\n",
      "Epoch [1940/5000], Train Loss: 301605903445.1465, Val Loss: 193093447341.5302\n",
      "Epoch [1960/5000], Train Loss: 346144765852.7108, Val Loss: 209056504309.0088\n",
      "Epoch [1980/5000], Train Loss: 225549289338.3614, Val Loss: 193685756984.5243\n",
      "Epoch [2000/5000], Train Loss: 108695149673.8280, Val Loss: 159723849285.8410\n",
      "Epoch [2020/5000], Train Loss: 99652168968.5724, Val Loss: 198383084556.3673\n",
      "Epoch [2040/5000], Train Loss: 123848923284.7739, Val Loss: 175355131343.4693\n",
      "Epoch [2060/5000], Train Loss: 891114356873.8439, Val Loss: 183077331230.0586\n",
      "Epoch [2080/5000], Train Loss: 91933117173.0930, Val Loss: 137367207993.8368\n",
      "Epoch [2100/5000], Train Loss: 173099445383.3430, Val Loss: 137783710750.9943\n",
      "Epoch [2120/5000], Train Loss: 259345440851.5929, Val Loss: 143446244625.3513\n",
      "Epoch [2140/5000], Train Loss: 247857417261.0947, Val Loss: 132027197326.0068\n",
      "Epoch [2160/5000], Train Loss: 127193280445.5429, Val Loss: 191989689500.8571\n",
      "Early stopping at epoch 2170\n",
      "\n",
      "Fold number: 1\n",
      "[0.565173929447481]\n",
      "MAPE =  0.565173929447481\n",
      "\n",
      "Epoch [20/5000], Train Loss: 45226277778055.2422, Val Loss: 15874427911186.0352\n",
      "Epoch [40/5000], Train Loss: 6073194036133.8379, Val Loss: 2310797818440.3911\n",
      "Epoch [60/5000], Train Loss: 3125509265270.7129, Val Loss: 1011932024554.8749\n",
      "Epoch [80/5000], Train Loss: 7612165695117.9551, Val Loss: 597510780404.2054\n",
      "Epoch [100/5000], Train Loss: 9471972769584.1094, Val Loss: 614211425275.6389\n",
      "Epoch [120/5000], Train Loss: 4767886243567.6865, Val Loss: 321973195138.8480\n",
      "Epoch [140/5000], Train Loss: 905294896328.1018, Val Loss: 305727791714.8513\n",
      "Epoch [160/5000], Train Loss: 326935347215.6202, Val Loss: 207295141674.1769\n",
      "Epoch [180/5000], Train Loss: 512697424457.5223, Val Loss: 139398021141.0417\n",
      "Epoch [200/5000], Train Loss: 363007518717.2901, Val Loss: 157683466986.5715\n",
      "Epoch [220/5000], Train Loss: 487007366711.0089, Val Loss: 143868752516.7567\n",
      "Epoch [240/5000], Train Loss: 234240418797.4042, Val Loss: 120972506220.3311\n",
      "Epoch [260/5000], Train Loss: 547656100532.8574, Val Loss: 122198104062.9526\n",
      "Epoch [280/5000], Train Loss: 917454461075.8683, Val Loss: 304019303547.3505\n",
      "Epoch [300/5000], Train Loss: 858094381945.8516, Val Loss: 111303089691.8014\n",
      "Epoch [320/5000], Train Loss: 860752123234.8267, Val Loss: 111385722729.8862\n",
      "Epoch [340/5000], Train Loss: 118736038181.9297, Val Loss: 111547865805.9767\n",
      "Epoch [360/5000], Train Loss: 523867257802.6362, Val Loss: 97026706426.8325\n",
      "Epoch [380/5000], Train Loss: 347964723282.4221, Val Loss: 96852337093.5525\n",
      "Epoch [400/5000], Train Loss: 252332125139.3164, Val Loss: 100423556511.9453\n",
      "Epoch [420/5000], Train Loss: 672128779283.3694, Val Loss: 100601620365.5762\n",
      "Epoch [440/5000], Train Loss: 523559021665.2037, Val Loss: 100450280348.1569\n",
      "Epoch [460/5000], Train Loss: 448574249597.2064, Val Loss: 262175254312.1872\n",
      "Epoch [480/5000], Train Loss: 264941699993.4995, Val Loss: 113896836537.5356\n",
      "Epoch [500/5000], Train Loss: 757174203941.7289, Val Loss: 93153770949.4505\n",
      "Epoch [520/5000], Train Loss: 296286413043.5649, Val Loss: 210597332622.1806\n",
      "Epoch [540/5000], Train Loss: 232471179161.8718, Val Loss: 119583964287.0741\n",
      "Epoch [560/5000], Train Loss: 214906293057.6059, Val Loss: 105209036617.6496\n",
      "Epoch [580/5000], Train Loss: 203622683461.5378, Val Loss: 105595100213.2795\n",
      "Epoch [600/5000], Train Loss: 258412662559.3148, Val Loss: 101058634185.8673\n",
      "Epoch [620/5000], Train Loss: 265782043504.4491, Val Loss: 111917522653.5690\n",
      "Epoch [640/5000], Train Loss: 141015216844.9473, Val Loss: 101109562255.9716\n",
      "Epoch [660/5000], Train Loss: 520632323321.3304, Val Loss: 100508120104.4538\n",
      "Epoch [680/5000], Train Loss: 224755464372.0153, Val Loss: 120333845132.1911\n",
      "Epoch [700/5000], Train Loss: 211458041626.4024, Val Loss: 104248620933.8753\n",
      "Epoch [720/5000], Train Loss: 133444470006.9724, Val Loss: 105650346763.2929\n",
      "Epoch [740/5000], Train Loss: 209528290289.0287, Val Loss: 98516411665.8843\n",
      "Epoch [760/5000], Train Loss: 375014774537.9223, Val Loss: 97265550254.4608\n",
      "Epoch [780/5000], Train Loss: 492677430771.1655, Val Loss: 112644450179.6820\n",
      "Epoch [800/5000], Train Loss: 225489325788.1414, Val Loss: 103982181758.2929\n",
      "Epoch [820/5000], Train Loss: 347109791290.8316, Val Loss: 97726453417.8487\n",
      "Epoch [840/5000], Train Loss: 347985965827.0316, Val Loss: 102882339450.2179\n",
      "Epoch [860/5000], Train Loss: 622088453908.3087, Val Loss: 99131881908.8375\n",
      "Epoch [880/5000], Train Loss: 120980537151.8583, Val Loss: 99021380235.4112\n",
      "Epoch [900/5000], Train Loss: 283960802123.5499, Val Loss: 96832889675.0615\n",
      "Epoch [920/5000], Train Loss: 1797562469986.3350, Val Loss: 138706424324.5297\n",
      "Epoch [940/5000], Train Loss: 249247497422.8125, Val Loss: 101060128389.7501\n",
      "Epoch [960/5000], Train Loss: 307417681324.8022, Val Loss: 218828876977.3254\n",
      "Epoch [980/5000], Train Loss: 290671482940.6724, Val Loss: 104031582791.5250\n",
      "Epoch [1000/5000], Train Loss: 1867702109683.2146, Val Loss: 110705679948.3651\n",
      "Epoch [1020/5000], Train Loss: 258947635922.5979, Val Loss: 100483064677.0992\n",
      "Epoch [1040/5000], Train Loss: 163076826139.7346, Val Loss: 100044896412.0917\n",
      "Epoch [1060/5000], Train Loss: 101126836226.6712, Val Loss: 92043373817.5732\n",
      "Epoch [1080/5000], Train Loss: 373488850084.3787, Val Loss: 106394316423.7621\n",
      "Epoch [1100/5000], Train Loss: 508725993096.4069, Val Loss: 317663863469.7274\n",
      "Epoch [1120/5000], Train Loss: 388881544989.4811, Val Loss: 98056788380.4760\n",
      "Epoch [1140/5000], Train Loss: 159500711986.9247, Val Loss: 99387958468.4663\n",
      "Epoch [1160/5000], Train Loss: 470505919238.6437, Val Loss: 292542677845.2073\n",
      "Epoch [1180/5000], Train Loss: 669018501022.9939, Val Loss: 82039597869.2745\n",
      "Epoch [1200/5000], Train Loss: 122445132411.6366, Val Loss: 103635333171.4010\n",
      "Epoch [1220/5000], Train Loss: 303729783278.9686, Val Loss: 99296645145.9271\n",
      "Epoch [1240/5000], Train Loss: 496562651936.6343, Val Loss: 296753377794.1791\n",
      "Epoch [1260/5000], Train Loss: 242006132026.0642, Val Loss: 104695556771.8199\n",
      "Epoch [1280/5000], Train Loss: 336131195716.7285, Val Loss: 101402070399.9328\n",
      "Epoch [1300/5000], Train Loss: 341247051422.5755, Val Loss: 209886769196.3485\n",
      "Epoch [1320/5000], Train Loss: 647412515485.7390, Val Loss: 99572660522.8376\n",
      "Epoch [1340/5000], Train Loss: 258870783601.0594, Val Loss: 122891764657.9482\n",
      "Epoch [1360/5000], Train Loss: 684584739587.2313, Val Loss: 94829459157.7851\n",
      "Epoch [1380/5000], Train Loss: 508296138473.1276, Val Loss: 192052719027.1862\n",
      "Epoch [1400/5000], Train Loss: 2249164640212.5146, Val Loss: 144791108514.5183\n",
      "Epoch [1420/5000], Train Loss: 143696343969.2719, Val Loss: 193970113951.7526\n",
      "Epoch [1440/5000], Train Loss: 86400602768.2999, Val Loss: 96261827334.3505\n",
      "Epoch [1460/5000], Train Loss: 538029095450.8841, Val Loss: 291681368987.2426\n",
      "Epoch [1480/5000], Train Loss: 408038455234.3903, Val Loss: 100490072147.8575\n",
      "Epoch [1500/5000], Train Loss: 194889934384.2036, Val Loss: 97009060578.8075\n",
      "Epoch [1520/5000], Train Loss: 200207062511.0192, Val Loss: 98477022427.0099\n",
      "Epoch [1540/5000], Train Loss: 165505472297.8536, Val Loss: 278651296052.8517\n",
      "Epoch [1560/5000], Train Loss: 540600493666.4885, Val Loss: 96324052478.2234\n",
      "Epoch [1580/5000], Train Loss: 129380044882.2034, Val Loss: 288030890755.4030\n",
      "Epoch [1600/5000], Train Loss: 641333594138.9565, Val Loss: 228410951701.8253\n",
      "Epoch [1620/5000], Train Loss: 275259821000.7098, Val Loss: 108931093383.0814\n",
      "Epoch [1640/5000], Train Loss: 221872569232.9485, Val Loss: 94839512438.7596\n",
      "Epoch [1660/5000], Train Loss: 232745903830.7541, Val Loss: 89706859322.7049\n",
      "Epoch [1680/5000], Train Loss: 345458166432.5988, Val Loss: 87518817574.9208\n",
      "Epoch [1700/5000], Train Loss: 292197547749.1067, Val Loss: 79163111318.5551\n",
      "Epoch [1720/5000], Train Loss: 352586607504.3495, Val Loss: 111283531186.8112\n",
      "Epoch [1740/5000], Train Loss: 416605681949.5665, Val Loss: 198212456987.7725\n",
      "Epoch [1760/5000], Train Loss: 322978866277.3102, Val Loss: 100423063544.9870\n",
      "Epoch [1780/5000], Train Loss: 421950676483.6675, Val Loss: 98622889011.5765\n",
      "Epoch [1800/5000], Train Loss: 263951018832.0691, Val Loss: 88080058135.0995\n",
      "Epoch [1820/5000], Train Loss: 282167410005.2831, Val Loss: 78877325143.0697\n",
      "Epoch [1840/5000], Train Loss: 886278616629.1007, Val Loss: 91640750773.3275\n",
      "Epoch [1860/5000], Train Loss: 171843175381.7015, Val Loss: 89237662703.7144\n",
      "Epoch [1880/5000], Train Loss: 198251185941.6705, Val Loss: 87824849671.9811\n",
      "Epoch [1900/5000], Train Loss: 101258170687.1967, Val Loss: 123943286581.9212\n",
      "Epoch [1920/5000], Train Loss: 519364456976.5364, Val Loss: 92026260071.0836\n",
      "Epoch [1940/5000], Train Loss: 296041451577.9169, Val Loss: 91469968833.2569\n",
      "Epoch [1960/5000], Train Loss: 394599095398.6438, Val Loss: 99846046541.7279\n",
      "Epoch [1980/5000], Train Loss: 229818399045.9651, Val Loss: 159691208900.2552\n",
      "Epoch [2000/5000], Train Loss: 138963207960.0852, Val Loss: 92556664508.3784\n",
      "Epoch [2020/5000], Train Loss: 345080293095.0952, Val Loss: 88924614329.7548\n",
      "Epoch [2040/5000], Train Loss: 260951069331.6806, Val Loss: 91923279306.2593\n",
      "Epoch [2060/5000], Train Loss: 1102686508977.6875, Val Loss: 89871029386.7187\n",
      "Epoch [2080/5000], Train Loss: 139797188659.5916, Val Loss: 93628144459.4879\n",
      "Epoch [2100/5000], Train Loss: 262881584470.1167, Val Loss: 261062557276.8564\n",
      "Epoch [2120/5000], Train Loss: 248440393018.7248, Val Loss: 91965228191.2711\n",
      "Epoch [2140/5000], Train Loss: 176860670704.1056, Val Loss: 187289277687.4719\n",
      "Epoch [2160/5000], Train Loss: 324253834697.2419, Val Loss: 93402510826.9811\n",
      "Epoch [2180/5000], Train Loss: 420976243685.1933, Val Loss: 93486480591.2016\n",
      "Early stopping at epoch 2199\n",
      "\n",
      "Fold number: 2\n",
      "[0.5147168241618453]\n",
      "MAPE =  0.5147168241618453\n",
      "\n",
      "Epoch [20/5000], Train Loss: 45068852154250.6797, Val Loss: 11094505118473.9180\n",
      "Epoch [40/5000], Train Loss: 7009638406195.9102, Val Loss: 2705620899440.2944\n",
      "Epoch [60/5000], Train Loss: 3273788663915.5322, Val Loss: 868474664880.9763\n",
      "Epoch [80/5000], Train Loss: 7600013796145.2256, Val Loss: 549611612500.8281\n",
      "Epoch [100/5000], Train Loss: 2328042878159.0024, Val Loss: 419986039115.2850\n",
      "Epoch [120/5000], Train Loss: 4890067353883.8398, Val Loss: 386314394872.6221\n",
      "Epoch [140/5000], Train Loss: 2362445548381.8228, Val Loss: 362341818367.7523\n",
      "Epoch [160/5000], Train Loss: 526968733363.7444, Val Loss: 251752480614.4569\n",
      "Epoch [180/5000], Train Loss: 312932026405.3304, Val Loss: 181143616348.3013\n",
      "Epoch [200/5000], Train Loss: 456266436074.4175, Val Loss: 155741180784.0362\n",
      "Epoch [220/5000], Train Loss: 602875554185.0522, Val Loss: 151908491774.6053\n",
      "Epoch [240/5000], Train Loss: 181232842938.1224, Val Loss: 142186852938.3117\n",
      "Epoch [260/5000], Train Loss: 616208210000.3022, Val Loss: 154286111358.6739\n",
      "Epoch [280/5000], Train Loss: 2035401968318.7671, Val Loss: 135519582762.2426\n",
      "Epoch [300/5000], Train Loss: 587039900302.0831, Val Loss: 329199571604.8582\n",
      "Epoch [320/5000], Train Loss: 700808565190.7819, Val Loss: 172747309071.4845\n",
      "Epoch [340/5000], Train Loss: 105608368417.6696, Val Loss: 247024995196.4858\n",
      "Epoch [360/5000], Train Loss: 327042681695.8581, Val Loss: 189262903551.0909\n",
      "Epoch [380/5000], Train Loss: 364330770051.2773, Val Loss: 217927081082.2965\n",
      "Epoch [400/5000], Train Loss: 141763048289.6398, Val Loss: 265591813982.9550\n",
      "Epoch [420/5000], Train Loss: 631068628467.4047, Val Loss: 172629302746.7952\n",
      "Epoch [440/5000], Train Loss: 166539423625.8088, Val Loss: 156206115571.7944\n",
      "Epoch [460/5000], Train Loss: 448125602293.0325, Val Loss: 272849829148.7690\n",
      "Epoch [480/5000], Train Loss: 242281697326.5459, Val Loss: 148044053311.9661\n",
      "Epoch [500/5000], Train Loss: 533738097811.4285, Val Loss: 235618653625.1580\n",
      "Epoch [520/5000], Train Loss: 177485470334.6744, Val Loss: 160093739550.4999\n",
      "Epoch [540/5000], Train Loss: 237457764023.4891, Val Loss: 164573440070.5636\n",
      "Epoch [560/5000], Train Loss: 188777716741.3636, Val Loss: 154096456310.5632\n",
      "Epoch [580/5000], Train Loss: 209163585801.4998, Val Loss: 127783192525.1822\n",
      "Epoch [600/5000], Train Loss: 130489106036.8792, Val Loss: 120624150505.4916\n",
      "Epoch [620/5000], Train Loss: 135505907042.5210, Val Loss: 216797987303.2803\n",
      "Epoch [640/5000], Train Loss: 307470263683.5798, Val Loss: 135654671554.8314\n",
      "Epoch [660/5000], Train Loss: 466793634976.5823, Val Loss: 108386238843.7677\n",
      "Epoch [680/5000], Train Loss: 281362973508.9302, Val Loss: 112157672198.2205\n",
      "Epoch [700/5000], Train Loss: 225147301380.6202, Val Loss: 96677533487.8652\n",
      "Epoch [720/5000], Train Loss: 134793247612.1907, Val Loss: 115746678873.4639\n",
      "Epoch [740/5000], Train Loss: 272752353183.7392, Val Loss: 107728212326.0121\n",
      "Epoch [760/5000], Train Loss: 258783398559.4076, Val Loss: 109413337884.4583\n",
      "Epoch [780/5000], Train Loss: 366566381055.4404, Val Loss: 114339840685.6547\n",
      "Epoch [800/5000], Train Loss: 75875599958.8043, Val Loss: 168291050512.5708\n",
      "Epoch [820/5000], Train Loss: 149323481037.4679, Val Loss: 133829426781.5582\n",
      "Epoch [840/5000], Train Loss: 325207158441.9364, Val Loss: 105346719171.0949\n",
      "Epoch [860/5000], Train Loss: 947420102489.3182, Val Loss: 100586333741.6265\n",
      "Epoch [880/5000], Train Loss: 528502908261.2832, Val Loss: 99947152071.5575\n",
      "Epoch [900/5000], Train Loss: 452192273019.3698, Val Loss: 328677938275.0153\n",
      "Epoch [920/5000], Train Loss: 761294345895.6979, Val Loss: 108776854580.8505\n",
      "Epoch [940/5000], Train Loss: 156150153486.9350, Val Loss: 207079687941.3533\n",
      "Epoch [960/5000], Train Loss: 264861934857.8754, Val Loss: 121910406105.5368\n",
      "Epoch [980/5000], Train Loss: 232220945804.0633, Val Loss: 107898681198.2765\n",
      "Epoch [1000/5000], Train Loss: 1521391710115.9214, Val Loss: 106477446614.0097\n",
      "Epoch [1020/5000], Train Loss: 301781130830.7932, Val Loss: 135062628997.1337\n",
      "Epoch [1040/5000], Train Loss: 91585840563.8588, Val Loss: 110481518662.6456\n",
      "Epoch [1060/5000], Train Loss: 79020941793.0715, Val Loss: 91846217556.6998\n",
      "Epoch [1080/5000], Train Loss: 439019708409.6849, Val Loss: 130572500888.1132\n",
      "Epoch [1100/5000], Train Loss: 291690893233.3567, Val Loss: 122879314801.3014\n",
      "Epoch [1120/5000], Train Loss: 416793491613.3865, Val Loss: 104925346131.5700\n",
      "Epoch [1140/5000], Train Loss: 130449469193.0660, Val Loss: 134344952379.3910\n",
      "Epoch [1160/5000], Train Loss: 268908557475.9607, Val Loss: 123637374960.7679\n",
      "Epoch [1180/5000], Train Loss: 180029401370.1863, Val Loss: 109646964463.8383\n",
      "Epoch [1200/5000], Train Loss: 99674316271.5979, Val Loss: 148384622824.7770\n",
      "Epoch [1220/5000], Train Loss: 168032145760.5637, Val Loss: 92306894916.3458\n",
      "Epoch [1240/5000], Train Loss: 643189728591.2294, Val Loss: 315474301279.0113\n",
      "Epoch [1260/5000], Train Loss: 392338480143.8056, Val Loss: 93978387607.2631\n",
      "Epoch [1280/5000], Train Loss: 479361390104.1263, Val Loss: 105233356565.1594\n",
      "Epoch [1300/5000], Train Loss: 157418886101.9667, Val Loss: 197224670281.5323\n",
      "Epoch [1320/5000], Train Loss: 1139979535903.4519, Val Loss: 346931313733.5215\n",
      "Epoch [1340/5000], Train Loss: 233538411439.3808, Val Loss: 305894074707.2162\n",
      "Epoch [1360/5000], Train Loss: 631096787712.0334, Val Loss: 103112846049.1681\n",
      "Epoch [1380/5000], Train Loss: 187146622297.3706, Val Loss: 110596991321.6890\n",
      "Epoch [1400/5000], Train Loss: 147782354410.9157, Val Loss: 96150243708.8295\n",
      "Epoch [1420/5000], Train Loss: 142573642532.8159, Val Loss: 141987416899.0565\n",
      "Epoch [1440/5000], Train Loss: 255125435299.3039, Val Loss: 167454899204.2092\n",
      "Epoch [1460/5000], Train Loss: 199078796017.4184, Val Loss: 113959394812.9166\n",
      "Epoch [1480/5000], Train Loss: 341042685451.5825, Val Loss: 98061474860.0636\n",
      "Epoch [1500/5000], Train Loss: 545066252579.8542, Val Loss: 160393868355.6547\n",
      "Epoch [1520/5000], Train Loss: 542052132907.1207, Val Loss: 329211897010.5998\n",
      "Epoch [1540/5000], Train Loss: 234484320907.0879, Val Loss: 97263355185.2927\n",
      "Epoch [1560/5000], Train Loss: 303883231339.5491, Val Loss: 343612327844.0451\n",
      "Early stopping at epoch 1566\n",
      "\n",
      "Fold number: 3\n",
      "[0.7075233089122148]\n",
      "MAPE =  0.7075233089122148\n",
      "\n",
      "Epoch [20/5000], Train Loss: 17567283916204.3184, Val Loss: 4313579457373.3896\n",
      "Epoch [40/5000], Train Loss: 36337827462610.4688, Val Loss: 1976201653586.7468\n",
      "Epoch [60/5000], Train Loss: 7373363629196.0576, Val Loss: 916417880000.7793\n",
      "Epoch [80/5000], Train Loss: 4334752215694.8301, Val Loss: 616570420597.8831\n",
      "Epoch [100/5000], Train Loss: 10199089373078.9980, Val Loss: 511863550318.2612\n",
      "Epoch [120/5000], Train Loss: 2096276762312.3740, Val Loss: 404965796569.6028\n",
      "Epoch [140/5000], Train Loss: 3246237937898.9316, Val Loss: 301985955480.0463\n",
      "Epoch [160/5000], Train Loss: 638518151057.6080, Val Loss: 222106061830.9094\n",
      "Epoch [180/5000], Train Loss: 418982963955.1718, Val Loss: 191771259279.4139\n",
      "Epoch [200/5000], Train Loss: 217169061336.6394, Val Loss: 174695161587.9478\n",
      "Epoch [220/5000], Train Loss: 485751785259.1653, Val Loss: 158445441369.9195\n",
      "Epoch [240/5000], Train Loss: 303619225485.4417, Val Loss: 160173982471.5871\n",
      "Epoch [260/5000], Train Loss: 347727780763.5220, Val Loss: 147960755364.4486\n",
      "Epoch [280/5000], Train Loss: 2357042590846.4551, Val Loss: 282287019128.7039\n",
      "Epoch [300/5000], Train Loss: 129326037741.6343, Val Loss: 163981746006.9305\n",
      "Epoch [320/5000], Train Loss: 379747444996.6750, Val Loss: 139584002393.1832\n",
      "Epoch [340/5000], Train Loss: 2127373650491.6379, Val Loss: 138654649647.9637\n",
      "Epoch [360/5000], Train Loss: 264758767788.8641, Val Loss: 122140959449.2073\n",
      "Epoch [380/5000], Train Loss: 269909624011.9901, Val Loss: 274270224071.1878\n",
      "Epoch [400/5000], Train Loss: 148367054733.6386, Val Loss: 134545406505.8808\n",
      "Epoch [420/5000], Train Loss: 630523985810.3314, Val Loss: 136354404466.1036\n",
      "Epoch [440/5000], Train Loss: 203461276547.0750, Val Loss: 133148232695.5124\n",
      "Epoch [460/5000], Train Loss: 444265383964.0032, Val Loss: 170740939143.8547\n",
      "Epoch [480/5000], Train Loss: 546162736193.6639, Val Loss: 132025378531.4641\n",
      "Epoch [500/5000], Train Loss: 398175491378.5456, Val Loss: 239918793073.2930\n",
      "Epoch [520/5000], Train Loss: 380770774393.6050, Val Loss: 138844158997.9714\n",
      "Epoch [540/5000], Train Loss: 418321000736.9605, Val Loss: 297064091501.6378\n",
      "Epoch [560/5000], Train Loss: 251168673102.6269, Val Loss: 141739502319.0305\n",
      "Epoch [580/5000], Train Loss: 505261888743.1533, Val Loss: 152519085999.5702\n",
      "Epoch [600/5000], Train Loss: 184300978489.8434, Val Loss: 144264099257.3518\n",
      "Epoch [620/5000], Train Loss: 157495106209.0435, Val Loss: 159618238094.6406\n",
      "Epoch [640/5000], Train Loss: 274588013597.1083, Val Loss: 141229648378.0555\n",
      "Epoch [660/5000], Train Loss: 480677903166.0012, Val Loss: 263225335760.0452\n",
      "Epoch [680/5000], Train Loss: 243637398603.8275, Val Loss: 130462649627.6187\n",
      "Epoch [700/5000], Train Loss: 214503946624.8993, Val Loss: 136738559935.1871\n",
      "Epoch [720/5000], Train Loss: 147270364244.4577, Val Loss: 141667798552.8160\n",
      "Epoch [740/5000], Train Loss: 255812139049.5290, Val Loss: 138906287818.3526\n",
      "Epoch [760/5000], Train Loss: 741712320856.8105, Val Loss: 194186827962.9417\n",
      "Epoch [780/5000], Train Loss: 307595404752.4308, Val Loss: 268630185470.6776\n",
      "Epoch [800/5000], Train Loss: 210695170903.5686, Val Loss: 135655007560.1949\n",
      "Epoch [820/5000], Train Loss: 418267520396.3381, Val Loss: 154179548464.9172\n",
      "Epoch [840/5000], Train Loss: 326792014240.6212, Val Loss: 130940167148.5914\n",
      "Epoch [860/5000], Train Loss: 472591387923.8088, Val Loss: 156801316220.4628\n",
      "Epoch [880/5000], Train Loss: 128094565391.5504, Val Loss: 130118536642.1056\n",
      "Epoch [900/5000], Train Loss: 457399994376.7869, Val Loss: 126093428298.9198\n",
      "Epoch [920/5000], Train Loss: 128572548589.7183, Val Loss: 129498943471.5805\n",
      "Epoch [940/5000], Train Loss: 306249633144.7958, Val Loss: 184239403401.2290\n",
      "Epoch [960/5000], Train Loss: 316782154336.3513, Val Loss: 99956479598.1693\n",
      "Epoch [980/5000], Train Loss: 1396125197275.6755, Val Loss: 114174185850.7600\n",
      "Epoch [1000/5000], Train Loss: 111233616770.7494, Val Loss: 116118304291.4332\n",
      "Epoch [1020/5000], Train Loss: 201078482532.0345, Val Loss: 103857539640.4008\n",
      "Epoch [1040/5000], Train Loss: 173332644293.3273, Val Loss: 110395966740.3230\n",
      "Epoch [1060/5000], Train Loss: 363314397633.9794, Val Loss: 128691775307.1297\n",
      "Epoch [1080/5000], Train Loss: 200993145262.7097, Val Loss: 146262637969.8493\n",
      "Epoch [1100/5000], Train Loss: 790152928851.7301, Val Loss: 142624794298.4084\n",
      "Epoch [1120/5000], Train Loss: 164216451099.3436, Val Loss: 144690975296.3339\n",
      "Epoch [1140/5000], Train Loss: 497293667639.4197, Val Loss: 149423341223.5928\n",
      "Epoch [1160/5000], Train Loss: 543499331009.8050, Val Loss: 136129981101.5552\n",
      "Epoch [1180/5000], Train Loss: 229137713363.1856, Val Loss: 171330624263.9804\n",
      "Epoch [1200/5000], Train Loss: 186477299829.9643, Val Loss: 127120697274.6831\n",
      "Epoch [1220/5000], Train Loss: 269728991984.8253, Val Loss: 140972448350.5984\n",
      "Epoch [1240/5000], Train Loss: 168360131895.7409, Val Loss: 135823087721.2493\n",
      "Epoch [1260/5000], Train Loss: 69718755499.9855, Val Loss: 135517568284.9036\n",
      "Epoch [1280/5000], Train Loss: 451187278696.2278, Val Loss: 252629627190.2247\n",
      "Epoch [1300/5000], Train Loss: 269440807574.1524, Val Loss: 203181900732.5551\n",
      "Epoch [1320/5000], Train Loss: 124801199605.2722, Val Loss: 138574706837.6885\n",
      "Epoch [1340/5000], Train Loss: 907967765648.0288, Val Loss: 277427561363.7278\n",
      "Epoch [1360/5000], Train Loss: 133662010564.4183, Val Loss: 157999106757.7116\n",
      "Epoch [1380/5000], Train Loss: 563571047629.8473, Val Loss: 164106051451.0356\n",
      "Epoch [1400/5000], Train Loss: 100911329533.1496, Val Loss: 252572731587.7746\n",
      "Epoch [1420/5000], Train Loss: 255458186013.1656, Val Loss: 162605868681.1750\n",
      "Epoch [1440/5000], Train Loss: 321480269774.5898, Val Loss: 189792000387.0621\n",
      "Epoch [1460/5000], Train Loss: 605726015568.3958, Val Loss: 245601699551.3078\n",
      "Early stopping at epoch 1474\n",
      "\n",
      "Fold number: 4\n",
      "[0.6395216966076303]\n",
      "MAPE =  0.6395216966076303\n",
      "-\n",
      "mape score =  [[0.5227127721107208], [0.565173929447481], [0.5147168241618453], [0.7075233089122148], [0.6395216966076303]]\n",
      ">>>>>>>>>>>>>>>>>>>>000753_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch [20/5000], Train Loss: 6435549194757.8105, Val Loss: 1380050393808.4399\n",
      "Epoch [40/5000], Train Loss: 2000564188925.0740, Val Loss: 734751411755.9198\n",
      "Epoch [60/5000], Train Loss: 1310697617071.2339, Val Loss: 273331489114.6788\n",
      "Epoch [80/5000], Train Loss: 431242198579.5215, Val Loss: 190571714068.3135\n",
      "Epoch [100/5000], Train Loss: 174537455442.6138, Val Loss: 129097164184.1607\n",
      "Epoch [120/5000], Train Loss: 650328423298.7280, Val Loss: 100295692670.7277\n",
      "Epoch [140/5000], Train Loss: 203274739473.5806, Val Loss: 82148127828.3694\n",
      "Epoch [160/5000], Train Loss: 126226855692.7791, Val Loss: 76436061111.3535\n",
      "Epoch [180/5000], Train Loss: 77528220752.2510, Val Loss: 57667874107.3431\n",
      "Epoch [200/5000], Train Loss: 474045423860.6990, Val Loss: 52973381147.6350\n",
      "Epoch [220/5000], Train Loss: 68316554857.3071, Val Loss: 53351134675.9492\n",
      "Epoch [240/5000], Train Loss: 80788794664.2583, Val Loss: 46029670008.4119\n",
      "Epoch [260/5000], Train Loss: 128008636009.9356, Val Loss: 36738577728.8518\n",
      "Epoch [280/5000], Train Loss: 48120872885.7453, Val Loss: 34420083072.0140\n",
      "Epoch [300/5000], Train Loss: 92460557386.2319, Val Loss: 36645313695.0862\n",
      "Epoch [320/5000], Train Loss: 560782140781.2072, Val Loss: 40412912031.3533\n",
      "Epoch [340/5000], Train Loss: 65127435441.6385, Val Loss: 52163851567.1191\n",
      "Epoch [360/5000], Train Loss: 73041286598.1916, Val Loss: 45477071559.5296\n",
      "Epoch [380/5000], Train Loss: 116901804133.3352, Val Loss: 32336624289.9184\n",
      "Epoch [400/5000], Train Loss: 178945793670.6273, Val Loss: 31602625943.6500\n",
      "Epoch [420/5000], Train Loss: 110609898624.0015, Val Loss: 33451562968.8108\n",
      "Epoch [440/5000], Train Loss: 1715313446550.3999, Val Loss: 31219203665.8026\n",
      "Epoch [460/5000], Train Loss: 144429468105.1489, Val Loss: 57024573538.0070\n",
      "Epoch [480/5000], Train Loss: 97107558344.6277, Val Loss: 33336418462.9006\n",
      "Epoch [500/5000], Train Loss: 33908652121.8035, Val Loss: 28324065841.7800\n",
      "Epoch [520/5000], Train Loss: 103858346445.4374, Val Loss: 92881548301.7703\n",
      "Epoch [540/5000], Train Loss: 162955184000.0879, Val Loss: 30214739002.2461\n",
      "Epoch [560/5000], Train Loss: 267173149194.3873, Val Loss: 33310117603.8931\n",
      "Epoch [580/5000], Train Loss: 48644847412.1412, Val Loss: 31096282229.9847\n",
      "Epoch [600/5000], Train Loss: 11526021263.8160, Val Loss: 35612667427.3374\n",
      "Epoch [620/5000], Train Loss: 405621472826.4311, Val Loss: 29249496683.2004\n",
      "Epoch [640/5000], Train Loss: 194351987709.9792, Val Loss: 90916720478.4483\n",
      "Epoch [660/5000], Train Loss: 61029804685.9463, Val Loss: 29129791669.1504\n",
      "Epoch [680/5000], Train Loss: 31667892287.2173, Val Loss: 32309936175.0200\n",
      "Epoch [700/5000], Train Loss: 154386488404.0265, Val Loss: 31144654849.5232\n",
      "Epoch [720/5000], Train Loss: 103454705363.9143, Val Loss: 30058294415.1343\n",
      "Epoch [740/5000], Train Loss: 111181421865.4164, Val Loss: 27609821350.5254\n",
      "Epoch [760/5000], Train Loss: 75069208763.8179, Val Loss: 30165870652.6612\n",
      "Epoch [780/5000], Train Loss: 597916877864.3929, Val Loss: 36936943951.8178\n",
      "Epoch [800/5000], Train Loss: 79240295612.4769, Val Loss: 29408861836.5846\n",
      "Epoch [820/5000], Train Loss: 135979623018.5480, Val Loss: 29444082488.7385\n",
      "Epoch [840/5000], Train Loss: 43405334906.2487, Val Loss: 26857467421.1936\n",
      "Epoch [860/5000], Train Loss: 65093744077.5273, Val Loss: 34073061123.2217\n",
      "Epoch [880/5000], Train Loss: 137682433090.3931, Val Loss: 29839916983.8006\n",
      "Epoch [900/5000], Train Loss: 73930778232.2423, Val Loss: 29327809192.3063\n",
      "Epoch [920/5000], Train Loss: 61232936845.1773, Val Loss: 37766667344.0752\n",
      "Epoch [940/5000], Train Loss: 37681155301.7832, Val Loss: 54545092863.9978\n",
      "Epoch [960/5000], Train Loss: 51320222062.0907, Val Loss: 28550531073.7539\n",
      "Epoch [980/5000], Train Loss: 119795767305.2122, Val Loss: 28508374177.2447\n",
      "Epoch [1000/5000], Train Loss: 97258781554.3810, Val Loss: 30144071971.9647\n",
      "Epoch [1020/5000], Train Loss: 119771216690.4780, Val Loss: 26012955837.4669\n",
      "Epoch [1040/5000], Train Loss: 75283729126.1161, Val Loss: 30386993822.4940\n",
      "Epoch [1060/5000], Train Loss: 93629466381.5827, Val Loss: 29991830598.3487\n",
      "Epoch [1080/5000], Train Loss: 72058128681.2073, Val Loss: 57937550178.2372\n",
      "Epoch [1100/5000], Train Loss: 102461104082.0183, Val Loss: 32661509480.9925\n",
      "Epoch [1120/5000], Train Loss: 108871325017.0631, Val Loss: 28303642478.0865\n",
      "Epoch [1140/5000], Train Loss: 37622167524.8975, Val Loss: 35176836957.2481\n",
      "Epoch [1160/5000], Train Loss: 42805450704.2499, Val Loss: 28184611175.8853\n",
      "Epoch [1180/5000], Train Loss: 54823347141.5502, Val Loss: 31181688515.2289\n",
      "Epoch [1200/5000], Train Loss: 28161795902.7934, Val Loss: 27708153584.1227\n",
      "Epoch [1220/5000], Train Loss: 50681478805.9607, Val Loss: 31550296306.8345\n",
      "Epoch [1240/5000], Train Loss: 67651069560.4259, Val Loss: 33234469030.2166\n",
      "Epoch [1260/5000], Train Loss: 139577458336.2611, Val Loss: 33869829927.9013\n",
      "Epoch [1280/5000], Train Loss: 44357843020.5414, Val Loss: 33840353806.2753\n",
      "Epoch [1300/5000], Train Loss: 110243419348.5577, Val Loss: 32505892376.1549\n",
      "Epoch [1320/5000], Train Loss: 56490218198.7694, Val Loss: 34298746565.0395\n",
      "Epoch [1340/5000], Train Loss: 200885524328.9142, Val Loss: 33401934517.7081\n",
      "Epoch [1360/5000], Train Loss: 27193576308.6081, Val Loss: 33612706991.5307\n",
      "Epoch [1380/5000], Train Loss: 35855369703.7279, Val Loss: 35561247425.4933\n",
      "Epoch [1400/5000], Train Loss: 30213470954.8932, Val Loss: 35514896738.0100\n",
      "Epoch [1420/5000], Train Loss: 82748867598.4400, Val Loss: 33470851192.2118\n",
      "Epoch [1440/5000], Train Loss: 51896381852.2295, Val Loss: 36767617216.9825\n",
      "Epoch [1460/5000], Train Loss: 89448091888.6766, Val Loss: 80985643139.7516\n",
      "Epoch [1480/5000], Train Loss: 204455449713.2388, Val Loss: 33556360573.7091\n",
      "Epoch [1500/5000], Train Loss: 31669948926.6579, Val Loss: 35416653388.9057\n",
      "Epoch [1520/5000], Train Loss: 103623832861.3589, Val Loss: 33516625122.2108\n",
      "Epoch [1540/5000], Train Loss: 36844309197.7086, Val Loss: 33883467833.4583\n",
      "Early stopping at epoch 1556\n",
      "\n",
      "Fold number: 0\n",
      "[0.6293475882772597]\n",
      "MAPE =  0.6293475882772597\n",
      "\n",
      "Epoch [20/5000], Train Loss: 8040354744046.8848, Val Loss: 3731603312740.8706\n",
      "Epoch [40/5000], Train Loss: 3574327932208.2749, Val Loss: 890534591612.7264\n",
      "Epoch [60/5000], Train Loss: 1203576085340.7026, Val Loss: 309839413934.5820\n",
      "Epoch [80/5000], Train Loss: 286516524011.3354, Val Loss: 182154554799.6167\n",
      "Epoch [100/5000], Train Loss: 210098691217.0824, Val Loss: 129048033202.8062\n",
      "Epoch [120/5000], Train Loss: 645844337056.0907, Val Loss: 102603202865.5388\n",
      "Epoch [140/5000], Train Loss: 145512996724.3002, Val Loss: 77246118935.9847\n",
      "Epoch [160/5000], Train Loss: 153068196634.4991, Val Loss: 60252525477.0005\n",
      "Epoch [180/5000], Train Loss: 81891267981.8126, Val Loss: 44171309068.1393\n",
      "Epoch [200/5000], Train Loss: 350244306249.8600, Val Loss: 27853994589.1205\n",
      "Epoch [220/5000], Train Loss: 138017770032.8199, Val Loss: 27609931289.5595\n",
      "Epoch [240/5000], Train Loss: 78684201296.9682, Val Loss: 26347000696.7108\n",
      "Epoch [260/5000], Train Loss: 205447229926.4511, Val Loss: 27317577743.3600\n",
      "Epoch [280/5000], Train Loss: 33457600924.5962, Val Loss: 36095215209.0832\n",
      "Epoch [300/5000], Train Loss: 186310257499.3138, Val Loss: 47518519005.3401\n",
      "Epoch [320/5000], Train Loss: 580656938163.5970, Val Loss: 28131794399.7405\n",
      "Epoch [340/5000], Train Loss: 136974952753.0972, Val Loss: 26919848627.8356\n",
      "Epoch [360/5000], Train Loss: 92734372681.4668, Val Loss: 27605200455.6912\n",
      "Epoch [380/5000], Train Loss: 66056897947.1609, Val Loss: 30335825895.5922\n",
      "Epoch [400/5000], Train Loss: 116971124247.7039, Val Loss: 29837883864.4671\n",
      "Epoch [420/5000], Train Loss: 108602022582.0686, Val Loss: 25140441760.8575\n",
      "Epoch [440/5000], Train Loss: 1750304485064.2158, Val Loss: 25721819195.5679\n",
      "Epoch [460/5000], Train Loss: 84332154893.6496, Val Loss: 26340477476.6425\n",
      "Epoch [480/5000], Train Loss: 124064685513.4552, Val Loss: 21493133335.6505\n",
      "Epoch [500/5000], Train Loss: 23387956963.1788, Val Loss: 24839059767.7383\n",
      "Epoch [520/5000], Train Loss: 54227116766.3161, Val Loss: 19967110135.1968\n",
      "Epoch [540/5000], Train Loss: 155808056783.7614, Val Loss: 16875055553.4130\n",
      "Epoch [560/5000], Train Loss: 135335468126.9990, Val Loss: 18778417779.3891\n",
      "Epoch [580/5000], Train Loss: 63713765358.2250, Val Loss: 21215085834.3138\n",
      "Epoch [600/5000], Train Loss: 82560485842.3234, Val Loss: 21461442271.2288\n",
      "Epoch [620/5000], Train Loss: 227365815248.9207, Val Loss: 22719576860.5215\n",
      "Epoch [640/5000], Train Loss: 69428191398.3352, Val Loss: 26197003901.5813\n",
      "Epoch [660/5000], Train Loss: 23614218599.0931, Val Loss: 18843504808.6104\n",
      "Epoch [680/5000], Train Loss: 62768637182.0700, Val Loss: 22365525707.4653\n",
      "Epoch [700/5000], Train Loss: 145483815027.7593, Val Loss: 17964386009.2778\n",
      "Epoch [720/5000], Train Loss: 694157921985.0735, Val Loss: 18070670369.5646\n",
      "Epoch [740/5000], Train Loss: 140393421374.7542, Val Loss: 25725206137.1331\n",
      "Epoch [760/5000], Train Loss: 62998796510.9437, Val Loss: 19445100471.2602\n",
      "Epoch [780/5000], Train Loss: 737730129045.6252, Val Loss: 23283628500.1921\n",
      "Epoch [800/5000], Train Loss: 87988346067.1028, Val Loss: 19462387847.9934\n",
      "Epoch [820/5000], Train Loss: 154857431928.0070, Val Loss: 21387676420.5829\n",
      "Epoch [840/5000], Train Loss: 72533022539.3698, Val Loss: 17926048418.8403\n",
      "Epoch [860/5000], Train Loss: 62889971683.7298, Val Loss: 19555391131.8287\n",
      "Epoch [880/5000], Train Loss: 202239047973.6598, Val Loss: 20292705903.8535\n",
      "Epoch [900/5000], Train Loss: 31476188790.9692, Val Loss: 20201690948.7737\n",
      "Epoch [920/5000], Train Loss: 47844458231.1998, Val Loss: 21107354215.7336\n",
      "Epoch [940/5000], Train Loss: 26075662617.2015, Val Loss: 22338797249.9581\n",
      "Epoch [960/5000], Train Loss: 54668075653.4723, Val Loss: 23270983948.5580\n",
      "Epoch [980/5000], Train Loss: 51452962568.7428, Val Loss: 22786046364.3409\n",
      "Epoch [1000/5000], Train Loss: 83085771579.7350, Val Loss: 21923452432.9508\n",
      "Epoch [1020/5000], Train Loss: 106857992337.2969, Val Loss: 23934755010.2692\n",
      "Epoch [1040/5000], Train Loss: 122029915162.8215, Val Loss: 23913767350.9657\n",
      "Epoch [1060/5000], Train Loss: 126955201093.1653, Val Loss: 21259423402.5955\n",
      "Epoch [1080/5000], Train Loss: 93045331132.6270, Val Loss: 52004947070.1583\n",
      "Epoch [1100/5000], Train Loss: 178085828034.1615, Val Loss: 27826485557.3588\n",
      "Epoch [1120/5000], Train Loss: 125791889567.2104, Val Loss: 22897884898.4805\n",
      "Epoch [1140/5000], Train Loss: 19755072748.4114, Val Loss: 24323449048.1237\n",
      "Epoch [1160/5000], Train Loss: 50810215584.6489, Val Loss: 29389254817.5340\n",
      "Epoch [1180/5000], Train Loss: 67512058694.0798, Val Loss: 60604926887.6206\n",
      "Epoch [1200/5000], Train Loss: 76715587415.5827, Val Loss: 56899623524.0376\n",
      "Early stopping at epoch 1209\n",
      "\n",
      "Fold number: 1\n",
      "[0.6557632127968624]\n",
      "MAPE =  0.6557632127968624\n",
      "\n",
      "Epoch [20/5000], Train Loss: 8892809277015.5254, Val Loss: 9712847208722.5059\n",
      "Epoch [40/5000], Train Loss: 3772018184482.8057, Val Loss: 1861580817923.0828\n",
      "Epoch [60/5000], Train Loss: 1220069088184.0249, Val Loss: 901631615661.3605\n",
      "Epoch [80/5000], Train Loss: 244696656813.3903, Val Loss: 372859152113.3536\n",
      "Epoch [100/5000], Train Loss: 154694212853.3331, Val Loss: 210733009250.0471\n",
      "Epoch [120/5000], Train Loss: 434829181808.5685, Val Loss: 154700547220.2329\n",
      "Epoch [140/5000], Train Loss: 149891457226.8776, Val Loss: 128577303377.7350\n",
      "Epoch [160/5000], Train Loss: 80574807223.9166, Val Loss: 85915736927.9721\n",
      "Epoch [180/5000], Train Loss: 81917619038.8759, Val Loss: 61158460021.9511\n",
      "Epoch [200/5000], Train Loss: 462914215394.9434, Val Loss: 51280459314.3222\n",
      "Epoch [220/5000], Train Loss: 140592495933.4994, Val Loss: 48197989390.3048\n",
      "Epoch [240/5000], Train Loss: 137166840650.2138, Val Loss: 38554375918.9291\n",
      "Epoch [260/5000], Train Loss: 118278874251.3646, Val Loss: 47111641277.7821\n",
      "Epoch [280/5000], Train Loss: 32496307860.0808, Val Loss: 45965724412.0521\n",
      "Epoch [300/5000], Train Loss: 92152887936.4281, Val Loss: 40729742203.6631\n",
      "Epoch [320/5000], Train Loss: 553847577217.9583, Val Loss: 37197797324.1800\n",
      "Epoch [340/5000], Train Loss: 122757545291.8658, Val Loss: 31521735548.6070\n",
      "Epoch [360/5000], Train Loss: 63986663251.5887, Val Loss: 93413084549.8876\n",
      "Epoch [380/5000], Train Loss: 70996420340.4831, Val Loss: 37570096691.4336\n",
      "Epoch [400/5000], Train Loss: 94258118145.6837, Val Loss: 45022603041.2245\n",
      "Epoch [420/5000], Train Loss: 120988538035.6356, Val Loss: 40031033900.8697\n",
      "Epoch [440/5000], Train Loss: 1724925278305.8850, Val Loss: 36782440103.2075\n",
      "Epoch [460/5000], Train Loss: 63536198089.8669, Val Loss: 38339181094.0534\n",
      "Epoch [480/5000], Train Loss: 103136579648.7467, Val Loss: 39602814110.1970\n",
      "Epoch [500/5000], Train Loss: 38652937972.5679, Val Loss: 71583153235.9205\n",
      "Epoch [520/5000], Train Loss: 61706224861.1067, Val Loss: 39471462959.9577\n",
      "Epoch [540/5000], Train Loss: 101563215698.0315, Val Loss: 44583973492.4685\n",
      "Epoch [560/5000], Train Loss: 114323487494.5070, Val Loss: 42455003423.9798\n",
      "Epoch [580/5000], Train Loss: 64948753897.7110, Val Loss: 38111866215.1783\n",
      "Epoch [600/5000], Train Loss: 150327279898.0962, Val Loss: 48575477592.2820\n",
      "Epoch [620/5000], Train Loss: 220411748750.9995, Val Loss: 37334955641.3627\n",
      "Epoch [640/5000], Train Loss: 80966299862.2389, Val Loss: 38235590384.4068\n",
      "Epoch [660/5000], Train Loss: 50309992341.5183, Val Loss: 40642105157.4625\n",
      "Epoch [680/5000], Train Loss: 59657329994.0548, Val Loss: 40245256815.1250\n",
      "Epoch [700/5000], Train Loss: 547918195188.0002, Val Loss: 38401661549.5398\n",
      "Epoch [720/5000], Train Loss: 693346984314.8542, Val Loss: 44075645832.2485\n",
      "Epoch [740/5000], Train Loss: 133762634319.0250, Val Loss: 50889997528.0100\n",
      "Epoch [760/5000], Train Loss: 59888704690.7856, Val Loss: 36165778387.2125\n",
      "Epoch [780/5000], Train Loss: 206078875122.4342, Val Loss: 34793579782.8697\n",
      "Epoch [800/5000], Train Loss: 507221091658.4785, Val Loss: 38057760810.9476\n",
      "Epoch [820/5000], Train Loss: 116963738458.3699, Val Loss: 32621677619.4399\n",
      "Epoch [840/5000], Train Loss: 69750052241.8751, Val Loss: 36170669491.0747\n",
      "Epoch [860/5000], Train Loss: 70085998394.2770, Val Loss: 35722281585.0457\n",
      "Epoch [880/5000], Train Loss: 74384387539.8931, Val Loss: 40638391540.1889\n",
      "Epoch [900/5000], Train Loss: 33862026355.2338, Val Loss: 38345545911.4677\n",
      "Epoch [920/5000], Train Loss: 157260456829.1333, Val Loss: 35871685425.5419\n",
      "Epoch [940/5000], Train Loss: 248485446526.0522, Val Loss: 44194774622.8646\n",
      "Epoch [960/5000], Train Loss: 47028522071.3288, Val Loss: 34142355989.6943\n",
      "Epoch [980/5000], Train Loss: 61006060360.1623, Val Loss: 41269416133.0868\n",
      "Epoch [1000/5000], Train Loss: 118619407089.0651, Val Loss: 38163543149.5815\n",
      "Epoch [1020/5000], Train Loss: 124936058601.7736, Val Loss: 37545765253.5433\n",
      "Epoch [1040/5000], Train Loss: 83479957442.3395, Val Loss: 30689582413.6304\n",
      "Epoch [1060/5000], Train Loss: 108016935643.3042, Val Loss: 33823314501.6705\n",
      "Epoch [1080/5000], Train Loss: 57636590511.0925, Val Loss: 34442644775.7648\n",
      "Epoch [1100/5000], Train Loss: 158413857042.8097, Val Loss: 30350179702.9522\n",
      "Epoch [1120/5000], Train Loss: 121052540252.9618, Val Loss: 39611954071.8702\n",
      "Epoch [1140/5000], Train Loss: 33277774480.1888, Val Loss: 32615207484.1725\n",
      "Epoch [1160/5000], Train Loss: 76990416936.0802, Val Loss: 30649111247.5394\n",
      "Epoch [1180/5000], Train Loss: 43964081161.6912, Val Loss: 33421146718.9615\n",
      "Epoch [1200/5000], Train Loss: 30087843520.2060, Val Loss: 27727397436.6352\n",
      "Epoch [1220/5000], Train Loss: 24796348067.4916, Val Loss: 32079089439.9370\n",
      "Epoch [1240/5000], Train Loss: 61254211756.0644, Val Loss: 25391507032.7771\n",
      "Epoch [1260/5000], Train Loss: 107962556863.8831, Val Loss: 28334734895.3957\n",
      "Epoch [1280/5000], Train Loss: 69111709603.9259, Val Loss: 25999059796.4315\n",
      "Epoch [1300/5000], Train Loss: 90023507376.0242, Val Loss: 28120385855.1005\n",
      "Epoch [1320/5000], Train Loss: 112591614970.1353, Val Loss: 28434837446.3304\n",
      "Epoch [1340/5000], Train Loss: 147895203255.3097, Val Loss: 76981373666.3647\n",
      "Epoch [1360/5000], Train Loss: 27093101665.7841, Val Loss: 26293667274.8656\n",
      "Epoch [1380/5000], Train Loss: 119073279046.9205, Val Loss: 40400529945.0472\n",
      "Epoch [1400/5000], Train Loss: 33279675129.4684, Val Loss: 29682956059.9067\n",
      "Epoch [1420/5000], Train Loss: 67271177288.3709, Val Loss: 25085437388.4892\n",
      "Epoch [1440/5000], Train Loss: 98618593763.6786, Val Loss: 28614464130.9487\n",
      "Epoch [1460/5000], Train Loss: 59375815964.6234, Val Loss: 100777661563.6013\n",
      "Epoch [1480/5000], Train Loss: 87660081226.7032, Val Loss: 27252680998.6200\n",
      "Epoch [1500/5000], Train Loss: 22741387022.5576, Val Loss: 21398291084.8008\n",
      "Epoch [1520/5000], Train Loss: 99718512919.5723, Val Loss: 28489137862.0918\n",
      "Epoch [1540/5000], Train Loss: 63043501689.2733, Val Loss: 85196330990.6609\n",
      "Epoch [1560/5000], Train Loss: 40440640343.3264, Val Loss: 29544917051.5880\n",
      "Epoch [1580/5000], Train Loss: 76797322302.3470, Val Loss: 27477910995.7005\n",
      "Epoch [1600/5000], Train Loss: 143736035412.6454, Val Loss: 26161285265.5981\n",
      "Epoch [1620/5000], Train Loss: 67309569806.1541, Val Loss: 29920364583.3279\n",
      "Epoch [1640/5000], Train Loss: 60203805823.8024, Val Loss: 23435464966.3544\n",
      "Epoch [1660/5000], Train Loss: 67013588873.4703, Val Loss: 107696418492.6891\n",
      "Epoch [1680/5000], Train Loss: 170257563400.2372, Val Loss: 103915661291.4827\n",
      "Epoch [1700/5000], Train Loss: 58444359671.9170, Val Loss: 82314417239.8458\n",
      "Epoch [1720/5000], Train Loss: 151970008448.3791, Val Loss: 24847460936.8327\n",
      "Epoch [1740/5000], Train Loss: 79368725850.3376, Val Loss: 26764474572.6071\n",
      "Epoch [1760/5000], Train Loss: 23781776812.9575, Val Loss: 33797276982.4493\n",
      "Epoch [1780/5000], Train Loss: 37521410267.2353, Val Loss: 29720093356.6510\n",
      "Epoch [1800/5000], Train Loss: 17607836130.7700, Val Loss: 32521602509.3938\n",
      "Epoch [1820/5000], Train Loss: 137229322385.1193, Val Loss: 66828831421.8230\n",
      "Epoch [1840/5000], Train Loss: 90580491643.4058, Val Loss: 29296891115.9052\n",
      "Epoch [1860/5000], Train Loss: 501275893018.2811, Val Loss: 24133818997.5518\n",
      "Epoch [1880/5000], Train Loss: 32698110806.6119, Val Loss: 30955881486.0293\n",
      "Epoch [1900/5000], Train Loss: 271085524392.9063, Val Loss: 27885934470.3803\n",
      "Epoch [1920/5000], Train Loss: 307065176155.1797, Val Loss: 28899360135.7021\n",
      "Epoch [1940/5000], Train Loss: 146793040296.3130, Val Loss: 27878683779.2072\n",
      "Epoch [1960/5000], Train Loss: 173623926105.2982, Val Loss: 81319170393.1508\n",
      "Epoch [1980/5000], Train Loss: 23601058904.8957, Val Loss: 24314106124.2904\n",
      "Epoch [2000/5000], Train Loss: 113203588713.0313, Val Loss: 23456735519.4370\n",
      "Epoch [2020/5000], Train Loss: 29897085584.0406, Val Loss: 25153689098.8704\n",
      "Epoch [2040/5000], Train Loss: 36571820827.1057, Val Loss: 24516898763.3182\n",
      "Epoch [2060/5000], Train Loss: 98092655938.7765, Val Loss: 25693726862.4641\n",
      "Epoch [2080/5000], Train Loss: 68052177076.0419, Val Loss: 29902348073.0831\n",
      "Epoch [2100/5000], Train Loss: 142260611330.1178, Val Loss: 28025943172.1861\n",
      "Epoch [2120/5000], Train Loss: 79356112346.5452, Val Loss: 25176823547.0576\n",
      "Epoch [2140/5000], Train Loss: 57430723758.6732, Val Loss: 27222095072.0541\n",
      "Epoch [2160/5000], Train Loss: 80873632851.8726, Val Loss: 47211906663.8678\n",
      "Epoch [2180/5000], Train Loss: 32207672392.0705, Val Loss: 28111806210.6241\n",
      "Epoch [2200/5000], Train Loss: 98409398138.2914, Val Loss: 28151326605.9663\n",
      "Epoch [2220/5000], Train Loss: 56190428902.8420, Val Loss: 30874869056.5882\n",
      "Early stopping at epoch 2222\n",
      "\n",
      "Fold number: 2\n",
      "[0.6601269815569901]\n",
      "MAPE =  0.6601269815569901\n",
      "\n",
      "Epoch [20/5000], Train Loss: 8120781862710.8564, Val Loss: 1385633820984.0061\n",
      "Epoch [40/5000], Train Loss: 4590185188675.1885, Val Loss: 1022674683036.5337\n",
      "Epoch [60/5000], Train Loss: 1434897974435.4080, Val Loss: 454544308857.7202\n",
      "Epoch [80/5000], Train Loss: 241615041530.9431, Val Loss: 363125521186.4402\n",
      "Epoch [100/5000], Train Loss: 361860358754.3072, Val Loss: 169664391857.8281\n",
      "Epoch [120/5000], Train Loss: 147624518688.9212, Val Loss: 149447507123.6184\n",
      "Epoch [140/5000], Train Loss: 173152024098.5749, Val Loss: 126786983290.9014\n",
      "Epoch [160/5000], Train Loss: 96601940832.6386, Val Loss: 108533706952.8535\n",
      "Epoch [180/5000], Train Loss: 99344187100.1711, Val Loss: 93125921560.8222\n",
      "Epoch [200/5000], Train Loss: 221966546704.4381, Val Loss: 84534272213.3900\n",
      "Epoch [220/5000], Train Loss: 104907877221.3016, Val Loss: 74488403866.8273\n",
      "Epoch [240/5000], Train Loss: 133014808932.1979, Val Loss: 73381929578.0764\n",
      "Epoch [260/5000], Train Loss: 117727188378.0865, Val Loss: 64410130611.8269\n",
      "Epoch [280/5000], Train Loss: 1439357509585.8718, Val Loss: 69838633216.3876\n",
      "Epoch [300/5000], Train Loss: 94289857745.2547, Val Loss: 61790281153.7673\n",
      "Epoch [320/5000], Train Loss: 167062435166.8536, Val Loss: 52683305560.7535\n",
      "Epoch [340/5000], Train Loss: 79818742287.0800, Val Loss: 50260329769.7141\n",
      "Epoch [360/5000], Train Loss: 42043232169.7641, Val Loss: 48602004909.6587\n",
      "Epoch [380/5000], Train Loss: 57650157550.9500, Val Loss: 45498412473.0873\n",
      "Epoch [400/5000], Train Loss: 271570989806.6112, Val Loss: 44940554078.6511\n",
      "Epoch [420/5000], Train Loss: 67981846907.0883, Val Loss: 46565286344.1478\n",
      "Epoch [440/5000], Train Loss: 1837642113791.0232, Val Loss: 108056570972.2211\n",
      "Epoch [460/5000], Train Loss: 71357547927.7405, Val Loss: 50928371430.3459\n",
      "Epoch [480/5000], Train Loss: 242665787380.9914, Val Loss: 46462826805.4982\n",
      "Epoch [500/5000], Train Loss: 59335650598.8097, Val Loss: 130198492678.5196\n",
      "Epoch [520/5000], Train Loss: 63239575665.2351, Val Loss: 46171055763.5542\n",
      "Epoch [540/5000], Train Loss: 38754797470.2680, Val Loss: 44955998785.8193\n",
      "Epoch [560/5000], Train Loss: 214693550176.8607, Val Loss: 42079862003.1550\n",
      "Epoch [580/5000], Train Loss: 28392163291.7428, Val Loss: 41435037143.8470\n",
      "Epoch [600/5000], Train Loss: 125925351775.6495, Val Loss: 44808868617.2209\n",
      "Epoch [620/5000], Train Loss: 204824819439.8111, Val Loss: 45765907955.4448\n",
      "Epoch [640/5000], Train Loss: 52359973412.4167, Val Loss: 144666550986.1755\n",
      "Epoch [660/5000], Train Loss: 33741701864.8758, Val Loss: 60179015694.2371\n",
      "Epoch [680/5000], Train Loss: 69774145693.8304, Val Loss: 39863540281.2933\n",
      "Epoch [700/5000], Train Loss: 508547947785.5511, Val Loss: 41173945297.1511\n",
      "Epoch [720/5000], Train Loss: 797514001119.6216, Val Loss: 38043050359.7152\n",
      "Epoch [740/5000], Train Loss: 86100577965.6801, Val Loss: 33540449991.0372\n",
      "Epoch [760/5000], Train Loss: 227748008382.4129, Val Loss: 33005040638.3265\n",
      "Epoch [780/5000], Train Loss: 216615643370.0482, Val Loss: 67312090594.4126\n",
      "Epoch [800/5000], Train Loss: 526834511500.3760, Val Loss: 35485480654.5989\n",
      "Epoch [820/5000], Train Loss: 117053019314.6705, Val Loss: 35975407619.2931\n",
      "Epoch [840/5000], Train Loss: 67170900379.8319, Val Loss: 102508047626.3122\n",
      "Epoch [860/5000], Train Loss: 1557529854562.9258, Val Loss: 34540652382.9693\n",
      "Epoch [880/5000], Train Loss: 63009541871.4529, Val Loss: 33743076180.6488\n",
      "Epoch [900/5000], Train Loss: 30675189307.1736, Val Loss: 33147551196.5334\n",
      "Epoch [920/5000], Train Loss: 144466510304.5836, Val Loss: 38685258622.8074\n",
      "Epoch [940/5000], Train Loss: 766004753200.9741, Val Loss: 33473224118.9895\n",
      "Epoch [960/5000], Train Loss: 46059411375.5574, Val Loss: 39280981042.1768\n",
      "Epoch [980/5000], Train Loss: 61663800936.6235, Val Loss: 36368898778.7627\n",
      "Epoch [1000/5000], Train Loss: 154460535947.7784, Val Loss: 35511812066.2236\n",
      "Epoch [1020/5000], Train Loss: 125123041656.2042, Val Loss: 36101356315.6861\n",
      "Epoch [1040/5000], Train Loss: 82288487001.3635, Val Loss: 38699862728.1730\n",
      "Epoch [1060/5000], Train Loss: 128903522889.0187, Val Loss: 36522425417.6462\n",
      "Epoch [1080/5000], Train Loss: 59220808063.1992, Val Loss: 43125377102.6485\n",
      "Epoch [1100/5000], Train Loss: 103569759643.2215, Val Loss: 40124771498.7224\n",
      "Epoch [1120/5000], Train Loss: 104499539849.9676, Val Loss: 41152489748.2841\n",
      "Epoch [1140/5000], Train Loss: 42743404781.6212, Val Loss: 39526424481.8631\n",
      "Epoch [1160/5000], Train Loss: 79518193592.5293, Val Loss: 37732032999.4425\n",
      "Epoch [1180/5000], Train Loss: 83826559336.4839, Val Loss: 42828037170.3513\n",
      "Epoch [1200/5000], Train Loss: 26976754937.0622, Val Loss: 41082089014.7370\n",
      "Epoch [1220/5000], Train Loss: 20360931398.1825, Val Loss: 36497426960.9862\n",
      "Epoch [1240/5000], Train Loss: 67841840925.4782, Val Loss: 42525915992.7154\n",
      "Epoch [1260/5000], Train Loss: 101359255357.8475, Val Loss: 39828301358.6022\n",
      "Epoch [1280/5000], Train Loss: 81795354199.0489, Val Loss: 39450081419.9156\n",
      "Epoch [1300/5000], Train Loss: 91434673878.9002, Val Loss: 63820485699.9554\n",
      "Epoch [1320/5000], Train Loss: 106824983718.3603, Val Loss: 46974545693.7563\n",
      "Epoch [1340/5000], Train Loss: 128977972585.5835, Val Loss: 35233248890.6646\n",
      "Epoch [1360/5000], Train Loss: 37522273042.4001, Val Loss: 35679358434.7486\n",
      "Epoch [1380/5000], Train Loss: 136530938342.1082, Val Loss: 31238654105.5495\n",
      "Epoch [1400/5000], Train Loss: 25529115189.8304, Val Loss: 30311434803.5019\n",
      "Epoch [1420/5000], Train Loss: 71293830466.7735, Val Loss: 31773388717.8783\n",
      "Epoch [1440/5000], Train Loss: 69216725671.8941, Val Loss: 106272913295.9425\n",
      "Epoch [1460/5000], Train Loss: 13320984283.6045, Val Loss: 110382101217.4032\n",
      "Epoch [1480/5000], Train Loss: 80356633705.0711, Val Loss: 37548226515.2382\n",
      "Epoch [1500/5000], Train Loss: 74582906548.4166, Val Loss: 31394857270.6103\n",
      "Epoch [1520/5000], Train Loss: 107039845781.2311, Val Loss: 33599982191.7777\n",
      "Epoch [1540/5000], Train Loss: 33081525522.7201, Val Loss: 46273918102.2404\n",
      "Epoch [1560/5000], Train Loss: 63376952432.5804, Val Loss: 43565213489.4257\n",
      "Epoch [1580/5000], Train Loss: 108130365088.9761, Val Loss: 31622686801.5249\n",
      "Epoch [1600/5000], Train Loss: 150429863958.9907, Val Loss: 33059054731.2600\n",
      "Epoch [1620/5000], Train Loss: 44754374117.8099, Val Loss: 35480323174.6261\n",
      "Epoch [1640/5000], Train Loss: 50936313849.9525, Val Loss: 33003443961.9139\n",
      "Epoch [1660/5000], Train Loss: 48201618790.3500, Val Loss: 108012519809.3272\n",
      "Epoch [1680/5000], Train Loss: 82925251879.3115, Val Loss: 31519193277.5209\n",
      "Epoch [1700/5000], Train Loss: 30576965614.2114, Val Loss: 35323881570.2123\n",
      "Epoch [1720/5000], Train Loss: 199429099548.0807, Val Loss: 35296125615.0612\n",
      "Epoch [1740/5000], Train Loss: 48295089584.7231, Val Loss: 32790913478.0248\n",
      "Epoch [1760/5000], Train Loss: 34164599383.4164, Val Loss: 127085426666.9195\n",
      "Epoch [1780/5000], Train Loss: 59348810943.2838, Val Loss: 34046765967.3916\n",
      "Epoch [1800/5000], Train Loss: 82254917539.1906, Val Loss: 34585975667.4051\n",
      "Epoch [1820/5000], Train Loss: 103709337990.7688, Val Loss: 41283942049.3201\n",
      "Epoch [1840/5000], Train Loss: 83803580160.7363, Val Loss: 29997459554.3095\n",
      "Epoch [1860/5000], Train Loss: 466488505823.7866, Val Loss: 33240822554.9281\n",
      "Epoch [1880/5000], Train Loss: 40103031159.6620, Val Loss: 31679487640.1593\n",
      "Epoch [1900/5000], Train Loss: 272750806284.0801, Val Loss: 146318429046.2602\n",
      "Epoch [1920/5000], Train Loss: 274274466838.1621, Val Loss: 61796422774.2188\n",
      "Epoch [1940/5000], Train Loss: 157239669377.1176, Val Loss: 30185788026.7261\n",
      "Epoch [1960/5000], Train Loss: 69529706513.5559, Val Loss: 27528307945.0005\n",
      "Epoch [1980/5000], Train Loss: 62466989885.1583, Val Loss: 140817068812.7370\n",
      "Epoch [2000/5000], Train Loss: 144606331000.6895, Val Loss: 131891198974.3386\n",
      "Epoch [2020/5000], Train Loss: 56784906656.2511, Val Loss: 34320215180.7591\n",
      "Epoch [2040/5000], Train Loss: 42521071518.7403, Val Loss: 33863692689.1718\n",
      "Epoch [2060/5000], Train Loss: 92627565390.8176, Val Loss: 31014032305.2520\n",
      "Epoch [2080/5000], Train Loss: 55324810073.9959, Val Loss: 31283402683.8243\n",
      "Epoch [2100/5000], Train Loss: 79299270777.2906, Val Loss: 33256985653.5585\n",
      "Epoch [2120/5000], Train Loss: 21243256273.1183, Val Loss: 37749070034.6085\n",
      "Epoch [2140/5000], Train Loss: 51815341400.7158, Val Loss: 34695152143.6044\n",
      "Epoch [2160/5000], Train Loss: 70924304444.1265, Val Loss: 83872784295.0731\n",
      "Epoch [2180/5000], Train Loss: 28362800703.3705, Val Loss: 33585387329.8867\n",
      "Epoch [2200/5000], Train Loss: 172802794729.0283, Val Loss: 143231906640.8341\n",
      "Epoch [2220/5000], Train Loss: 88816994494.4176, Val Loss: 135743813504.2585\n",
      "Epoch [2240/5000], Train Loss: 81175468741.1672, Val Loss: 37523319893.3627\n",
      "Epoch [2260/5000], Train Loss: 36680080339.1289, Val Loss: 35273341505.6528\n",
      "Epoch [2280/5000], Train Loss: 145222949009.4666, Val Loss: 37161138612.5144\n",
      "Epoch [2300/5000], Train Loss: 112748364151.7485, Val Loss: 39126779180.7215\n",
      "Epoch [2320/5000], Train Loss: 30326233124.5658, Val Loss: 39481082351.7527\n",
      "Epoch [2340/5000], Train Loss: 72064000211.1898, Val Loss: 38278279937.9684\n",
      "Epoch [2360/5000], Train Loss: 72180854228.1378, Val Loss: 41891366648.6824\n",
      "Epoch [2380/5000], Train Loss: 53764057494.1085, Val Loss: 36864592872.8319\n",
      "Epoch [2400/5000], Train Loss: 1562760852410.9143, Val Loss: 78815655624.0233\n",
      "Epoch [2420/5000], Train Loss: 136839062190.1238, Val Loss: 34869031512.0531\n",
      "Epoch [2440/5000], Train Loss: 54662125918.2777, Val Loss: 34175400683.7728\n",
      "Early stopping at epoch 2441\n",
      "\n",
      "Fold number: 3\n",
      "[0.5496406962038535]\n",
      "MAPE =  0.5496406962038535\n",
      "\n",
      "Epoch [20/5000], Train Loss: 1959841860237.9822, Val Loss: 804369152060.4264\n",
      "Epoch [40/5000], Train Loss: 447882224798.4565, Val Loss: 327667977422.2095\n",
      "Epoch [60/5000], Train Loss: 1680225247696.5479, Val Loss: 188413989008.8523\n",
      "Epoch [80/5000], Train Loss: 292594517663.7897, Val Loss: 131184921393.0329\n",
      "Epoch [100/5000], Train Loss: 280854505836.6293, Val Loss: 86512265776.9559\n",
      "Epoch [120/5000], Train Loss: 171486936471.6741, Val Loss: 83208004100.1531\n",
      "Epoch [140/5000], Train Loss: 253384469912.2342, Val Loss: 80837179637.5141\n",
      "Epoch [160/5000], Train Loss: 177050900283.0031, Val Loss: 61961094020.8978\n",
      "Epoch [180/5000], Train Loss: 44736230286.1231, Val Loss: 48511223993.7143\n",
      "Epoch [200/5000], Train Loss: 63192528117.6967, Val Loss: 47685528660.8495\n",
      "Epoch [220/5000], Train Loss: 77982451155.3967, Val Loss: 44179571378.9439\n",
      "Epoch [240/5000], Train Loss: 582285181772.0000, Val Loss: 40446667008.9641\n",
      "Epoch [260/5000], Train Loss: 71791524586.2502, Val Loss: 41621704815.3271\n",
      "Epoch [280/5000], Train Loss: 46073091642.7728, Val Loss: 38680934820.2671\n",
      "Epoch [300/5000], Train Loss: 120361453374.2794, Val Loss: 40948002299.9996\n",
      "Epoch [320/5000], Train Loss: 434543209960.4423, Val Loss: 40199377572.0741\n",
      "Epoch [340/5000], Train Loss: 50093228561.7590, Val Loss: 38303319703.8098\n",
      "Epoch [360/5000], Train Loss: 45964862751.3954, Val Loss: 37826616190.4578\n",
      "Epoch [380/5000], Train Loss: 109430424216.2901, Val Loss: 39964211931.0285\n",
      "Epoch [400/5000], Train Loss: 376541183678.8932, Val Loss: 39019422840.8227\n",
      "Epoch [420/5000], Train Loss: 47081051578.0733, Val Loss: 37409971545.6643\n",
      "Epoch [440/5000], Train Loss: 155121236030.9573, Val Loss: 32350801548.0222\n",
      "Epoch [460/5000], Train Loss: 84354251926.4495, Val Loss: 56386111664.9624\n",
      "Epoch [480/5000], Train Loss: 128943993120.4896, Val Loss: 34305555707.9211\n",
      "Epoch [500/5000], Train Loss: 65183332619.8229, Val Loss: 33594528524.2196\n",
      "Epoch [520/5000], Train Loss: 87494318251.9817, Val Loss: 34652200065.4996\n",
      "Epoch [540/5000], Train Loss: 100636881894.0856, Val Loss: 51991996308.7437\n",
      "Epoch [560/5000], Train Loss: 64344658008.5208, Val Loss: 32422943297.7372\n",
      "Epoch [580/5000], Train Loss: 49734883763.1117, Val Loss: 33072198417.8231\n",
      "Epoch [600/5000], Train Loss: 47410477306.9043, Val Loss: 30912479263.8116\n",
      "Epoch [620/5000], Train Loss: 84886860519.8147, Val Loss: 48987933959.4526\n",
      "Epoch [640/5000], Train Loss: 125855682811.6639, Val Loss: 32419465089.8492\n",
      "Epoch [660/5000], Train Loss: 79658998463.0363, Val Loss: 32522433694.5470\n",
      "Epoch [680/5000], Train Loss: 1010411223852.8331, Val Loss: 35839848443.9504\n",
      "Epoch [700/5000], Train Loss: 64483373230.7343, Val Loss: 34277734274.6700\n",
      "Epoch [720/5000], Train Loss: 30926979270.2177, Val Loss: 35838761854.9838\n",
      "Epoch [740/5000], Train Loss: 21798397675.1715, Val Loss: 30648028156.0655\n",
      "Epoch [760/5000], Train Loss: 41927782508.5221, Val Loss: 34969043883.4736\n",
      "Epoch [780/5000], Train Loss: 125604087941.3485, Val Loss: 33444536445.9820\n",
      "Epoch [800/5000], Train Loss: 34410087403.5835, Val Loss: 37148161903.0417\n",
      "Epoch [820/5000], Train Loss: 56241513750.6845, Val Loss: 47363398922.1417\n",
      "Epoch [840/5000], Train Loss: 25486933527.6003, Val Loss: 29454259620.8493\n",
      "Epoch [860/5000], Train Loss: 35124155180.5002, Val Loss: 28474994309.5192\n",
      "Epoch [880/5000], Train Loss: 124014232462.0059, Val Loss: 29505143101.0362\n",
      "Epoch [900/5000], Train Loss: 247043703231.7552, Val Loss: 37287784113.9432\n",
      "Epoch [920/5000], Train Loss: 268004939044.5795, Val Loss: 30329550377.1097\n",
      "Epoch [940/5000], Train Loss: 37901552764.7119, Val Loss: 33503618091.8435\n",
      "Epoch [960/5000], Train Loss: 106092296217.3020, Val Loss: 34832265119.0882\n",
      "Epoch [980/5000], Train Loss: 28425322968.1719, Val Loss: 30419434613.8246\n",
      "Epoch [1000/5000], Train Loss: 147858369972.5749, Val Loss: 33938130081.7092\n",
      "Epoch [1020/5000], Train Loss: 88121919079.2899, Val Loss: 29451480365.8879\n",
      "Epoch [1040/5000], Train Loss: 33254390640.5125, Val Loss: 25614799833.5023\n",
      "Epoch [1060/5000], Train Loss: 46259536615.6277, Val Loss: 30217444278.3143\n",
      "Epoch [1080/5000], Train Loss: 138469568901.8678, Val Loss: 28102120064.8816\n",
      "Epoch [1100/5000], Train Loss: 47282237038.7067, Val Loss: 25747478151.1569\n",
      "Epoch [1120/5000], Train Loss: 27222260363.8261, Val Loss: 24397306202.4472\n",
      "Epoch [1140/5000], Train Loss: 252423764517.7940, Val Loss: 25798150753.2937\n",
      "Epoch [1160/5000], Train Loss: 73087624058.7681, Val Loss: 38705489629.0222\n",
      "Epoch [1180/5000], Train Loss: 97274066020.1152, Val Loss: 26215273030.5836\n",
      "Epoch [1200/5000], Train Loss: 24594045618.6304, Val Loss: 26446222409.2140\n",
      "Epoch [1220/5000], Train Loss: 73761003295.9214, Val Loss: 28170459049.1465\n",
      "Epoch [1240/5000], Train Loss: 60821231098.2103, Val Loss: 27090617258.5580\n",
      "Epoch [1260/5000], Train Loss: 37752743138.8236, Val Loss: 27311481835.5134\n",
      "Epoch [1280/5000], Train Loss: 64000810789.7304, Val Loss: 26846418351.5375\n",
      "Epoch [1300/5000], Train Loss: 210220600571.7898, Val Loss: 27882319323.3478\n",
      "Epoch [1320/5000], Train Loss: 43961918763.0269, Val Loss: 30462258949.3147\n",
      "Epoch [1340/5000], Train Loss: 175914765142.7688, Val Loss: 31795566538.9068\n",
      "Epoch [1360/5000], Train Loss: 93476093275.4821, Val Loss: 28716345181.8966\n",
      "Epoch [1380/5000], Train Loss: 90804063263.1833, Val Loss: 28030268399.1753\n",
      "Epoch [1400/5000], Train Loss: 98662219594.0922, Val Loss: 28218360868.6951\n",
      "Epoch [1420/5000], Train Loss: 94832020992.4466, Val Loss: 26201999023.5401\n",
      "Epoch [1440/5000], Train Loss: 54522442392.0562, Val Loss: 49331399325.0713\n",
      "Epoch [1460/5000], Train Loss: 876080396042.8521, Val Loss: 28494035427.6250\n",
      "Epoch [1480/5000], Train Loss: 374311566172.3710, Val Loss: 31044242935.0328\n",
      "Epoch [1500/5000], Train Loss: 423617616113.2281, Val Loss: 27961550709.1364\n",
      "Epoch [1520/5000], Train Loss: 136412236700.0852, Val Loss: 27120786506.7538\n",
      "Epoch [1540/5000], Train Loss: 52834645747.9908, Val Loss: 26506727649.9238\n",
      "Epoch [1560/5000], Train Loss: 115266953682.3349, Val Loss: 24933413363.0901\n",
      "Epoch [1580/5000], Train Loss: 34221996057.6375, Val Loss: 24867957514.2862\n",
      "Epoch [1600/5000], Train Loss: 24879220085.1647, Val Loss: 25001433974.7699\n",
      "Epoch [1620/5000], Train Loss: 56681856600.3826, Val Loss: 24182289362.2086\n",
      "Epoch [1640/5000], Train Loss: 67215211696.1776, Val Loss: 25422344542.7843\n",
      "Epoch [1660/5000], Train Loss: 24680064030.3220, Val Loss: 30290993203.6088\n",
      "Epoch [1680/5000], Train Loss: 38213955023.2692, Val Loss: 28873733619.5746\n",
      "Early stopping at epoch 1690\n",
      "\n",
      "Fold number: 4\n",
      "[0.5696224907657341]\n",
      "MAPE =  0.5696224907657341\n",
      "-\n",
      "mape score =  [[0.6293475882772597], [0.6557632127968624], [0.6601269815569901], [0.5496406962038535], [0.5696224907657341]]\n",
      ">>>>>>>>>>>>>>>>>>>>000951_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch [20/5000], Train Loss: 5762109354753.6699, Val Loss: 3050325792594.9683\n",
      "Epoch [40/5000], Train Loss: 3277846014896.8135, Val Loss: 977921477505.3925\n",
      "Epoch [60/5000], Train Loss: 3285523314301.9253, Val Loss: 410121434718.0389\n",
      "Epoch [80/5000], Train Loss: 445557501547.7710, Val Loss: 342963246898.1157\n",
      "Epoch [100/5000], Train Loss: 669476672298.5492, Val Loss: 288457891563.6391\n",
      "Epoch [120/5000], Train Loss: 156159096252.1514, Val Loss: 265203994201.0749\n",
      "Epoch [140/5000], Train Loss: 182677167166.3752, Val Loss: 194270789730.2736\n",
      "Epoch [160/5000], Train Loss: 95317713100.1278, Val Loss: 120665607861.2099\n",
      "Epoch [180/5000], Train Loss: 56434198696.5665, Val Loss: 95388188233.9358\n",
      "Epoch [200/5000], Train Loss: 44951428411.9408, Val Loss: 99897961025.5652\n",
      "Epoch [220/5000], Train Loss: 38983917029.3060, Val Loss: 100175631472.0237\n",
      "Epoch [240/5000], Train Loss: 68707255942.0216, Val Loss: 101356156884.8903\n",
      "Epoch [260/5000], Train Loss: 33774628215.7036, Val Loss: 89590279936.3538\n",
      "Epoch [280/5000], Train Loss: 55227169926.1756, Val Loss: 178310691002.6018\n",
      "Epoch [300/5000], Train Loss: 48246386832.0706, Val Loss: 80531289051.1577\n",
      "Epoch [320/5000], Train Loss: 58785065008.2223, Val Loss: 164699361803.7704\n",
      "Epoch [340/5000], Train Loss: 166173069073.4256, Val Loss: 182989439888.7853\n",
      "Epoch [360/5000], Train Loss: 149486428762.6400, Val Loss: 142265032707.6173\n",
      "Epoch [380/5000], Train Loss: 96630769667.2250, Val Loss: 129305481097.8707\n",
      "Epoch [400/5000], Train Loss: 22745632559.1747, Val Loss: 67636783407.6929\n",
      "Epoch [420/5000], Train Loss: 31409140395.8568, Val Loss: 58902869007.9122\n",
      "Epoch [440/5000], Train Loss: 61351014981.9289, Val Loss: 76902413303.3544\n",
      "Epoch [460/5000], Train Loss: 35639039172.1094, Val Loss: 116601639590.7496\n",
      "Epoch [480/5000], Train Loss: 33753204482.6829, Val Loss: 141284812404.0695\n",
      "Epoch [500/5000], Train Loss: 150030991029.4597, Val Loss: 86593384967.6494\n",
      "Epoch [520/5000], Train Loss: 31504942618.1917, Val Loss: 85914778762.3100\n",
      "Epoch [540/5000], Train Loss: 42795581321.6170, Val Loss: 86933410781.2528\n",
      "Epoch [560/5000], Train Loss: 36351469502.5101, Val Loss: 81709630697.0852\n",
      "Epoch [580/5000], Train Loss: 129146951549.6172, Val Loss: 86413264752.8211\n",
      "Epoch [600/5000], Train Loss: 41078575108.0606, Val Loss: 68354544479.9719\n",
      "Epoch [620/5000], Train Loss: 108846673211.4957, Val Loss: 87418005373.5064\n",
      "Epoch [640/5000], Train Loss: 25859456811.6179, Val Loss: 72926845487.9490\n",
      "Epoch [660/5000], Train Loss: 49275486986.1626, Val Loss: 151836543751.6294\n",
      "Epoch [680/5000], Train Loss: 46174646305.9993, Val Loss: 66820827754.8604\n",
      "Epoch [700/5000], Train Loss: 97083276011.1160, Val Loss: 72868641094.8991\n",
      "Epoch [720/5000], Train Loss: 35830122598.6918, Val Loss: 79437631905.3036\n",
      "Epoch [740/5000], Train Loss: 195328963281.5356, Val Loss: 165991313522.6109\n",
      "Epoch [760/5000], Train Loss: 32560507674.9254, Val Loss: 76627877240.9087\n",
      "Epoch [780/5000], Train Loss: 57968580639.0023, Val Loss: 80921761460.8864\n",
      "Epoch [800/5000], Train Loss: 52780292018.9757, Val Loss: 78275399186.3978\n",
      "Epoch [820/5000], Train Loss: 132220548902.0877, Val Loss: 76431590708.4165\n",
      "Epoch [840/5000], Train Loss: 52795611891.2396, Val Loss: 171290972009.9631\n",
      "Epoch [860/5000], Train Loss: 54982606911.6742, Val Loss: 67050483044.8640\n",
      "Epoch [880/5000], Train Loss: 20126345752.2083, Val Loss: 81091821583.1839\n",
      "Epoch [900/5000], Train Loss: 29698126587.1129, Val Loss: 64462729259.3270\n",
      "Early stopping at epoch 905\n",
      "\n",
      "Fold number: 0\n",
      "[0.4906880357936948]\n",
      "MAPE =  0.4906880357936948\n",
      "\n",
      "Epoch [20/5000], Train Loss: 5272969733026.9688, Val Loss: 4371564197535.0552\n",
      "Epoch [40/5000], Train Loss: 3668116776281.1616, Val Loss: 1360030554900.5273\n",
      "Epoch [60/5000], Train Loss: 830557328522.8455, Val Loss: 690176922870.3363\n",
      "Epoch [80/5000], Train Loss: 736943532730.8149, Val Loss: 333141878782.9949\n",
      "Epoch [100/5000], Train Loss: 538764715852.3537, Val Loss: 233214050760.1186\n",
      "Epoch [120/5000], Train Loss: 125569946115.1166, Val Loss: 125118602851.3569\n",
      "Epoch [140/5000], Train Loss: 221387838099.8721, Val Loss: 111848231038.6073\n",
      "Epoch [160/5000], Train Loss: 108881011940.8062, Val Loss: 75421538021.3456\n",
      "Epoch [180/5000], Train Loss: 46962406264.8149, Val Loss: 53274370972.3519\n",
      "Epoch [200/5000], Train Loss: 71546938323.1177, Val Loss: 49862116101.9747\n",
      "Epoch [220/5000], Train Loss: 31435385866.8905, Val Loss: 46922605863.2419\n",
      "Epoch [240/5000], Train Loss: 89649730693.5034, Val Loss: 50841884838.9166\n",
      "Epoch [260/5000], Train Loss: 54213074932.5629, Val Loss: 44237890994.5149\n",
      "Epoch [280/5000], Train Loss: 88178140521.3501, Val Loss: 39400261371.0404\n",
      "Epoch [300/5000], Train Loss: 29462817940.3715, Val Loss: 60484502474.4296\n",
      "Epoch [320/5000], Train Loss: 63616575323.9426, Val Loss: 48409036683.6397\n",
      "Epoch [340/5000], Train Loss: 97676017462.4374, Val Loss: 34807536716.9617\n",
      "Epoch [360/5000], Train Loss: 184247488531.7156, Val Loss: 54503353492.8387\n",
      "Epoch [380/5000], Train Loss: 74754406827.2301, Val Loss: 57303065463.9689\n",
      "Epoch [400/5000], Train Loss: 29319349378.2027, Val Loss: 32914550307.3403\n",
      "Epoch [420/5000], Train Loss: 48335234800.0186, Val Loss: 75912961198.2551\n",
      "Epoch [440/5000], Train Loss: 68898740395.9158, Val Loss: 76541825451.2570\n",
      "Epoch [460/5000], Train Loss: 44660114027.3511, Val Loss: 71384162680.8918\n",
      "Epoch [480/5000], Train Loss: 31110034553.8656, Val Loss: 79853210225.6945\n",
      "Epoch [500/5000], Train Loss: 145815016883.4610, Val Loss: 45594600004.3372\n",
      "Epoch [520/5000], Train Loss: 33601897785.2460, Val Loss: 37357977178.9023\n",
      "Epoch [540/5000], Train Loss: 40848382644.7223, Val Loss: 65278380721.5632\n",
      "Epoch [560/5000], Train Loss: 48312942574.5442, Val Loss: 80665442704.0335\n",
      "Epoch [580/5000], Train Loss: 102799210369.5634, Val Loss: 45604650892.7204\n",
      "Epoch [600/5000], Train Loss: 41694893334.5219, Val Loss: 34192141175.5014\n",
      "Epoch [620/5000], Train Loss: 45543240784.3322, Val Loss: 63705869216.7172\n",
      "Epoch [640/5000], Train Loss: 29768016687.1859, Val Loss: 35488500982.5542\n",
      "Epoch [660/5000], Train Loss: 49090428132.8109, Val Loss: 65361056249.0007\n",
      "Epoch [680/5000], Train Loss: 33074810380.4466, Val Loss: 36612609028.8635\n",
      "Epoch [700/5000], Train Loss: 104376613819.5951, Val Loss: 58857378353.8545\n",
      "Epoch [720/5000], Train Loss: 35477269816.0630, Val Loss: 68642034420.4662\n",
      "Epoch [740/5000], Train Loss: 179581472369.8166, Val Loss: 41665175463.9205\n",
      "Epoch [760/5000], Train Loss: 31464758791.6543, Val Loss: 41131576304.3556\n",
      "Epoch [780/5000], Train Loss: 102705238165.6306, Val Loss: 67119850239.5854\n",
      "Epoch [800/5000], Train Loss: 48716828455.8720, Val Loss: 39639444354.7288\n",
      "Epoch [820/5000], Train Loss: 141184727672.6726, Val Loss: 46672935874.7261\n",
      "Epoch [840/5000], Train Loss: 22387705928.2687, Val Loss: 60986881113.2035\n",
      "Epoch [860/5000], Train Loss: 59408644313.8351, Val Loss: 32456642057.6153\n",
      "Epoch [880/5000], Train Loss: 27182154830.7616, Val Loss: 35740538113.7629\n",
      "Early stopping at epoch 888\n",
      "\n",
      "Fold number: 1\n",
      "[0.4285724387063681]\n",
      "MAPE =  0.4285724387063681\n",
      "\n",
      "Epoch [20/5000], Train Loss: 8598320371091.6475, Val Loss: 3266260110091.3076\n",
      "Epoch [40/5000], Train Loss: 3206995694897.6768, Val Loss: 485809494018.6202\n",
      "Epoch [60/5000], Train Loss: 509139801834.7677, Val Loss: 172853252763.5858\n",
      "Epoch [80/5000], Train Loss: 631642983038.1733, Val Loss: 112963885841.9293\n",
      "Epoch [100/5000], Train Loss: 555994154149.3617, Val Loss: 104391222811.1012\n",
      "Epoch [120/5000], Train Loss: 131539527557.1832, Val Loss: 104898776387.0173\n",
      "Epoch [140/5000], Train Loss: 219350639304.1242, Val Loss: 95905414960.6074\n",
      "Epoch [160/5000], Train Loss: 102041959812.5928, Val Loss: 55515556065.2212\n",
      "Epoch [180/5000], Train Loss: 41078884842.5670, Val Loss: 30626500620.6675\n",
      "Epoch [200/5000], Train Loss: 131207385391.8612, Val Loss: 27570503102.1554\n",
      "Epoch [220/5000], Train Loss: 59606033658.9397, Val Loss: 27077994368.9343\n",
      "Epoch [240/5000], Train Loss: 113129090343.0716, Val Loss: 28397163825.7978\n",
      "Epoch [260/5000], Train Loss: 49181326894.5501, Val Loss: 27947367240.4724\n",
      "Epoch [280/5000], Train Loss: 66061677344.0918, Val Loss: 89973714456.8821\n",
      "Epoch [300/5000], Train Loss: 39629676070.2401, Val Loss: 45511735807.7864\n",
      "Epoch [320/5000], Train Loss: 65609076409.8338, Val Loss: 29086218149.5268\n",
      "Epoch [340/5000], Train Loss: 115497402522.6546, Val Loss: 30778136385.7814\n",
      "Epoch [360/5000], Train Loss: 172542246535.9810, Val Loss: 37832397616.4795\n",
      "Epoch [380/5000], Train Loss: 74578261565.9292, Val Loss: 27886186600.0962\n",
      "Epoch [400/5000], Train Loss: 71760804080.1405, Val Loss: 42857739204.5794\n",
      "Epoch [420/5000], Train Loss: 66035704451.4030, Val Loss: 20830271380.9101\n",
      "Epoch [440/5000], Train Loss: 100476548792.3544, Val Loss: 18971332546.0220\n",
      "Epoch [460/5000], Train Loss: 36396879741.7902, Val Loss: 20152438287.2056\n",
      "Epoch [480/5000], Train Loss: 27629314347.7030, Val Loss: 21521764805.0756\n",
      "Epoch [500/5000], Train Loss: 134109141366.3480, Val Loss: 23192245096.0014\n",
      "Epoch [520/5000], Train Loss: 32467304288.1931, Val Loss: 16048695872.0710\n",
      "Epoch [540/5000], Train Loss: 39275913698.6497, Val Loss: 20428052975.4134\n",
      "Epoch [560/5000], Train Loss: 32219437462.0094, Val Loss: 21994239645.6028\n",
      "Epoch [580/5000], Train Loss: 92169654698.8785, Val Loss: 27713393722.6699\n",
      "Epoch [600/5000], Train Loss: 40932908063.7827, Val Loss: 17909080940.5303\n",
      "Epoch [620/5000], Train Loss: 45877711361.5134, Val Loss: 18540661003.0313\n",
      "Epoch [640/5000], Train Loss: 65716381667.0447, Val Loss: 43934944637.6434\n",
      "Epoch [660/5000], Train Loss: 23024776281.1307, Val Loss: 16830431387.9242\n",
      "Epoch [680/5000], Train Loss: 31031049658.4769, Val Loss: 14199902327.5386\n",
      "Epoch [700/5000], Train Loss: 67806263506.1574, Val Loss: 12901280044.8627\n",
      "Epoch [720/5000], Train Loss: 58341229452.0676, Val Loss: 74855307376.5070\n",
      "Epoch [740/5000], Train Loss: 124447014318.5457, Val Loss: 14444616061.2182\n",
      "Epoch [760/5000], Train Loss: 80050178820.0940, Val Loss: 86792783926.2276\n",
      "Epoch [780/5000], Train Loss: 53933103797.1459, Val Loss: 16765106377.7196\n",
      "Epoch [800/5000], Train Loss: 67089044598.1640, Val Loss: 14864754043.4256\n",
      "Epoch [820/5000], Train Loss: 136941611248.5146, Val Loss: 13092281389.3908\n",
      "Epoch [840/5000], Train Loss: 23225998023.2639, Val Loss: 29581870350.8340\n",
      "Epoch [860/5000], Train Loss: 60783700524.9954, Val Loss: 13703593761.6894\n",
      "Epoch [880/5000], Train Loss: 24352074355.1585, Val Loss: 14706006169.6605\n",
      "Epoch [900/5000], Train Loss: 108126721218.5728, Val Loss: 18483580467.7573\n",
      "Epoch [920/5000], Train Loss: 25144836610.8108, Val Loss: 18553843673.0903\n",
      "Epoch [940/5000], Train Loss: 201150876903.7242, Val Loss: 14260507851.4443\n",
      "Epoch [960/5000], Train Loss: 84588352189.8454, Val Loss: 27295142744.1582\n",
      "Epoch [980/5000], Train Loss: 56844906695.5578, Val Loss: 14140236071.1693\n",
      "Epoch [1000/5000], Train Loss: 65615683175.3425, Val Loss: 17353672338.3228\n",
      "Early stopping at epoch 1009\n",
      "\n",
      "Fold number: 2\n",
      "[0.4710498178580623]\n",
      "MAPE =  0.4710498178580623\n",
      "\n",
      "Epoch [20/5000], Train Loss: 11595867420459.0957, Val Loss: 9839958801838.2363\n",
      "Epoch [40/5000], Train Loss: 1301521348113.3899, Val Loss: 2801886425919.8926\n",
      "Epoch [60/5000], Train Loss: 531132896246.7917, Val Loss: 293068918929.2233\n",
      "Epoch [80/5000], Train Loss: 1292791928551.7412, Val Loss: 236640691043.7175\n",
      "Epoch [100/5000], Train Loss: 484274687406.4626, Val Loss: 222505869843.4763\n",
      "Epoch [120/5000], Train Loss: 88215548664.4842, Val Loss: 208363271737.4674\n",
      "Epoch [140/5000], Train Loss: 221029491698.5010, Val Loss: 186692846537.2556\n",
      "Epoch [160/5000], Train Loss: 86054803992.6004, Val Loss: 164825699169.6053\n",
      "Epoch [180/5000], Train Loss: 36148532399.4670, Val Loss: 108578058469.0241\n",
      "Epoch [200/5000], Train Loss: 69694460723.9213, Val Loss: 73120400165.3668\n",
      "Epoch [220/5000], Train Loss: 52043191734.2024, Val Loss: 70075878115.2643\n",
      "Epoch [240/5000], Train Loss: 106513552624.1153, Val Loss: 69281650280.5113\n",
      "Epoch [260/5000], Train Loss: 60072211873.3906, Val Loss: 65861307956.1409\n",
      "Epoch [280/5000], Train Loss: 87646394495.5920, Val Loss: 61998085410.6307\n",
      "Epoch [300/5000], Train Loss: 44158115055.6285, Val Loss: 61710911479.8181\n",
      "Epoch [320/5000], Train Loss: 72501932332.5689, Val Loss: 60618186788.8885\n",
      "Epoch [340/5000], Train Loss: 34998641203.2863, Val Loss: 45623890360.6785\n",
      "Epoch [360/5000], Train Loss: 50596640843.1891, Val Loss: 148102543366.3618\n",
      "Epoch [380/5000], Train Loss: 46427170213.1569, Val Loss: 62468866813.4644\n",
      "Epoch [400/5000], Train Loss: 50940819918.4130, Val Loss: 62886141397.9738\n",
      "Epoch [420/5000], Train Loss: 74841988379.3937, Val Loss: 49849550608.9084\n",
      "Epoch [440/5000], Train Loss: 88161129007.2019, Val Loss: 54732785586.1647\n",
      "Epoch [460/5000], Train Loss: 38479032400.7767, Val Loss: 44953582314.0229\n",
      "Epoch [480/5000], Train Loss: 44868276990.3776, Val Loss: 69973575564.7342\n",
      "Epoch [500/5000], Train Loss: 163562579877.0813, Val Loss: 109600357818.5405\n",
      "Epoch [520/5000], Train Loss: 45028418668.2130, Val Loss: 95923344310.6957\n",
      "Epoch [540/5000], Train Loss: 70064724466.1754, Val Loss: 35806769792.2104\n",
      "Epoch [560/5000], Train Loss: 55745397042.8075, Val Loss: 110040299238.9049\n",
      "Epoch [580/5000], Train Loss: 152340042451.2249, Val Loss: 48492710113.8571\n",
      "Epoch [600/5000], Train Loss: 45718113181.0425, Val Loss: 44732058800.7293\n",
      "Epoch [620/5000], Train Loss: 44429561037.0404, Val Loss: 118992055811.9845\n",
      "Epoch [640/5000], Train Loss: 38734900330.4353, Val Loss: 38996895725.2373\n",
      "Epoch [660/5000], Train Loss: 22567171649.0053, Val Loss: 52262417123.7940\n",
      "Epoch [680/5000], Train Loss: 44476939373.5192, Val Loss: 84644421500.1178\n",
      "Epoch [700/5000], Train Loss: 65966382158.7011, Val Loss: 43732935266.7934\n",
      "Epoch [720/5000], Train Loss: 50463666255.9937, Val Loss: 73915409384.8604\n",
      "Epoch [740/5000], Train Loss: 148419008648.2995, Val Loss: 44330417284.1062\n",
      "Epoch [760/5000], Train Loss: 36876386945.2801, Val Loss: 45756305424.0735\n",
      "Epoch [780/5000], Train Loss: 52999684725.3390, Val Loss: 52767473924.3508\n",
      "Epoch [800/5000], Train Loss: 57228059932.4457, Val Loss: 35649365393.1000\n",
      "Epoch [820/5000], Train Loss: 38415254141.4483, Val Loss: 37826195800.4291\n",
      "Epoch [840/5000], Train Loss: 49895785317.0952, Val Loss: 45658784295.9273\n",
      "Epoch [860/5000], Train Loss: 69704927659.7741, Val Loss: 48147020746.3617\n",
      "Epoch [880/5000], Train Loss: 29269169581.6675, Val Loss: 46258831959.4382\n",
      "Epoch [900/5000], Train Loss: 119789627646.1174, Val Loss: 49242048254.3707\n",
      "Epoch [920/5000], Train Loss: 25769648305.5526, Val Loss: 63870210471.8437\n",
      "Epoch [940/5000], Train Loss: 219211452038.1823, Val Loss: 69798323025.8227\n",
      "Epoch [960/5000], Train Loss: 72603316810.0922, Val Loss: 60795560959.3939\n",
      "Epoch [980/5000], Train Loss: 47438513888.7913, Val Loss: 52074083149.1033\n",
      "Epoch [1000/5000], Train Loss: 42378072522.6052, Val Loss: 55975576999.3955\n",
      "Epoch [1020/5000], Train Loss: 30006625278.9876, Val Loss: 48161739054.1317\n",
      "Epoch [1040/5000], Train Loss: 125122717833.1853, Val Loss: 188153000850.5575\n",
      "Epoch [1060/5000], Train Loss: 28969399012.3235, Val Loss: 41001901399.3328\n",
      "Epoch [1080/5000], Train Loss: 69791545913.8421, Val Loss: 48037451927.2425\n",
      "Epoch [1100/5000], Train Loss: 26374736567.8632, Val Loss: 106466765305.7573\n",
      "Epoch [1120/5000], Train Loss: 44237284414.9442, Val Loss: 42797818760.5033\n",
      "Epoch [1140/5000], Train Loss: 117341103755.9429, Val Loss: 46200115279.9272\n",
      "Epoch [1160/5000], Train Loss: 56261495328.1327, Val Loss: 52534850930.6486\n",
      "Epoch [1180/5000], Train Loss: 71523510365.2117, Val Loss: 77426362711.2209\n",
      "Epoch [1200/5000], Train Loss: 49486771699.2406, Val Loss: 98974686664.4754\n",
      "Epoch [1220/5000], Train Loss: 29937293109.7552, Val Loss: 46018305604.1214\n",
      "Epoch [1240/5000], Train Loss: 61401385125.6452, Val Loss: 45715269062.9359\n",
      "Epoch [1260/5000], Train Loss: 27538958384.8229, Val Loss: 53476819670.6070\n",
      "Epoch [1280/5000], Train Loss: 40824700188.2376, Val Loss: 46643850319.0317\n",
      "Epoch [1300/5000], Train Loss: 59401993896.0597, Val Loss: 58806352672.7235\n",
      "Epoch [1320/5000], Train Loss: 83369724734.7471, Val Loss: 98580277206.6384\n",
      "Early stopping at epoch 1329\n",
      "\n",
      "Fold number: 3\n",
      "[0.4212248962482277]\n",
      "MAPE =  0.4212248962482277\n",
      "\n",
      "Epoch [20/5000], Train Loss: 11695939466457.5645, Val Loss: 2728693388550.9707\n",
      "Epoch [40/5000], Train Loss: 1475762015116.2515, Val Loss: 200135503522.7711\n",
      "Epoch [60/5000], Train Loss: 2943930039736.2485, Val Loss: 204054961402.0141\n",
      "Epoch [80/5000], Train Loss: 186121417109.9301, Val Loss: 164153761033.8604\n",
      "Epoch [100/5000], Train Loss: 636202462735.9131, Val Loss: 128690422346.0640\n",
      "Epoch [120/5000], Train Loss: 271903352171.8612, Val Loss: 109831357004.3680\n",
      "Epoch [140/5000], Train Loss: 81162144121.3627, Val Loss: 85872643754.2327\n",
      "Epoch [160/5000], Train Loss: 50528784175.8888, Val Loss: 51332219763.4945\n",
      "Epoch [180/5000], Train Loss: 60461808005.9536, Val Loss: 49957238387.7172\n",
      "Epoch [200/5000], Train Loss: 50216513320.5406, Val Loss: 48668300165.6378\n",
      "Epoch [220/5000], Train Loss: 26651342892.9329, Val Loss: 46896262308.5381\n",
      "Epoch [240/5000], Train Loss: 59704519330.1730, Val Loss: 44518931431.9712\n",
      "Epoch [260/5000], Train Loss: 67597816935.5652, Val Loss: 41942204156.2173\n",
      "Epoch [280/5000], Train Loss: 86921674485.8505, Val Loss: 38232463860.6610\n",
      "Epoch [300/5000], Train Loss: 38944894236.4197, Val Loss: 35162507056.1400\n",
      "Epoch [320/5000], Train Loss: 54295520828.9586, Val Loss: 69515206176.7757\n",
      "Epoch [340/5000], Train Loss: 97719832075.3515, Val Loss: 30829063334.4604\n",
      "Epoch [360/5000], Train Loss: 47826065761.5292, Val Loss: 35116689069.7873\n",
      "Epoch [380/5000], Train Loss: 32802014736.6516, Val Loss: 47168973986.8432\n",
      "Epoch [400/5000], Train Loss: 111887038254.0464, Val Loss: 21504057946.2123\n",
      "Epoch [420/5000], Train Loss: 157099759182.3797, Val Loss: 40639344778.4619\n",
      "Epoch [440/5000], Train Loss: 40060307235.6101, Val Loss: 38055151636.8869\n",
      "Epoch [460/5000], Train Loss: 26818904699.6275, Val Loss: 19217652410.5616\n",
      "Epoch [480/5000], Train Loss: 75690006942.0126, Val Loss: 37373161958.8885\n",
      "Epoch [500/5000], Train Loss: 38563756268.0854, Val Loss: 39165587442.6109\n",
      "Epoch [520/5000], Train Loss: 99099528888.1284, Val Loss: 18834555438.4695\n",
      "Epoch [540/5000], Train Loss: 22570738971.0050, Val Loss: 40024834843.5530\n",
      "Epoch [560/5000], Train Loss: 68365845884.9491, Val Loss: 52841740694.4327\n",
      "Epoch [580/5000], Train Loss: 95201157194.9200, Val Loss: 48564469040.4227\n",
      "Epoch [600/5000], Train Loss: 65666614293.9403, Val Loss: 38525111781.1871\n",
      "Epoch [620/5000], Train Loss: 61235254536.6830, Val Loss: 32316176114.5320\n",
      "Epoch [640/5000], Train Loss: 17340687294.8855, Val Loss: 47398351519.7123\n",
      "Epoch [660/5000], Train Loss: 67550579116.9350, Val Loss: 30443912785.1919\n",
      "Epoch [680/5000], Train Loss: 80533216903.3131, Val Loss: 28524313661.1647\n",
      "Epoch [700/5000], Train Loss: 74564373366.5126, Val Loss: 24751225305.7659\n",
      "Epoch [720/5000], Train Loss: 121911908473.3421, Val Loss: 43011373012.5169\n",
      "Epoch [740/5000], Train Loss: 40920513098.0420, Val Loss: 40157103338.1181\n",
      "Epoch [760/5000], Train Loss: 37306111360.1903, Val Loss: 26326459921.4882\n",
      "Epoch [780/5000], Train Loss: 145824404176.4848, Val Loss: 26978657919.7806\n",
      "Epoch [800/5000], Train Loss: 112436515276.6622, Val Loss: 33812576760.5908\n",
      "Epoch [820/5000], Train Loss: 54135228886.8381, Val Loss: 16603443380.1160\n",
      "Epoch [840/5000], Train Loss: 46845382972.4495, Val Loss: 36574330338.1999\n",
      "Epoch [860/5000], Train Loss: 61743631554.5210, Val Loss: 53278869920.7685\n",
      "Epoch [880/5000], Train Loss: 87345163506.7286, Val Loss: 28739071776.2796\n",
      "Epoch [900/5000], Train Loss: 43990688411.5449, Val Loss: 26285825117.8426\n",
      "Epoch [920/5000], Train Loss: 31373677047.2286, Val Loss: 27931984056.7060\n",
      "Epoch [940/5000], Train Loss: 38484474110.0012, Val Loss: 20005812383.1384\n",
      "Epoch [960/5000], Train Loss: 47159934472.3942, Val Loss: 26095892489.9967\n",
      "Epoch [980/5000], Train Loss: 66629806472.4013, Val Loss: 18937367810.5227\n",
      "Epoch [1000/5000], Train Loss: 30445724384.8973, Val Loss: 20840404002.9800\n",
      "Epoch [1020/5000], Train Loss: 63009593926.6289, Val Loss: 23652271970.2139\n",
      "Epoch [1040/5000], Train Loss: 32297347294.7610, Val Loss: 25553327856.0835\n",
      "Epoch [1060/5000], Train Loss: 34549357058.7214, Val Loss: 25853438779.0850\n",
      "Epoch [1080/5000], Train Loss: 23341209088.2856, Val Loss: 25947688628.4510\n",
      "Epoch [1100/5000], Train Loss: 63205162072.7470, Val Loss: 25461415618.2474\n",
      "Epoch [1120/5000], Train Loss: 40834145459.1866, Val Loss: 31079277935.3146\n",
      "Epoch [1140/5000], Train Loss: 40300665282.9930, Val Loss: 31196607923.4664\n",
      "Epoch [1160/5000], Train Loss: 26604225930.9657, Val Loss: 23104479466.2399\n",
      "Epoch [1180/5000], Train Loss: 27040161343.6315, Val Loss: 45077086722.3680\n",
      "Epoch [1200/5000], Train Loss: 71338210916.2788, Val Loss: 43720716975.2903\n",
      "Epoch [1220/5000], Train Loss: 36005218970.7262, Val Loss: 31137226729.7463\n",
      "Epoch [1240/5000], Train Loss: 41012328737.6600, Val Loss: 26863270528.0329\n",
      "Epoch [1260/5000], Train Loss: 63898197454.2206, Val Loss: 30402396438.8473\n",
      "Epoch [1280/5000], Train Loss: 87714335529.7492, Val Loss: 24179883474.7223\n",
      "Epoch [1300/5000], Train Loss: 66364720627.6266, Val Loss: 22947918485.2213\n",
      "Early stopping at epoch 1318\n",
      "\n",
      "Fold number: 4\n",
      "[0.42071154887438345]\n",
      "MAPE =  0.42071154887438345\n",
      "-\n",
      "mape score =  [[0.4906880357936948], [0.4285724387063681], [0.4710498178580623], [0.4212248962482277], [0.42071154887438345]]\n",
      ">>>>>>>>>>>>>>>>>>>>000998_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch [20/5000], Train Loss: 217954096475873.8750, Val Loss: 41803140169679.1797\n",
      "Epoch [40/5000], Train Loss: 21413106205824.6406, Val Loss: 16824179394095.7168\n",
      "Epoch [60/5000], Train Loss: 10839493739974.0918, Val Loss: 9895924473982.5273\n",
      "Epoch [80/5000], Train Loss: 2938260456853.6260, Val Loss: 8093722170441.2793\n",
      "Epoch [100/5000], Train Loss: 55729209718440.2031, Val Loss: 7365714889327.5254\n",
      "Epoch [120/5000], Train Loss: 16337811288112.5938, Val Loss: 6087731049265.8477\n",
      "Epoch [140/5000], Train Loss: 12685084344056.7422, Val Loss: 5950473668698.8174\n",
      "Epoch [160/5000], Train Loss: 1779801163543.1941, Val Loss: 4946885551242.2197\n",
      "Epoch [180/5000], Train Loss: 2134339457917.0759, Val Loss: 4698468228518.4629\n",
      "Epoch [200/5000], Train Loss: 6772734646771.2412, Val Loss: 4690816921684.6387\n",
      "Epoch [220/5000], Train Loss: 10973926190996.8359, Val Loss: 3795004796276.2339\n",
      "Epoch [240/5000], Train Loss: 790443102464.0244, Val Loss: 3922667683027.1743\n",
      "Epoch [260/5000], Train Loss: 786882785906.1140, Val Loss: 3793831370080.7495\n",
      "Epoch [280/5000], Train Loss: 4269338656850.0044, Val Loss: 3757985513743.5869\n",
      "Epoch [300/5000], Train Loss: 3645527710338.9360, Val Loss: 4294303934089.5103\n",
      "Epoch [320/5000], Train Loss: 1654888830572.1323, Val Loss: 5306051958282.5518\n",
      "Epoch [340/5000], Train Loss: 822700020620.1780, Val Loss: 4354199599316.8931\n",
      "Epoch [360/5000], Train Loss: 9253594796636.7656, Val Loss: 3753277821697.5488\n",
      "Epoch [380/5000], Train Loss: 247439085027.3924, Val Loss: 3537755256920.3428\n",
      "Epoch [400/5000], Train Loss: 765132177025.5107, Val Loss: 3634361339076.6724\n",
      "Epoch [420/5000], Train Loss: 5272839937167.0811, Val Loss: 5278332377351.1680\n",
      "Epoch [440/5000], Train Loss: 1525685537085.6631, Val Loss: 3471436895980.9121\n",
      "Epoch [460/5000], Train Loss: 556393285538.3572, Val Loss: 3437601372680.4688\n",
      "Epoch [480/5000], Train Loss: 4858400236034.0352, Val Loss: 3237600535127.1851\n",
      "Epoch [500/5000], Train Loss: 12056262249864.6074, Val Loss: 5213895172402.7754\n",
      "Epoch [520/5000], Train Loss: 4735178299168.3926, Val Loss: 3319784418507.6426\n",
      "Epoch [540/5000], Train Loss: 326808153813.7603, Val Loss: 3508707689358.7466\n",
      "Epoch [560/5000], Train Loss: 3232913685002.5200, Val Loss: 4141091356026.3760\n",
      "Epoch [580/5000], Train Loss: 1394591087550.2019, Val Loss: 3218667228784.9102\n",
      "Epoch [600/5000], Train Loss: 611014562473.1538, Val Loss: 3311650826590.5947\n",
      "Epoch [620/5000], Train Loss: 463250335915.5892, Val Loss: 2977424222390.2881\n",
      "Epoch [640/5000], Train Loss: 299900739205.4929, Val Loss: 3460216803981.8530\n",
      "Epoch [660/5000], Train Loss: 712408950792.7957, Val Loss: 2938053994161.4561\n",
      "Epoch [680/5000], Train Loss: 713562702462.8347, Val Loss: 3084135177948.7974\n",
      "Epoch [700/5000], Train Loss: 2070751113655.5576, Val Loss: 2940725064459.1089\n",
      "Epoch [720/5000], Train Loss: 1834703940796.0901, Val Loss: 5413548935543.6367\n",
      "Epoch [740/5000], Train Loss: 973411774758.3228, Val Loss: 2821765408252.3379\n",
      "Epoch [760/5000], Train Loss: 3601096310761.4321, Val Loss: 3137230627040.4619\n",
      "Epoch [780/5000], Train Loss: 409767173666.3616, Val Loss: 2155582921397.1919\n",
      "Epoch [800/5000], Train Loss: 1696913989841.2432, Val Loss: 2847235703023.1973\n",
      "Epoch [820/5000], Train Loss: 2108771116800.3091, Val Loss: 2881527034461.5332\n",
      "Epoch [840/5000], Train Loss: 2083764523049.1189, Val Loss: 3049848880959.8428\n",
      "Epoch [860/5000], Train Loss: 1290219287670.9253, Val Loss: 3248292520128.5439\n",
      "Epoch [880/5000], Train Loss: 655272065655.0574, Val Loss: 3091854626450.7275\n",
      "Epoch [900/5000], Train Loss: 649353506243.4209, Val Loss: 2978232677828.1943\n",
      "Epoch [920/5000], Train Loss: 648162420738.4683, Val Loss: 3194259566925.4375\n",
      "Early stopping at epoch 921\n",
      "\n",
      "Fold number: 0\n",
      "[0.4842028749511463]\n",
      "MAPE =  0.4842028749511463\n",
      "\n",
      "Epoch [20/5000], Train Loss: 64024183753146.1250, Val Loss: 40339839593065.9844\n",
      "Epoch [40/5000], Train Loss: 3922835153082.0635, Val Loss: 18836875017609.2578\n",
      "Epoch [60/5000], Train Loss: 14722743253027.7148, Val Loss: 7885304193817.0674\n",
      "Epoch [80/5000], Train Loss: 234223002677346.1562, Val Loss: 4955868017154.2959\n",
      "Epoch [100/5000], Train Loss: 48611588086187.4766, Val Loss: 3964982905012.4185\n",
      "Epoch [120/5000], Train Loss: 15134107626850.3984, Val Loss: 3570080771697.6821\n",
      "Epoch [140/5000], Train Loss: 13194889542538.5078, Val Loss: 2843779007241.8521\n",
      "Epoch [160/5000], Train Loss: 959679490571.2397, Val Loss: 2538107745332.7534\n",
      "Epoch [180/5000], Train Loss: 2707350947439.1904, Val Loss: 2128402978672.6765\n",
      "Epoch [200/5000], Train Loss: 479589137544.9728, Val Loss: 1951441421400.4343\n",
      "Epoch [220/5000], Train Loss: 280253781460.0602, Val Loss: 1863756530556.3499\n",
      "Epoch [240/5000], Train Loss: 1142338037345.1982, Val Loss: 1696560532967.9653\n",
      "Epoch [260/5000], Train Loss: 1212761786997.2839, Val Loss: 1808908891200.6316\n",
      "Epoch [280/5000], Train Loss: 567760340196.6541, Val Loss: 1727235645865.4500\n",
      "Epoch [300/5000], Train Loss: 4483872810055.7939, Val Loss: 1672180883834.5579\n",
      "Epoch [320/5000], Train Loss: 1968932579363.1636, Val Loss: 3043563091097.3589\n",
      "Epoch [340/5000], Train Loss: 544961261464.2382, Val Loss: 1727814810464.9717\n",
      "Epoch [360/5000], Train Loss: 1900184248591.4192, Val Loss: 1716516075753.0752\n",
      "Epoch [380/5000], Train Loss: 463141392794.0112, Val Loss: 1806955010357.0823\n",
      "Epoch [400/5000], Train Loss: 1400229424748.3125, Val Loss: 1739257286080.6597\n",
      "Epoch [420/5000], Train Loss: 1461703475490.2109, Val Loss: 2892794071810.6187\n",
      "Epoch [440/5000], Train Loss: 989820572872.0729, Val Loss: 1716378999959.9785\n",
      "Epoch [460/5000], Train Loss: 338189189885.3630, Val Loss: 1699727340874.3308\n",
      "Epoch [480/5000], Train Loss: 6073371351288.7607, Val Loss: 3076120271675.0522\n",
      "Epoch [500/5000], Train Loss: 10897837501530.6484, Val Loss: 1596217434900.7832\n",
      "Epoch [520/5000], Train Loss: 6178193319669.5996, Val Loss: 3067003743541.1812\n",
      "Epoch [540/5000], Train Loss: 3055653111874.4941, Val Loss: 1615619123177.9482\n",
      "Epoch [560/5000], Train Loss: 1666537773733.6692, Val Loss: 1768437536315.8186\n",
      "Epoch [580/5000], Train Loss: 2623415633553.4839, Val Loss: 1777112265701.8748\n",
      "Epoch [600/5000], Train Loss: 1501594780075.1973, Val Loss: 2945601490639.4634\n",
      "Epoch [620/5000], Train Loss: 586786007855.7197, Val Loss: 1824646822510.4910\n",
      "Epoch [640/5000], Train Loss: 501326234723.9781, Val Loss: 1778887749825.2520\n",
      "Epoch [660/5000], Train Loss: 541640340378.3687, Val Loss: 1962820086539.1091\n",
      "Epoch [680/5000], Train Loss: 269059797021.3992, Val Loss: 1847533834681.9004\n",
      "Epoch [700/5000], Train Loss: 2154937259081.4583, Val Loss: 1831792505870.8745\n",
      "Epoch [720/5000], Train Loss: 859847881669.3772, Val Loss: 1958241092988.0332\n",
      "Epoch [740/5000], Train Loss: 1522726148546.1650, Val Loss: 1988627089236.4578\n",
      "Epoch [760/5000], Train Loss: 2886623407787.7773, Val Loss: 1844630896839.1223\n",
      "Epoch [780/5000], Train Loss: 530533223296.7341, Val Loss: 1858099764422.1848\n",
      "Epoch [800/5000], Train Loss: 1580973057432.1570, Val Loss: 3047820099456.8257\n",
      "Epoch [820/5000], Train Loss: 2111774985916.0886, Val Loss: 2029631197111.7034\n",
      "Early stopping at epoch 821\n",
      "\n",
      "Fold number: 1\n",
      "[0.6434898915799535]\n",
      "MAPE =  0.6434898915799535\n",
      "\n",
      "Epoch [20/5000], Train Loss: 45018880052680.4297, Val Loss: 190633790498889.5625\n",
      "Epoch [40/5000], Train Loss: 2097760387325.9053, Val Loss: 15480384662966.3711\n",
      "Epoch [60/5000], Train Loss: 36642386842047.7812, Val Loss: 7307522832843.2012\n",
      "Epoch [80/5000], Train Loss: 249007112508918.8438, Val Loss: 6550246780260.2070\n",
      "Epoch [100/5000], Train Loss: 51285382431730.6250, Val Loss: 5841411728164.5615\n",
      "Epoch [120/5000], Train Loss: 20929779428177.3906, Val Loss: 5076688933629.3818\n",
      "Epoch [140/5000], Train Loss: 7982904369555.5088, Val Loss: 4400090720434.4697\n",
      "Epoch [160/5000], Train Loss: 833879295815.1724, Val Loss: 3653743947945.0000\n",
      "Epoch [180/5000], Train Loss: 1329129542477.9727, Val Loss: 3200291629110.1089\n",
      "Epoch [200/5000], Train Loss: 28175398178318.0352, Val Loss: 2561014613737.1094\n",
      "Epoch [220/5000], Train Loss: 667747120424.9796, Val Loss: 2123662676856.5413\n",
      "Epoch [240/5000], Train Loss: 2855035803644.7017, Val Loss: 2036899187037.8635\n",
      "Epoch [260/5000], Train Loss: 2462659203116.7334, Val Loss: 1814811435016.2722\n",
      "Epoch [280/5000], Train Loss: 626224886477.8835, Val Loss: 4024248718953.8252\n",
      "Epoch [300/5000], Train Loss: 935978781883.5636, Val Loss: 1769152846355.7310\n",
      "Epoch [320/5000], Train Loss: 2038955652151.8801, Val Loss: 4193228949807.3931\n",
      "Epoch [340/5000], Train Loss: 1481582913420.9702, Val Loss: 4087684331428.1870\n",
      "Epoch [360/5000], Train Loss: 1235811469207.2957, Val Loss: 4101773615910.4155\n",
      "Epoch [380/5000], Train Loss: 3539341032866.8296, Val Loss: 3998164204642.8970\n",
      "Epoch [400/5000], Train Loss: 1509205188806.1987, Val Loss: 1954518183477.6592\n",
      "Epoch [420/5000], Train Loss: 2921035219752.8994, Val Loss: 1375725257665.4612\n",
      "Epoch [440/5000], Train Loss: 950016557720.4652, Val Loss: 1619328351605.9622\n",
      "Epoch [460/5000], Train Loss: 1001757849929.3167, Val Loss: 1619777765586.3733\n",
      "Epoch [480/5000], Train Loss: 318151016933.2520, Val Loss: 1491004111290.7769\n",
      "Epoch [500/5000], Train Loss: 11449963777596.2266, Val Loss: 4129796677668.0918\n",
      "Epoch [520/5000], Train Loss: 4953911268311.0020, Val Loss: 3138014442979.0933\n",
      "Epoch [540/5000], Train Loss: 696606384833.4646, Val Loss: 3695185887845.7778\n",
      "Epoch [560/5000], Train Loss: 917287279158.8330, Val Loss: 2192242275180.9141\n",
      "Epoch [580/5000], Train Loss: 1932659842805.5322, Val Loss: 1654620715441.7454\n",
      "Epoch [600/5000], Train Loss: 879076367905.8265, Val Loss: 2478634486081.9136\n",
      "Epoch [620/5000], Train Loss: 415083612592.3544, Val Loss: 1668978216830.8655\n",
      "Epoch [640/5000], Train Loss: 156131018387.1802, Val Loss: 1704454987570.9182\n",
      "Epoch [660/5000], Train Loss: 1917081513603.8164, Val Loss: 1774214267502.9932\n",
      "Epoch [680/5000], Train Loss: 845346809361.4830, Val Loss: 2174134234622.1689\n",
      "Epoch [700/5000], Train Loss: 595649423574.7803, Val Loss: 1310055454621.5984\n",
      "Epoch [720/5000], Train Loss: 1384557275619.4055, Val Loss: 2085667058297.4541\n",
      "Epoch [740/5000], Train Loss: 4338003462935.7109, Val Loss: 2066223883620.8950\n",
      "Epoch [760/5000], Train Loss: 2128667853898.2913, Val Loss: 3758751965697.7930\n",
      "Epoch [780/5000], Train Loss: 536737334765.3759, Val Loss: 2065701569155.9856\n",
      "Epoch [800/5000], Train Loss: 369461011857.7981, Val Loss: 3819566506812.8994\n",
      "Early stopping at epoch 811\n",
      "\n",
      "Fold number: 2\n",
      "[0.525609412620512]\n",
      "MAPE =  0.525609412620512\n",
      "\n",
      "Epoch [20/5000], Train Loss: 62844802672384.4609, Val Loss: 55954879323587.6094\n",
      "Epoch [40/5000], Train Loss: 5511838023795.4199, Val Loss: 11673208276122.3770\n",
      "Epoch [60/5000], Train Loss: 35083687805636.4844, Val Loss: 6198934711655.6143\n",
      "Epoch [80/5000], Train Loss: 246843504220559.4688, Val Loss: 5879860960739.6562\n",
      "Epoch [100/5000], Train Loss: 1189732811104.8152, Val Loss: 3412664909489.4614\n",
      "Epoch [120/5000], Train Loss: 8476808585842.3242, Val Loss: 2477330194268.8418\n",
      "Epoch [140/5000], Train Loss: 11677773045978.5234, Val Loss: 2842169330437.5747\n",
      "Epoch [160/5000], Train Loss: 1558363722212.2209, Val Loss: 1937876195985.5845\n",
      "Epoch [180/5000], Train Loss: 1427768746922.7119, Val Loss: 1292199927225.4338\n",
      "Epoch [200/5000], Train Loss: 25335219104647.2422, Val Loss: 1358494915291.6328\n",
      "Epoch [220/5000], Train Loss: 611599850130.8040, Val Loss: 1365183992907.4700\n",
      "Epoch [240/5000], Train Loss: 12101509619730.6582, Val Loss: 1245809596320.1377\n",
      "Epoch [260/5000], Train Loss: 3057027531009.8384, Val Loss: 1288776145821.9500\n",
      "Epoch [280/5000], Train Loss: 745428180240.0542, Val Loss: 2546155355886.6001\n",
      "Epoch [300/5000], Train Loss: 2318571433773.2661, Val Loss: 2542559423738.3760\n",
      "Epoch [320/5000], Train Loss: 2266307928733.0415, Val Loss: 1134826059871.8318\n",
      "Epoch [340/5000], Train Loss: 807296799042.7274, Val Loss: 1155928533997.0232\n",
      "Epoch [360/5000], Train Loss: 362966422275.7104, Val Loss: 1216420371064.7600\n",
      "Epoch [380/5000], Train Loss: 3355280672366.2842, Val Loss: 2030295788738.3359\n",
      "Epoch [400/5000], Train Loss: 2393439098365.1846, Val Loss: 1749865166344.7864\n",
      "Epoch [420/5000], Train Loss: 2362094579802.1875, Val Loss: 2015576857206.2190\n",
      "Epoch [440/5000], Train Loss: 827938413037.3647, Val Loss: 2018745254868.3542\n",
      "Epoch [460/5000], Train Loss: 448795609229.2875, Val Loss: 1700972640208.7703\n",
      "Epoch [480/5000], Train Loss: 337095834245.9946, Val Loss: 1476354497526.2815\n",
      "Epoch [500/5000], Train Loss: 738828663417.5084, Val Loss: 1495038585506.6277\n",
      "Epoch [520/5000], Train Loss: 5463365140812.9072, Val Loss: 1482757434344.6409\n",
      "Epoch [540/5000], Train Loss: 1317473365562.6987, Val Loss: 1516727670260.1433\n",
      "Epoch [560/5000], Train Loss: 1214292363422.2664, Val Loss: 1835299055297.4241\n",
      "Epoch [580/5000], Train Loss: 1043757653264.9271, Val Loss: 1404658864761.0762\n",
      "Epoch [600/5000], Train Loss: 696760306021.5667, Val Loss: 1136813534013.4675\n",
      "Epoch [620/5000], Train Loss: 1828483644547.7625, Val Loss: 1788398167713.5520\n",
      "Epoch [640/5000], Train Loss: 219695091959.3221, Val Loss: 1475196053913.4775\n",
      "Epoch [660/5000], Train Loss: 2911822720255.8184, Val Loss: 1206232086551.4385\n",
      "Epoch [680/5000], Train Loss: 944232950698.4022, Val Loss: 1625618659345.8872\n",
      "Epoch [700/5000], Train Loss: 450214717481.1232, Val Loss: 1587688919762.9912\n",
      "Epoch [720/5000], Train Loss: 454946407483.3615, Val Loss: 1169046577846.5198\n",
      "Epoch [740/5000], Train Loss: 1579335746266.7739, Val Loss: 1142105937725.6970\n",
      "Epoch [760/5000], Train Loss: 9273832627308.7891, Val Loss: 2446887982413.9624\n",
      "Epoch [780/5000], Train Loss: 682961082039.1538, Val Loss: 2130339288085.8916\n",
      "Epoch [800/5000], Train Loss: 1365567712215.0928, Val Loss: 2134244006744.1948\n",
      "Epoch [820/5000], Train Loss: 787348789506.4171, Val Loss: 1046569256563.2506\n",
      "Epoch [840/5000], Train Loss: 163048938469.6769, Val Loss: 1059583837103.5975\n",
      "Epoch [860/5000], Train Loss: 1849193613865.1338, Val Loss: 1082013312970.1978\n",
      "Epoch [880/5000], Train Loss: 2719532510581.8481, Val Loss: 2156277482270.7800\n",
      "Epoch [900/5000], Train Loss: 6162776750036.2207, Val Loss: 1083279627429.6006\n",
      "Epoch [920/5000], Train Loss: 744630436547.4935, Val Loss: 847262757654.3297\n",
      "Epoch [940/5000], Train Loss: 7000365326727.4580, Val Loss: 1043389743688.4028\n",
      "Epoch [960/5000], Train Loss: 522629262025.9910, Val Loss: 2322819567369.9741\n",
      "Epoch [980/5000], Train Loss: 1101376434773.2549, Val Loss: 2104725160949.2764\n",
      "Epoch [1000/5000], Train Loss: 711811838918.8124, Val Loss: 1165257366191.8772\n",
      "Epoch [1020/5000], Train Loss: 1124728770544.7856, Val Loss: 1215858650178.5535\n",
      "Epoch [1040/5000], Train Loss: 4016303364458.0278, Val Loss: 2088762725356.9824\n",
      "Epoch [1060/5000], Train Loss: 4292949947397.6597, Val Loss: 1246631362771.4951\n",
      "Epoch [1080/5000], Train Loss: 792439068581.5227, Val Loss: 1159009117285.7268\n",
      "Epoch [1100/5000], Train Loss: 737249778262.0510, Val Loss: 2067351973319.3213\n",
      "Epoch [1120/5000], Train Loss: 494827989500.2680, Val Loss: 1108896537042.6487\n",
      "Epoch [1140/5000], Train Loss: 1507134091038.4180, Val Loss: 1208165266753.3120\n",
      "Epoch [1160/5000], Train Loss: 454648352826.1723, Val Loss: 1276177640835.7888\n",
      "Epoch [1180/5000], Train Loss: 403881605627.9555, Val Loss: 1951347157286.9517\n",
      "Epoch [1200/5000], Train Loss: 7085758456698.2363, Val Loss: 1329862511422.3855\n",
      "Epoch [1220/5000], Train Loss: 705848245003.2838, Val Loss: 2134157489591.6404\n",
      "Epoch [1240/5000], Train Loss: 1164634284582.9375, Val Loss: 2009242661961.1599\n",
      "Epoch [1260/5000], Train Loss: 2928399138875.3149, Val Loss: 1232129275511.9558\n",
      "Epoch [1280/5000], Train Loss: 884985418289.4161, Val Loss: 1979292412159.1345\n",
      "Epoch [1300/5000], Train Loss: 576691530192.0660, Val Loss: 1117086991724.4346\n",
      "Epoch [1320/5000], Train Loss: 20618882549136.9375, Val Loss: 1114478000777.1050\n",
      "Epoch [1340/5000], Train Loss: 3834822051055.3359, Val Loss: 1144762245658.3525\n",
      "Epoch [1360/5000], Train Loss: 4900699992308.9961, Val Loss: 2087123867496.9890\n",
      "Epoch [1380/5000], Train Loss: 1529889632206.0562, Val Loss: 1152169419184.3987\n",
      "Epoch [1400/5000], Train Loss: 2301906121147.3696, Val Loss: 1381244701960.7136\n",
      "Early stopping at epoch 1416\n",
      "\n",
      "Fold number: 3\n",
      "[0.43603013050223055]\n",
      "MAPE =  0.43603013050223055\n",
      "\n",
      "Epoch [20/5000], Train Loss: 240467049827684.8750, Val Loss: 39499208454861.1016\n",
      "Epoch [40/5000], Train Loss: 12341623364038.9551, Val Loss: 20597173937030.4883\n",
      "Epoch [60/5000], Train Loss: 3481393237980.5640, Val Loss: 11410027268763.1289\n",
      "Epoch [80/5000], Train Loss: 11962545915061.5410, Val Loss: 8666134295786.2188\n",
      "Epoch [100/5000], Train Loss: 4729982352828.3828, Val Loss: 6522527560403.8486\n",
      "Epoch [120/5000], Train Loss: 11443473203525.7090, Val Loss: 5126896679586.8760\n",
      "Epoch [140/5000], Train Loss: 1556343956511.3040, Val Loss: 4049713841262.7969\n",
      "Epoch [160/5000], Train Loss: 2520350647435.7925, Val Loss: 3645163224369.9131\n",
      "Epoch [180/5000], Train Loss: 3267045913883.4443, Val Loss: 3097763430362.8989\n",
      "Epoch [200/5000], Train Loss: 1382719400386.1113, Val Loss: 2802637408165.2368\n",
      "Epoch [220/5000], Train Loss: 6537181663478.1807, Val Loss: 2836475569521.4087\n",
      "Epoch [240/5000], Train Loss: 984226490306.9515, Val Loss: 2942659795159.5620\n",
      "Epoch [260/5000], Train Loss: 16476778986883.5508, Val Loss: 2893563016980.2944\n",
      "Epoch [280/5000], Train Loss: 1228373754679.7622, Val Loss: 2853188690034.0806\n",
      "Epoch [300/5000], Train Loss: 2520405171322.0693, Val Loss: 2589221570442.2471\n",
      "Epoch [320/5000], Train Loss: 915990755114.4834, Val Loss: 2765082750140.7881\n",
      "Epoch [340/5000], Train Loss: 804317685809.2719, Val Loss: 2764730408888.7803\n",
      "Epoch [360/5000], Train Loss: 2370202482617.0776, Val Loss: 2636023464801.8228\n",
      "Epoch [380/5000], Train Loss: 982882740318.3390, Val Loss: 2706154078737.1836\n",
      "Epoch [400/5000], Train Loss: 2440155439901.3740, Val Loss: 2787525257331.7891\n",
      "Epoch [420/5000], Train Loss: 1890111736366.8894, Val Loss: 2759692465148.3047\n",
      "Epoch [440/5000], Train Loss: 2033530796027.5083, Val Loss: 2628909986395.7134\n",
      "Epoch [460/5000], Train Loss: 2282158821394.9619, Val Loss: 2523115147364.1909\n",
      "Epoch [480/5000], Train Loss: 590908427072.8787, Val Loss: 3808533239196.6631\n",
      "Epoch [500/5000], Train Loss: 1454416765453.3713, Val Loss: 2512936160092.0171\n",
      "Epoch [520/5000], Train Loss: 893603999740.5449, Val Loss: 2417008506852.3252\n",
      "Epoch [540/5000], Train Loss: 280733039546.4129, Val Loss: 2643251882033.0405\n",
      "Epoch [560/5000], Train Loss: 912709558554.1571, Val Loss: 2386345349899.7227\n",
      "Epoch [580/5000], Train Loss: 24800471193270.4922, Val Loss: 3731278622642.3384\n",
      "Epoch [600/5000], Train Loss: 2009050213902.6072, Val Loss: 3074003301397.0952\n",
      "Epoch [620/5000], Train Loss: 891569291953.5099, Val Loss: 2652694770213.9365\n",
      "Epoch [640/5000], Train Loss: 642460687304.3286, Val Loss: 2625493332476.5820\n",
      "Epoch [660/5000], Train Loss: 30481595326427.6680, Val Loss: 2455830556879.7378\n",
      "Epoch [680/5000], Train Loss: 397736515155.9480, Val Loss: 2705774596653.3042\n",
      "Epoch [700/5000], Train Loss: 1233024427870.9575, Val Loss: 2498287102256.8569\n",
      "Epoch [720/5000], Train Loss: 1276818608133.6914, Val Loss: 2227477503971.5020\n",
      "Epoch [740/5000], Train Loss: 165081636643.9474, Val Loss: 2550576865922.3896\n",
      "Epoch [760/5000], Train Loss: 2589134792499.8931, Val Loss: 2432432966300.4712\n",
      "Epoch [780/5000], Train Loss: 812504182504.2244, Val Loss: 3342301201510.8706\n",
      "Epoch [800/5000], Train Loss: 11168008858511.2754, Val Loss: 3969731866223.6924\n",
      "Epoch [820/5000], Train Loss: 2964714213812.5430, Val Loss: 2095002239162.4714\n",
      "Epoch [840/5000], Train Loss: 419771506161.3859, Val Loss: 2799614329126.6118\n",
      "Epoch [860/5000], Train Loss: 242355502712.5659, Val Loss: 3044691338545.0239\n",
      "Epoch [880/5000], Train Loss: 1318666365378.2754, Val Loss: 3811749897078.3096\n",
      "Epoch [900/5000], Train Loss: 452128980620.3118, Val Loss: 2276913222704.1709\n",
      "Epoch [920/5000], Train Loss: 6876259414242.3271, Val Loss: 2548717538152.7837\n",
      "Epoch [940/5000], Train Loss: 1343436783832.4968, Val Loss: 3045490145573.5322\n",
      "Epoch [960/5000], Train Loss: 567748429782.5546, Val Loss: 2302292943849.8501\n",
      "Epoch [980/5000], Train Loss: 6020500481786.6016, Val Loss: 2581297698904.8975\n",
      "Epoch [1000/5000], Train Loss: 257797122040.6885, Val Loss: 2125859427755.6731\n",
      "Epoch [1020/5000], Train Loss: 2415207626887.7153, Val Loss: 3627936056392.0498\n",
      "Epoch [1040/5000], Train Loss: 638326235999.2943, Val Loss: 2270325157112.9678\n",
      "Epoch [1060/5000], Train Loss: 248688248897.2131, Val Loss: 2272617267978.0796\n",
      "Epoch [1080/5000], Train Loss: 3293888273937.4600, Val Loss: 2125292873956.7947\n",
      "Epoch [1100/5000], Train Loss: 4007620698389.5176, Val Loss: 2353207676229.1436\n",
      "Epoch [1120/5000], Train Loss: 1303335556475.4275, Val Loss: 2173641861796.2339\n",
      "Epoch [1140/5000], Train Loss: 935042979584.3914, Val Loss: 3925938309228.2822\n",
      "Epoch [1160/5000], Train Loss: 1427368179648.8545, Val Loss: 2304950122970.5562\n",
      "Epoch [1180/5000], Train Loss: 1826470413839.9048, Val Loss: 3139961290998.9028\n",
      "Epoch [1200/5000], Train Loss: 7152612964487.4795, Val Loss: 2353577434769.7329\n",
      "Epoch [1220/5000], Train Loss: 5817051216550.7197, Val Loss: 2380826862756.8130\n",
      "Epoch [1240/5000], Train Loss: 1899595700947.3748, Val Loss: 2373790533793.0664\n",
      "Epoch [1260/5000], Train Loss: 3397847891420.1353, Val Loss: 2638761006761.0454\n",
      "Epoch [1280/5000], Train Loss: 7731015594050.5361, Val Loss: 2471648590648.6904\n",
      "Epoch [1300/5000], Train Loss: 146663330179.4151, Val Loss: 2236397398582.7134\n",
      "Epoch [1320/5000], Train Loss: 1767558216627.1155, Val Loss: 3019876854901.5347\n",
      "Epoch [1340/5000], Train Loss: 664898268211.9618, Val Loss: 2232186483132.2319\n",
      "Epoch [1360/5000], Train Loss: 753994797483.2683, Val Loss: 2199154035832.9199\n",
      "Epoch [1380/5000], Train Loss: 2788846615634.9526, Val Loss: 2173442961019.2815\n",
      "Epoch [1400/5000], Train Loss: 1709878252043.9175, Val Loss: 2124731425518.4666\n",
      "Epoch [1420/5000], Train Loss: 3279068075883.6211, Val Loss: 2265575394438.7158\n",
      "Epoch [1440/5000], Train Loss: 5130188910109.0400, Val Loss: 2591874957906.6851\n",
      "Epoch [1460/5000], Train Loss: 3459623971789.8457, Val Loss: 3297474966704.5820\n",
      "Epoch [1480/5000], Train Loss: 1376564149250.7107, Val Loss: 2087805498105.6113\n",
      "Epoch [1500/5000], Train Loss: 587046290428.1427, Val Loss: 2127211580292.2869\n",
      "Epoch [1520/5000], Train Loss: 890553390899.9897, Val Loss: 2787946006574.9424\n",
      "Epoch [1540/5000], Train Loss: 853455070116.2585, Val Loss: 2359215026959.5415\n",
      "Epoch [1560/5000], Train Loss: 1316161927551.1150, Val Loss: 2408102726542.5107\n",
      "Epoch [1580/5000], Train Loss: 441112222325.9629, Val Loss: 3119297178368.2896\n",
      "Epoch [1600/5000], Train Loss: 7712527808753.0625, Val Loss: 2342360007112.7515\n",
      "Epoch [1620/5000], Train Loss: 2735704905714.3071, Val Loss: 2190523257788.4102\n",
      "Epoch [1640/5000], Train Loss: 3049726527079.6792, Val Loss: 2202525277465.5913\n",
      "Epoch [1660/5000], Train Loss: 733463746929.0173, Val Loss: 3126656504539.1226\n",
      "Epoch [1680/5000], Train Loss: 719513214533.6935, Val Loss: 2619137011219.9067\n",
      "Epoch [1700/5000], Train Loss: 1116142071761.6587, Val Loss: 2375813369079.1440\n",
      "Epoch [1720/5000], Train Loss: 1308215081500.4585, Val Loss: 2526043658441.1367\n",
      "Epoch [1740/5000], Train Loss: 2907374880361.7710, Val Loss: 3407954763500.0942\n",
      "Epoch [1760/5000], Train Loss: 19269076849765.6836, Val Loss: 2464034604842.2676\n",
      "Epoch [1780/5000], Train Loss: 1642765629785.0979, Val Loss: 2273635502632.7778\n",
      "Epoch [1800/5000], Train Loss: 1161172261268.6868, Val Loss: 2253351164734.0986\n",
      "Epoch [1820/5000], Train Loss: 419983151420.3904, Val Loss: 2225430558260.4385\n",
      "Epoch [1840/5000], Train Loss: 265235048953.0286, Val Loss: 3592974011640.7388\n",
      "Epoch [1860/5000], Train Loss: 5863511657457.5967, Val Loss: 2598471303145.3740\n",
      "Epoch [1880/5000], Train Loss: 1244465863270.8389, Val Loss: 2438868679592.1846\n",
      "Epoch [1900/5000], Train Loss: 1848361904119.0491, Val Loss: 2385895957921.7119\n",
      "Epoch [1920/5000], Train Loss: 390950105869.7001, Val Loss: 2285989338758.1567\n",
      "Epoch [1940/5000], Train Loss: 10912515206460.7891, Val Loss: 2254490547310.1406\n",
      "Epoch [1960/5000], Train Loss: 1563708273871.2173, Val Loss: 2348686037856.8818\n",
      "Early stopping at epoch 1974\n",
      "\n",
      "Fold number: 4\n",
      "[0.4721488970105877]\n",
      "MAPE =  0.4721488970105877\n",
      "-\n",
      "mape score =  [[0.4842028749511463], [0.6434898915799535], [0.525609412620512], [0.43603013050223055], [0.4721488970105877]]\n",
      ">>>>>>>>>>>>>>>>>>>>002282_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Epoch [20/5000], Train Loss: 53057839645.9456, Val Loss: 459293882407.1779\n",
      "Epoch [40/5000], Train Loss: 686857138752.2987, Val Loss: 161724043379.6044\n",
      "Epoch [60/5000], Train Loss: 16051747597.0323, Val Loss: 108326078933.8179\n",
      "Epoch [80/5000], Train Loss: 103360064831.2520, Val Loss: 55041529027.7281\n",
      "Epoch [100/5000], Train Loss: 27767437939.6017, Val Loss: 24705067746.1722\n",
      "Epoch [120/5000], Train Loss: 106807975914.6138, Val Loss: 19658045269.0547\n",
      "Epoch [140/5000], Train Loss: 72191249895.3057, Val Loss: 14073324087.2919\n",
      "Epoch [160/5000], Train Loss: 5615147745.7356, Val Loss: 10012317995.8913\n",
      "Epoch [180/5000], Train Loss: 35477149572.2522, Val Loss: 9453652540.8962\n",
      "Epoch [200/5000], Train Loss: 4252098428.4297, Val Loss: 8897427091.1325\n",
      "Epoch [220/5000], Train Loss: 2469684952.9461, Val Loss: 9092107978.0612\n",
      "Epoch [240/5000], Train Loss: 7547739622.7184, Val Loss: 8537843117.5914\n",
      "Epoch [260/5000], Train Loss: 12140358083.6700, Val Loss: 8059095809.3080\n",
      "Epoch [280/5000], Train Loss: 6868691545.8693, Val Loss: 8349731527.6655\n",
      "Epoch [300/5000], Train Loss: 2520458750.4991, Val Loss: 8656735874.9973\n",
      "Epoch [320/5000], Train Loss: 20892045705.9106, Val Loss: 7984758748.1859\n",
      "Epoch [340/5000], Train Loss: 15518515544.4840, Val Loss: 8378771070.7596\n",
      "Epoch [360/5000], Train Loss: 6350555346.8156, Val Loss: 8023445796.6536\n",
      "Epoch [380/5000], Train Loss: 7966687184.0171, Val Loss: 7976147059.3593\n",
      "Epoch [400/5000], Train Loss: 10416528659.2845, Val Loss: 7796782424.5642\n",
      "Epoch [420/5000], Train Loss: 8213093099.2262, Val Loss: 8400536349.0611\n",
      "Epoch [440/5000], Train Loss: 65991412720.9018, Val Loss: 9035243570.5028\n",
      "Epoch [460/5000], Train Loss: 3084502694.1124, Val Loss: 8015761602.2158\n",
      "Epoch [480/5000], Train Loss: 6895219371.1223, Val Loss: 7821183993.8021\n",
      "Epoch [500/5000], Train Loss: 15944377335.0409, Val Loss: 7949172987.8001\n",
      "Epoch [520/5000], Train Loss: 2925051401.9137, Val Loss: 8256039071.2841\n",
      "Epoch [540/5000], Train Loss: 5566179822.4771, Val Loss: 10067368009.6693\n",
      "Epoch [560/5000], Train Loss: 9261671671.4582, Val Loss: 7990032172.5067\n",
      "Epoch [580/5000], Train Loss: 3682080469.0156, Val Loss: 8930654987.4005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = sorted(list(set(s.split('_25')[0] for s in files)))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:5]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        mape_list = []\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(testInputs), torch.tensor(testgraphInput), torch.tensor(testTargets))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "        K = 5  # Number of folds\n",
    "        for k in range(K):\n",
    "            torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "            trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "            trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "            trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "            valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "            valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "            trainInputs = normalize(trainInputsAll[:, :])\n",
    "            valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "            # Assuming trainInputs, trainGraphInput, trainGraphFeatureInput, trainTargets are PyTorch tensors\n",
    "            train_dataset = TensorDataset(torch.tensor(trainInputs), torch.tensor(trainGraphInput),torch.tensor(trainTargets))\n",
    "            val_dataset = TensorDataset(torch.tensor(valInputs), torch.tensor(valGraphInput),torch.tensor(valTargets))\n",
    "\n",
    "            # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "            # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "        \n",
    "            model = GAT(7,8,1,0.2,0.2,6)\n",
    "            print()\n",
    "            model.fit(train_loader, val_loader)\n",
    "            predictions=model.test(test_loader)\n",
    "            torch.save(model.state_dict(), f'models/gat_{stock_info}_{lag_bin}_{lag_day}_gcn_model_iteration_{k}.pt')\n",
    "    \n",
    "            print()\n",
    "            print('Fold number:', k)\n",
    "\n",
    "            new_predictions = np.array([item.detach().numpy() for item in predictions]).flatten()\n",
    "            MAPE = []\n",
    "\n",
    "            MAPE.append(mean_absolute_percentage_error(testTargets[:], new_predictions[:]))\n",
    "            print(MAPE)\n",
    "            testTargets0 = list(testTargets)\n",
    "\n",
    "            res = {\n",
    "                'testTargets': testTargets0,\n",
    "                'new_predictions': new_predictions\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame(res)\n",
    "            res_df.to_csv(f'./result/gat_{stock_info}_{lag_bin}_{lag_day}_res_test_MAPE{k}.csv', index=False)\n",
    "\n",
    "            print('MAPE = ', np.array(MAPE).mean())\n",
    "            MAPE_mean = np.array(MAPE).mean()\n",
    "            mape_list.append(MAPE)\n",
    "\n",
    "        print('-')\n",
    "        print('mape score = ', mape_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb60ce-22d1-488d-8ba8-004f07b4da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_attention_weights(model, graph_input):\n",
    "    \"\"\"\n",
    "    可视化注意力权重\n",
    "    Args:\n",
    "    - model: 已训练的图注意力模型\n",
    "    - graph_input: 图的邻接矩阵\n",
    "    \"\"\"\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "    # 获取注意力层\n",
    "    attention_layer = model.attentions[0]  # 假设使用的是第一个注意力层\n",
    "    # 提取节点表示\n",
    "    node_features = model(graph_input)\n",
    "    # 计算注意力权重\n",
    "    att_weights = attention_layer.get_attention_weights(node_features, graph_input)\n",
    "    # 创建图对象\n",
    "    G = nx.from_numpy_matrix(graph_input.numpy())\n",
    "    # 绘制图形\n",
    "    pos = nx.spring_layout(G)  # 定义节点位置\n",
    "    nx.draw(G, pos, with_labels=True)\n",
    "    # 绘制注意力权重\n",
    "    for i in range(att_weights.size(0)):\n",
    "        for j in range(att_weights.size(1)):\n",
    "            if graph_input[i][j] != 0:  # 如果节点之间有连接\n",
    "                plt.text(pos[i][0], pos[i][1], f'{att_weights[i][j]:.2f}', fontweight='bold', fontsize=10)\n",
    "    # 显示图形\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2148f-e205-462b-b6ae-bc59ddf3fc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe9494-54dc-489b-b48a-df8c44dbdb48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
