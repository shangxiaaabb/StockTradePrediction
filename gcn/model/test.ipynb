{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bin0': ['2021-03-07 15:00', '2021-03-08 09:30'],\n",
       " 'bin1': ['2021-03-08 09:30', '2021-03-08 09:39'],\n",
       " 'bin2': ['2021-03-08 09:39', '2021-03-08 09:48'],\n",
       " 'bin3': ['2021-03-08 09:48', '2021-03-08 09:57'],\n",
       " 'bin4': ['2021-03-08 09:57', '2021-03-08 10:06'],\n",
       " 'bin5': ['2021-03-08 10:06', '2021-03-08 10:15'],\n",
       " 'bin6': ['2021-03-08 10:15', '2021-03-08 10:24'],\n",
       " 'bin7': ['2021-03-08 10:24', '2021-03-08 10:33'],\n",
       " 'bin8': ['2021-03-08 10:33', '2021-03-08 10:42'],\n",
       " 'bin9': ['2021-03-08 10:42', '2021-03-08 10:51'],\n",
       " 'bin10': ['2021-03-08 10:51', '2021-03-08 11:30'],\n",
       " 'bin11': ['2021-03-08 11:30', '2021-03-08 13:00'],\n",
       " 'bin12': ['2021-03-08 13:00', '2021-03-08 13:09'],\n",
       " 'bin13': ['2021-03-08 13:09', '2021-03-08 13:18'],\n",
       " 'bin14': ['2021-03-08 13:18', '2021-03-08 13:27'],\n",
       " 'bin15': ['2021-03-08 13:27', '2021-03-08 13:36'],\n",
       " 'bin16': ['2021-03-08 13:36', '2021-03-08 13:45'],\n",
       " 'bin17': ['2021-03-08 13:45', '2021-03-08 13:54'],\n",
       " 'bin18': ['2021-03-08 13:54', '2021-03-08 14:03'],\n",
       " 'bin19': ['2021-03-08 14:03', '2021-03-08 14:12'],\n",
       " 'bin20': ['2021-03-08 14:12', '2021-03-08 14:21'],\n",
       " 'bin21': ['2021-03-08 14:21', '2021-03-08 14:30'],\n",
       " 'bin22': ['2021-03-08 14:30', '2021-03-08 14:39'],\n",
       " 'bin23': ['2021-03-08 14:39', '2021-03-08 14:48'],\n",
       " 'bin24': ['2021-03-08 14:48', '2021-03-08 15:00']}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BuildData import BuildData\n",
    "\n",
    "conf = {'lag_day': 3,\n",
    "        'lag_bin': 3,\n",
    "        'lag_week': 1,\n",
    "        'bin_num': 24,\n",
    "        'file_dir': '../data/0308/0308-data/',\n",
    "        'comment_dir': '../data/0308/0308-number/'}\n",
    "\n",
    "# inputs_df, output_list = BuildData(conf= conf).gen_input_output_data(file_path= '../data/0308/0308-data/000046_XSHE_25_daily.csv',\n",
    "#                                                                      comment_path= '../data/0308/0308-number/000046_comment_sentiment.csv',\n",
    "#                                                                      stock_info= None)\n",
    "\n",
    "path = '../data/volume/0308/Input/000046_3_3_inputs.npy'\n",
    "input_ = np.load('../data/volume/0308/Input/000046_3_3_inputs.npy', allow_pickle= True)\n",
    "output_matrix = np.load(path.replace('inputs', 'output').replace('Input', 'Output'), allow_pickle= True)\n",
    "input_matrix = np.array([value for item in input_ for value in item], dtype= np.float32).reshape(input_.shape[0], 13, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 13, 70])\n",
      "多头注意力结果输出： torch.Size([32, 26, 14])\n",
      "3 torch.Size([32, 26, 14])\n",
      "输出 torch.Size([32, 26, 14])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: h-jie huangjie20011001@163.com\n",
    "Date: 2024-06-23 16:19:53\n",
    "'''\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout= 0.5, alpha= 0.2, concat= True, device= device, *args, **kwargs) -> None:\n",
    "        super(GraphAttentionLayer, self).__init__(*args, **kwargs)\n",
    "        self.in_fetures = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = True\n",
    "\n",
    "        self.device = device\n",
    "        self.W = nn.Parameter(torch.empty(size= (in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def MLP(self, N):\n",
    "        self.a = nn.Parameter(torch.empty(size= (2*self.out_features, N))).to(device= device) # 2FxN\n",
    "        nn.init.xavier_uniform_(self.a.data, gain= 1.414)\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, h):\n",
    "        \"\"\"\n",
    "        infer: \n",
    "            https://github.com/Diego999/pyGAT/blob/master/layers.py\n",
    "        \"\"\"\n",
    "        h1 = torch.matmul(h, self.a[:self.out_features, :])\n",
    "        h2 = torch.matmul(h, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        # print(h1.shape, h2.shape)\n",
    "        e = h1 + h2\n",
    "        return self.leakyrelu(e)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp： input_features [B, N, in_features]\n",
    "        adj: adjacent_matrix [N, N]\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp, self.W) # 计算 w*x\n",
    "        adj = adj.to(h.device)\n",
    "        N = h.size()[1] # TODO: 对于不同的需要需改，对于batch_size 修改为2 对应节点数量\n",
    "        self.MLP(N)\n",
    "\n",
    "        e = self._prepare_attentional_mechanism_input(h)\n",
    "        zero_vec = -1e12 * torch.ones_like(e).to(h.device)   # 将没有连接的边置为负无穷\n",
    "\n",
    "        attention = torch.where(adj> 0, e, zero_vec)   # [B, N, N]\n",
    "        attention = F.softmax(attention, dim=1)    # [B, N, N]！\n",
    "        attention = F.dropout(attention, self.dropout)\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout:float=0.5, alpha: float=0.3, n_heads: int=3, *args, **kwargs) -> None:\n",
    "        super(GAT, self).__init__(*args, **kwargs)\n",
    "        self.dropout = dropout \n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   \n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout)  \n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        print('多头注意力结果输出：', x.shape) \n",
    "        x = F.dropout(x, self.dropout)\n",
    "\n",
    "        # x = self.out_att(x, adj)\n",
    "        print('3', x.shape)\n",
    "        x = F.relu(x)\n",
    "        return F.softmax(x, dim= 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # input: batch length node features\n",
    "    # output: batch length features\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    x = torch.rand(size= (32, 10, 13, 7)).to(device= device)\n",
    "    x = x.permute(0, 2, 1, 3)\n",
    "    x = x.reshape(32, 13, -1)\n",
    "    print(x.shape)\n",
    "    adj_matrix = torch.rand(size= (13, 13))\n",
    "\n",
    "    model = GAT(n_feat= 70, n_hid= 14, n_class= 7, n_heads= 2).to(device= device) # n_class 代表未来预测的日期\n",
    "    out = model(x, adj_matrix)\n",
    "    print('输出', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 13, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 13, 7)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout= 0.5, alpha= 0.2, concat= True, device= device, *args, **kwargs) -> None:\n",
    "        super(GraphAttentionLayer, self).__init__(*args, **kwargs)\n",
    "        self.in_fetures = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = True\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size= (in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def MLP(self, N):\n",
    "        self.a = nn.Parameter(torch.empty(size= (2*self.out_features, N))).to(device= device) # 2FxN \n",
    "        nn.init.xavier_uniform_(self.a.data, gain= 1.414)\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, h):\n",
    "        \"\"\"\n",
    "        infer: \n",
    "            https://github.com/Diego999/pyGAT/blob/master/layers.py\n",
    "        \"\"\"\n",
    "        h1 = torch.matmul(h, self.a[:self.out_features, :])\n",
    "        h2 = torch.matmul(h, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = h1 + h2\n",
    "        return self.leakyrelu(e)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp： input_features [B, N, in_features]\n",
    "        adj: adjacent_matrix [N, N]\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp, self.W) # 计算 w*x\n",
    "        adj = adj.to(h.device)\n",
    "        N = h.size()[1]\n",
    "        self.MLP(N)\n",
    "\n",
    "        e = self._prepare_attentional_mechanism_input(h)\n",
    "        zero_vec = -1e12 * torch.ones_like(e).to(h.device)   # 将没有连接的边置为负无穷\n",
    "\n",
    "        attention = torch.where(adj> 0, e, zero_vec)   # [B, N, N]\n",
    "        attention = F.softmax(attention, dim=1)    # [B, N, N]！\n",
    "        attention = F.dropout(attention, self.dropout)\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout:float=0.5, alpha: float=0.3, n_heads: int=3, *args, **kwargs) -> None:\n",
    "        super(GAT, self).__init__(*args, **kwargs)\n",
    "        self.dropout = dropout \n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   \n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout)  \n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2) \n",
    "        x = F.dropout(x, self.dropout)  \n",
    "        x = self.out_att(x, adj)\n",
    "        \n",
    "        #x = F.log_softmax(x, dim=2)[:, -1, :]\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    x = torch.rand(size= (10, 13, 7)).to(device= device)\n",
    "    adj_matrix = torch.rand(size= (13, 13))\n",
    "\n",
    "    model = GAT(n_feat= 7, n_hid= 14, n_class= 7, n_heads= 2).to(device= device) # n_class 代表未来预测的日期\n",
    "    out = model(x, adj_matrix)\n",
    "    print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
