{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BuildData import BuildData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from turtle import st\n",
    "from urllib import response\n",
    "import requests\n",
    "import time\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request, urllib.error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "conf = {'lag_day': 3,\n",
    "        'lag_bin': 3,\n",
    "        'lag_week': 1,\n",
    "        'bin_num': 24,\n",
    "        'file_dir': '../data/0308/'}\n",
    "\n",
    "data = pd.read_csv('../data/0308/000046_XSHE_25_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetStockTimeContent():\n",
    "    \"\"\"\n",
    "    爬取东方财富评论：\n",
    "    \n",
    "    基本功能：\n",
    "        1、爬取得到评论、时间、具体评论内容\n",
    "    \n",
    "    进阶功能：\n",
    "        1、可以争对更加多的网站进行爬取：1、东方财富；2、同花顺等\n",
    "    \n",
    "    代码编写：\n",
    "        1、将不同的功能进行隔离开，比如说：打开解析网页、存储数据等\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: dict) -> None:\n",
    "        self.conf = conf\n",
    "    \n",
    "    def open_url(self, url: str):\n",
    "        \"\"\"\n",
    "        根据输入的 url 进行解析得到网页内内容\n",
    "        \"\"\"\n",
    "        headers = self.conf['headers']\n",
    "\n",
    "        request = urllib.request.Request(url, headers= headers)\n",
    "        html = \"\"\n",
    "        try:\n",
    "            response = urllib.request.urlopen(request)\n",
    "            html = response.read().decode(\"utf-8\")\n",
    "        except urllib.error.URLError:\n",
    "            print(\"error\")\n",
    "        return html\n",
    "\n",
    "    def get_content(self, url:str= 'http://guba.eastmoney.com/list', num_pages: int= 1, stock_num: str= None,\n",
    "                    title = re.compile(r'<div class=\"title\"><a .*?>(.*?)</a></div>'),\n",
    "                    author = re.compile(r'<div class=\"author\"><a href=\"(.*?)\">'),\n",
    "                    times = re.compile(r'<div class=\"update\">(\\d{2}-\\d{2} \\d{2}:\\d{2})</div>'),\n",
    "                    href = re.compile(r'href=\"([^\"]+)\"'),\n",
    "                    read_num = re.compile(r'<div class=\"read\">(\\d+)</div>'),\n",
    "                    reply_num = re.compile(r'<div class=\"reply\">(\\d+)</div>'),\n",
    "                    ):\n",
    "        \"\"\"\n",
    "        打开网页，获取具体的内容\n",
    "\n",
    "        获取内容：\n",
    "            1、title：标题；2、times：发表时间（年-月-日-xx-xx）3、url：文本内容链接；\n",
    "            4、content：文章具体内容\n",
    "            5、read：阅读数量；6、reply：评论数量；7、reply_content：具体评论\n",
    "        \"\"\"\n",
    "        data_list = {'title': [], 'author': [],'times': [], 'url': [], 'content': [], 'read_num': [], 'reply_num': [], 'reply_content': []}\n",
    "\n",
    "        # url = f'http://guba.eastmoney.com/list,{stock_num},f_{page}.html'\n",
    "        for page in range(1, num_pages+ 1):\n",
    "            if stock_num is None:\n",
    "                print('指定url链接')\n",
    "                return\n",
    "            base_url = f'http://guba.eastmoney.com/list,{stock_num},f_{page}.html'\n",
    "            html = self.open_url(url= base_url)\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            print(soup)\n",
    "            for item in soup.find_all('div', class_ = 'title'):\n",
    "                title_text = item.text.strip()\n",
    "                print(title_text)\n",
    "                # print(item)\n",
    "        #         print(item)\n",
    "        #         data_list['title'] += re.findall(title, item)\n",
    "        #         data_list['author'] += re.findall(author, item)\n",
    "        #         data_list['times'] += re.findall(times, item)\n",
    "        #         data_list['url'] += re.findall(href, item)\n",
    "        #         data_list['read_num'] += re.findall(read_num, item)\n",
    "        #         data_list['reply_num'] += re.findall(reply_num, item)\n",
    "        # return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url0 = 'https://guba.eastmoney.com/news,600916,1439573793.html'\n",
    "url1 = 'https://caifuhao.eastmoney.com/news/20240622174403440627910?from=guba&name=5Lit5Zu96buE6YeR5ZCn&gubaurl=aHR0cHM6Ly9ndWJhLmVhc3Rtb25leS5jb20vbGlzdCw2MDA5MTYuaHRtbA%3D%3D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url0, headers= {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'})\n",
    "root = etree.HTML(response.text)\n",
    "root.xpath('//*[@id=\"main\"]/div[2]/div[1]/div[1]/div[1]/div[1]/div[1]/span[2]//text()')\n",
    "root.xpath('//*[@id=\"main\"]/div[2]/div[1]/div[1]/div[1]/div[3]/div[1]//text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024年06月22日 17:44\n",
      "\n",
      "\n",
      "浙江\n",
      "\n",
      "\n",
      "$菜百股份(SH605599)$  昨天量能不到6200亿，又是年内新低。虽然下午某队突击出动护盘，但人心散了，根本带不动！现在的市场是场内资金认为3000点以下不需要割肉，场外资金却认为经济下行期3000点不能抄底，因为根本没有赚钱效应。没有了买卖，自然放不出量来，没有量只能继续阴跌。以茅指数为代表的大白马消费股都在加速下跌，而且没有一点止跌迹象。消费疲软已经是板上钉钉的事，前期靠开店盲目扩张的公司其隐患将会逐渐浮出水面。目前各公司都在忙于收缩规模，把经济下行带来的影响降到最低。回到黄金首饰板块具体个股，本人目前仍持有菜百和中国黄金，菜百成本为5元且股数不少，前面一直都有实盘帖子，目前价位请各位菜友放心持有。虽然菜百现在的股价也不尽人意，但和同板块的老凤祥周大生比已经是非常坚挺了，他现在的实际价格和周大生只差了3毛钱，满足吧。中国黄金还是2月初的建仓数量成本为9.20元，买中国黄金时我是经过仔细分析研究过的，这家公司只要进入价格足够便宜，买了放他个3年不翻倍也难，我的自信来源于我几十年的炒股经验和所学专业知识。近期看，如果黄金首饰板块有反弹，带头的一定是老凤祥！这家公司只有业绩下滑的预期，没有其他的雷，所以可以放心适量抄底，当然只是搏个反弹，我不会长拿。跌下来有量才会有反弹，这是基本常识，个股和大盘都一样，所以大盘这连续的无量下跌，没有外部力量的干预绝对是还要阴跌不止。\n"
     ]
    }
   ],
   "source": [
    "request = urllib.request.Request(url1, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'})\n",
    "response = urllib.request.urlopen(request)\n",
    "html = response.read().decode(\"utf-8\")\n",
    "\n",
    "time_pattern = r'<div class=\"time\">(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})</div>|<span class=\"txt\">(\\d{4}年\\d{2}月\\d{2}日 \\d{2}:\\d{2})</span>'\\\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "for item in soup.find_all('div', class_ = ['time', 'txt']):\n",
    "    print(item.text.strip())\n",
    "\n",
    "for item in soup.find_all('span', class_ = ['time', 'txt']):\n",
    "    print(item.text.strip())\n",
    "    print('\\n')\n",
    "\n",
    "for item in soup.find_all('div', class_ = [\"xeditor_content app_h5_article\", \"xeditor_content editorlungo_content\", \"question\"]):\n",
    "    print(item.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "文本内容:\n",
    "<div class=\"xeditor_content app_h5_article\">\n",
    "<div class=\"xeditor_content editorlungo_content\">\n",
    "<div class=\"xeditor_content editorlungo_content\"><p></div>\n",
    "<div class=\"xeditor_content app_h5_article\">\n",
    "<div class=\"xeditor_content editorlungo_content\"></div>\n",
    "<div class=\"question\"></div>\n",
    "<div class=\"xeditor_content app_h5_article\"></div>\n",
    "\n",
    "\n",
    "时间：\n",
    "<div class=\"time\">2024-06-27 13:25:50</div>\n",
    "<span class=\"txt\">2024年06月22日 17:44</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "conf = {'max_pages': 1,\n",
    "        'time_sleep': 2,\n",
    "        'headers': [{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'},\n",
    "                    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'},\n",
    "                    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}]\n",
    "        }\n",
    "random_num = np.random.randint(0, len(conf['headers']))\n",
    "print(random_num)\n",
    "headers = conf['headers'][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
