{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "c = torch.eye(9, 9)\n",
    "c[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data written to ../data/0308/0308-data/300263_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/000753_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/000951_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/002282_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/000046_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/603095_XSHG_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/300540_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/603359_XSHG_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/300343_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/300433_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/600622_XSHG_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/002882_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/300174_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/603053_XSHG_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/300133_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/002841_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/002679_XSHE_25_daily_normalized.csv\n",
      "Normalized data written to ../data/0308/0308-data/000998_XSHE_25_daily_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 设置数据目录路径\n",
    "path_dir = '../data/0308/0308-data/'\n",
    "# 初始化MinMaxScaler对象\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 遍历目录中的所有文件\n",
    "for filename in os.listdir(path_dir):\n",
    "    # 拼接完整的文件路径\n",
    "    file_path = os.path.join(path_dir, filename)\n",
    "    # 确保是文件而不是文件夹\n",
    "    if os.path.isfile(file_path):\n",
    "        # 读取CSV文件\n",
    "        data = pd.read_csv(file_path)\n",
    "        # 选取除了第一列的所有数据\n",
    "        data_use = data.iloc[:, 1:]\n",
    "        # 正则化数据\n",
    "        data_use_normalized = scaler.fit_transform(data_use)\n",
    "        \n",
    "        # 将正则化后的数据转换为DataFrame\n",
    "        data_normalized_df = pd.DataFrame(data_use_normalized, columns=data_use.columns)\n",
    "        \n",
    "        # 将第一列（假设是索引或ID）添加回去\n",
    "        data_normalized_df.insert(0, data.columns[0], data.iloc[:, 0])\n",
    "        \n",
    "        # 构建新的文件名，例如在原文件名后添加\"_normalized\"\n",
    "        new_file_name = os.path.splitext(filename)[0] + \"_normalized.csv\"\n",
    "        new_file_path = os.path.join(path_dir, new_file_name)\n",
    "        \n",
    "        # 将正则化后的数据写入新的CSV文件\n",
    "        data_normalized_df.to_csv(new_file_path, index=False)\n",
    "        print(f'Normalized data written to {new_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bin0': ['2021-03-07 15:00', '2021-03-08 09:30'],\n",
       " 'bin1': ['2021-03-08 09:30', '2021-03-08 09:39'],\n",
       " 'bin2': ['2021-03-08 09:39', '2021-03-08 09:48'],\n",
       " 'bin3': ['2021-03-08 09:48', '2021-03-08 09:57'],\n",
       " 'bin4': ['2021-03-08 09:57', '2021-03-08 10:06'],\n",
       " 'bin5': ['2021-03-08 10:06', '2021-03-08 10:15'],\n",
       " 'bin6': ['2021-03-08 10:15', '2021-03-08 10:24'],\n",
       " 'bin7': ['2021-03-08 10:24', '2021-03-08 10:33'],\n",
       " 'bin8': ['2021-03-08 10:33', '2021-03-08 10:42'],\n",
       " 'bin9': ['2021-03-08 10:42', '2021-03-08 10:51'],\n",
       " 'bin10': ['2021-03-08 10:51', '2021-03-08 11:30'],\n",
       " 'bin11': ['2021-03-08 11:30', '2021-03-08 13:00'],\n",
       " 'bin12': ['2021-03-08 13:00', '2021-03-08 13:09'],\n",
       " 'bin13': ['2021-03-08 13:09', '2021-03-08 13:18'],\n",
       " 'bin14': ['2021-03-08 13:18', '2021-03-08 13:27'],\n",
       " 'bin15': ['2021-03-08 13:27', '2021-03-08 13:36'],\n",
       " 'bin16': ['2021-03-08 13:36', '2021-03-08 13:45'],\n",
       " 'bin17': ['2021-03-08 13:45', '2021-03-08 13:54'],\n",
       " 'bin18': ['2021-03-08 13:54', '2021-03-08 14:03'],\n",
       " 'bin19': ['2021-03-08 14:03', '2021-03-08 14:12'],\n",
       " 'bin20': ['2021-03-08 14:12', '2021-03-08 14:21'],\n",
       " 'bin21': ['2021-03-08 14:21', '2021-03-08 14:30'],\n",
       " 'bin22': ['2021-03-08 14:30', '2021-03-08 14:39'],\n",
       " 'bin23': ['2021-03-08 14:39', '2021-03-08 14:48'],\n",
       " 'bin24': ['2021-03-08 14:48', '2021-03-08 15:00']}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BuildData import BuildData\n",
    "\n",
    "conf = {'lag_day': 3,\n",
    "        'lag_bin': 3,\n",
    "        'lag_week': 1,\n",
    "        'bin_num': 24,\n",
    "        'file_dir': '../data/0308/0308-data/',\n",
    "        'comment_dir': '../data/0308/0308-number/'}\n",
    "\n",
    "# inputs_df, output_list = BuildData(conf= conf).gen_input_output_data(file_path= '../data/0308/0308-data/000046_XSHE_25_daily.csv',\n",
    "#                                                                      comment_path= '../data/0308/0308-number/000046_comment_sentiment.csv',\n",
    "#                                                                      stock_info= None)\n",
    "\n",
    "path = '../data/volume/0308/Input/000046_3_3_inputs.npy'\n",
    "input_ = np.load('../data/volume/0308/Input/000046_3_3_inputs.npy', allow_pickle= True)\n",
    "output_matrix = np.load(path.replace('inputs', 'output').replace('Input', 'Output'), allow_pickle= True)\n",
    "input_matrix = np.array([value for item in input_ for value in item], dtype= np.float32).reshape(input_.shape[0], 13, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 13, 9)\n",
    "w = torch.randn(9, 12)\n",
    "a = torch.randn(24, 13)\n",
    "c = torch.eye(13, 13)\n",
    "\n",
    "connection = [\n",
    "    (1, 0),\n",
    "    (9, 0), (12, 0), \n",
    "    (8, 9), (8, 12), (5, 9), (11, 12), \n",
    "    (4, 5), (4, 8), (7, 8), (7, 11), (10, 11),\n",
    "    (3, 4), (3, 7), (6, 7), (6, 10), (2, 3), (2, 6)]\n",
    "adj_matrix = torch.zeros(13, 13)\n",
    "for source, target in connection:\n",
    "    adj_matrix[source][target] = 1\n",
    "h = torch.matmul(x, w)\n",
    "\n",
    "connection_coord = {}\n",
    "from_list, to_list = [t.item() for t in torch.where(adj_matrix>0)[0]],  [t.item() for t in torch.where(adj_matrix>0)[1]]\n",
    "for pointA, pointB in zip(to_list, from_list):\n",
    "    if pointA not in connection_coord:\n",
    "        connection_coord[pointA] = [pointB]\n",
    "    elif pointA in connection_coord:\n",
    "        connection_coord[pointA].append(pointB)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 9, 12],\n",
       " 3: [2],\n",
       " 6: [2],\n",
       " 4: [3],\n",
       " 7: [3, 6],\n",
       " 5: [4],\n",
       " 8: [4, 7],\n",
       " 9: [5, 8],\n",
       " 10: [6],\n",
       " 11: [7, 10],\n",
       " 12: [8, 11]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 13, 12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node0: tensor([ 1,  9, 12])\n",
      "node1: tensor([], dtype=torch.int64)\n",
      "node2: tensor([], dtype=torch.int64)\n",
      "node3: tensor([2])\n",
      "node4: tensor([3])\n",
      "node5: tensor([4])\n",
      "node6: tensor([2])\n",
      "node7: tensor([3, 6])\n",
      "node8: tensor([4, 7])\n",
      "node9: tensor([5, 8])\n",
      "node10: tensor([6])\n",
      "node11: tensor([ 7, 10])\n",
      "node12: tensor([ 8, 11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def _gen_adj_matrix():\n",
    "    connection = [\n",
    "        (1, 0),\n",
    "        (9, 0), (12, 0), \n",
    "        (8, 9), (8, 12), (5, 9), (11, 12), \n",
    "        (4, 5), (4, 8), (7, 8), (7, 11), (10, 11),\n",
    "        (3, 4), (3, 7), (6, 7), (6, 10), (2, 3), (2, 6)]\n",
    "    adj_matrix = np.zeros((13, 13))\n",
    "    for source, target in connection:\n",
    "        adj_matrix[source][target] = 1\n",
    "    return adj_matrix\n",
    "\n",
    "adj = _gen_adj_matrix()\n",
    "x = torch.randn(32, 32, 13, 9)\n",
    "for i in range(0, 13):\n",
    "    index = torch.where(torch.Tensor(adj[:, i] > 0))[0]\n",
    "    print(f\"node{i}: {index}\")\n",
    "\n",
    "# # 选择切片\n",
    "# x_row = x[:, :, index, :]\n",
    "\n",
    "# # 对切片进行加和，并保持最后一个维度的形状\n",
    "# x_sum = x_row.sum(dim=2, keepdim=True)\n",
    "\n",
    "# print(x_sum.shape)  # 输出应该是 torch.Size([32, 32, 1, 9])\n",
    "# print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before x torch.Size([4, 3])\n",
      "com1 x torch.Size([4, 8])\n",
      "com2 x torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.6, alpha=0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # 权重矩阵\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        \n",
    "        # 初始化参数\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # h: [N, in_features], adj: [N, N]\n",
    "        Wh = torch.mm(h, self.W)  # [N, out_features]\n",
    "        # 计算注意力机制的分数\n",
    "        N = Wh.size()[0]\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # 将注意力分数应用到邻接矩阵上\n",
    "        attention = F.softmax(e.view(N, N), dim=1)  # [N, N]\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # 应用注意力权重\n",
    "        h_prime = torch.matmul(attention, Wh)  # [N, out_features]\n",
    "        return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # 将Wh复制一份并转置，以便进行自注意力计算\n",
    "        N = Wh.size()[0]\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        \n",
    "        # 拼接Wh_repeated_in_chunks和Wh_repeated_alternating\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout_rate=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.attention1 = GraphAttentionLayer(nfeat, nhid, dropout_rate=dropout_rate)\n",
    "        self.attention2 = GraphAttentionLayer(nhid, nclass, dropout_rate=dropout_rate)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, adj = data['x'], data['adj']\n",
    "        print(f\"before x {x.shape}\")\n",
    "        x = F.relu(self.attention1(x, adj))\n",
    "        print(f\"com1 x {x.shape}\")\n",
    "        x = self.dropout(x)\n",
    "        x = self.attention2(x, adj)\n",
    "        print(f\"com2 x {x.shape}\")\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 假设我们有一个简单的图数据\n",
    "num_nodes = 4\n",
    "nfeat = 3\n",
    "nhid = 8\n",
    "nclass = 2\n",
    "\n",
    "# 随机生成一些节点特征\n",
    "node_features = torch.randn(num_nodes, nfeat)\n",
    "\n",
    "# 定义邻接矩阵\n",
    "adj = torch.tensor([[0, 1, 0, 1],\n",
    "                     [1, 0, 1, 0],\n",
    "                     [0, 1, 0, 1],\n",
    "                     [1, 0, 1, 0]], dtype=torch.float)\n",
    "\n",
    "# 创建图数据对象\n",
    "data = {'x': node_features, 'adj': adj}\n",
    "\n",
    "# 初始化模型\n",
    "model = GAT(nfeat, nhid, nclass)\n",
    "out = model(data)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
