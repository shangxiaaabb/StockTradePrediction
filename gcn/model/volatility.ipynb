{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfcf87a3-3356-4231-b4d6-5e9aa31eb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def calculate_volatility(df, x_ti, j):\n",
    "    df['volatility'] = np.nan  # Initialize the 'volatility' column with NaN\n",
    "    for i in range(j,len(x_ti)):\n",
    "        windows = x_ti[i - j :i ]   # Form a sliding window\n",
    "        volatility = np.std(windows)\n",
    "        df.at[i, 'volatility'] = volatility\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a1349c2-969f-48f6-a1e3-a1517e288c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK','INDEX']\n",
    "venues = ['XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE','XSHG']\n",
    "tickers = ['000725', '300015', '300185', '000002', '000807', '002340', '000001','300750','300059','000166','000009','000001']\n",
    "bin_num = 25\n",
    "def data_generating_all(data_types, venues, tickers,bin_num):\n",
    "    for i in range(len(data_types)):\n",
    "        data_type = data_types[i]\n",
    "        venue = venues[i]\n",
    "        ticker = tickers[i]\n",
    "        file = '/volume1/home/rzhu/LHH/result/' + str(ticker) + '_' + str(venue) + '_' + str(bin_num) +'_'+'daily.csv'\n",
    "        data = pd.read_csv(file)\n",
    "        result_df = calculate_volatility(data,data['bin_volume'],j=4)\n",
    "        filename_basic = '/volume1/home/rzhu/volatility/' + str(ticker) + '_' + str(venue) + '_' + str(\n",
    "            bin_num) +'_'+'daily.csv'\n",
    "        result_df.to_csv(filename_basic, index=False)\n",
    "\n",
    "data_generating_all(data_types, venues, tickers,bin_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73ccd17-3099-46ad-9773-49f2b07968e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/volume1/home/rzhu/LHH/result/000807_XSHE_25_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5937fe9-7af6-497e-b0c4-a73604039c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['volatility']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb348fc-4d4d-4aac-8b0a-2d7cc7e7b5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_volume</th>\n",
       "      <th>bin_num</th>\n",
       "      <th>bin_volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>quote_imbalance</th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>36083097.0</td>\n",
       "      <td>0</td>\n",
       "      <td>269600.0</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.102093</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>36083097.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4074940.0</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>-0.060793</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>36083097.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1383700.0</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.068181</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>36083097.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2900043.0</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.272326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>36083097.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1601400.0</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.025657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>40961006.2</td>\n",
       "      <td>20</td>\n",
       "      <td>775100.0</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.042039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>40961006.2</td>\n",
       "      <td>21</td>\n",
       "      <td>511000.0</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.175842</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>40961006.2</td>\n",
       "      <td>22</td>\n",
       "      <td>802200.0</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.364790</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>40961006.2</td>\n",
       "      <td>23</td>\n",
       "      <td>1307439.0</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>-0.024152</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>40961006.2</td>\n",
       "      <td>24</td>\n",
       "      <td>3892197.0</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>-0.263882</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  daily_volume  bin_num  bin_volume    spread  \\\n",
       "0     2020-09-01    36083097.0        0    269600.0  0.000116   \n",
       "1     2020-09-01    36083097.0        1   4074940.0  0.000917   \n",
       "2     2020-09-01    36083097.0        2   1383700.0  0.000820   \n",
       "3     2020-09-01    36083097.0        3   2900043.0  0.000849   \n",
       "4     2020-09-01    36083097.0        4   1601400.0  0.000859   \n",
       "...          ...           ...      ...         ...       ...   \n",
       "4995  2021-06-30    40961006.2       20    775100.0  0.000456   \n",
       "4996  2021-06-30    40961006.2       21    511000.0  0.000435   \n",
       "4997  2021-06-30    40961006.2       22    802200.0  0.000428   \n",
       "4998  2021-06-30    40961006.2       23   1307439.0  0.000431   \n",
       "4999  2021-06-30    40961006.2       24   3892197.0  0.000426   \n",
       "\n",
       "      quote_imbalance  volatility  \n",
       "0           -0.102093         NaN  \n",
       "1           -0.060793         NaN  \n",
       "2            0.068181         NaN  \n",
       "3            0.272326         NaN  \n",
       "4            0.025657         NaN  \n",
       "...               ...         ...  \n",
       "4995         0.042039         NaN  \n",
       "4996         0.175842         NaN  \n",
       "4997         0.364790         NaN  \n",
       "4998        -0.024152         NaN  \n",
       "4999        -0.263882         NaN  \n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b1f7e0c-8e6e-4f29-ab9b-e9ea434d1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows=df['bin_volume'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f23f8f3-2107-4dca-ba1e-ee086d5d26f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     269600.0\n",
       "1    4074940.0\n",
       "2    1383700.0\n",
       "3    2900043.0\n",
       "Name: bin_volume, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4cd6690-8fe0-4275-8e22-d9b54a91dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ti=df['bin_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f668fa3-1aa9-4684-8aa2-026fd76d1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows=x_ti[11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a1d02e-485f-41f8-8b7e-87c28050df5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210846.75722251457"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d5f76879-21c8-4cd3-b526-ac1d69acc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = (windows - np.mean(windows)) / np.std(windows, ddof=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b259b1ef-fef7-4aaa-aa40-9d525fc85105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11   -1.163410\n",
       "12   -0.118668\n",
       "13    1.277644\n",
       "14    0.004434\n",
       "Name: bin_volume, dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a9855d43-cb98-4233-9837-5569f8f04c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility = np.std(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9273d93f-37c6-4d97-a147-89f0ea97c6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8660254037844386"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80f79f-0b33-4526-a5a4-042223bc08cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae8a13-397d-4339-bba0-c0a112870267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ad02e-ff59-4282-ad07-470d3ef381ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f11b76-5203-44e7-8462-d8b8c4a32e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[5,7,10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ecf61d-beca-4219-ab94-0986e241830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dateutil.parser as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b999d82-cefb-4524-8448-283aeee0cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter):\n",
    "    '''\n",
    "    从sever中读取一只股票一天的数据\n",
    "    data_home:数据所在folder\n",
    "    data_type:类型\n",
    "    venue:交易所\n",
    "    is_filter:是否进行filter操作\n",
    "    '''\n",
    "\n",
    "    path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "        month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue)  # 读数据的路径\n",
    "\n",
    "    if (os.path.exists(path)):\n",
    "        data0 = open(path, 'r')\n",
    "        data1 = pd.read_csv(StringIO(data0.read()))\n",
    "        data = data1.loc[:, ['time', 'volume', 'current','a1_v','a1_p','b1_v','b1_p',]]\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['current'] = data['current']/10000\n",
    "        data['spread'] = (data['a1_p'] - data['b1_p'])/(data['a1_p'] + data['b1_p'])  # 计算 spread\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "    else:\n",
    "        path = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "            month) + '/' + str(day) + '/' + str(ticker) + '.' + str(venue) + '.gz'\n",
    "        with gzip.open(path, 'rb') as gf:\n",
    "            data1 = pd.read_csv(gf)\n",
    "        data = data1.loc[:, ['time', 'volume','current', 'a1_v','a1_p','b1_v','b1_p',]]\n",
    "        diff_df = data.loc[:, ['time', 'volume']].diff()  # 差分，求出每次交易的交易量、交易额\n",
    "        data.iloc[1:len(data['volume']), 1] = diff_df.iloc[1:len(data['volume']), 1] # 第0个为NaN,从第一个代替原数据的volume\n",
    "        data['current'] = data['current']/10000\n",
    "        data['spread'] = (data['a1_p'] - data['b1_p'])/(data['a1_p'] + data['b1_p'])  # 计算 spread\n",
    "        if is_filter == 0:  # 不filter数据的时候\n",
    "            return data\n",
    "        else:\n",
    "            quantile = np.percentile(data['volume'], 99.5)\n",
    "            quantile2 = np.percentile(data['volume'], 0.5) \n",
    "            data = data[(quantile2 <= data['volume']) & (data['volume'] <= quantile)]  \n",
    "            return data\n",
    "def trans_date(date):  # transform date to year-month-day\n",
    "    dates = []\n",
    "    for i in range(len(date)):\n",
    "        year = str(date[i])[0:4]\n",
    "        month = str(date[i])[4:6]\n",
    "        day = str(date[i])[6:8]\n",
    "        date_std = datetime.date(int(year), int(month), int(day)).isoformat()\n",
    "        dates.append(date_std)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def trans_time(time):  # transform time to hour:minute:second\n",
    "    times = []\n",
    "    for i in range(len(time)):\n",
    "        hour = str(time[i])[8:10]\n",
    "        minute = str(time[i])[10:12]\n",
    "        second = str(time[i])[12:14]\n",
    "        time_std = datetime.time(int(hour), int(minute), int(second)).isoformat()\n",
    "        times.append(time_std)\n",
    "    return times\n",
    "\n",
    "def divide_bin(time, binnumber):  # 计算每条交易所属的bin number\n",
    "    '''\n",
    "    time:columns of time\n",
    "    '''\n",
    "    n = 237 / (binnumber - 1) * 60\n",
    "    bin_nums = []\n",
    "    for i in range(len(time)):\n",
    "        if datetime.datetime.strptime(time[i],\"%H:%M:%S\") < datetime.datetime.strptime(\"09:30:00\",\"%H:%M:%S\"):\n",
    "            bin_num = 0  # 交易发生在9：30之前，bin number为0\n",
    "        elif datetime.datetime.strptime(time[i],\"%H:%M:%S\")>datetime.datetime.strptime(\"15:00:00\",\"%H:%M:%S\"):\n",
    "            bin_num = binnumber+1\n",
    "        else:\n",
    "            starttime = parser.parse(datetime.time(9, 30, 0).isoformat())  # 开始时间设为9：30\n",
    "            endtime = parser.parse(time[i])  # 结束时间是该条数据的交易时间\n",
    "            s = (endtime - starttime).seconds  # 从开盘到现在的秒数\n",
    "            if s > -1 and s < 7201:  # 交易发生在9：30-11：30之前\n",
    "                bin_num = int((s - 0.5) // n) + 1  # 9：30之后的bin number从1开始\n",
    "            elif s > 12599 and s < 19801:  # 13:00-15:00\n",
    "                bin_num = int((s - 0.5 - 5400) // n)+1  # 去掉中间的90分钟\n",
    "            else:\n",
    "                bin_num = binnumber\n",
    "        bin_nums.append(bin_num)\n",
    "    return bin_nums\n",
    "\n",
    "####### volatility\n",
    "def cal_bin_volume(subdf, binnumber):\n",
    "    '''\n",
    "    subdf: data to be processed, DataFrame\n",
    "    return: DataFrame including one stock, ranked by bin number\n",
    "    '''\n",
    "\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber + 1])]  # Exclude rows with bin number binnumber+1\n",
    "    subdf = subdf[~subdf['bin_num'].isin([binnumber])]  # Exclude rows with bin number binnumber\n",
    "\n",
    "    daily_volume = subdf['volume'].groupby(subdf['date']).sum().reset_index()  # Calculate total volume for each day\n",
    "    bin_volume = subdf['volume'].groupby([subdf['date'], subdf['bin_num']]).sum().reset_index()  # Calculate volume for each bin\n",
    "    \n",
    "    # 计算标准差\n",
    "    \n",
    "    volatility = subdf['current'].groupby([subdf['date'], subdf['bin_num']]).std().reset_index().rename(columns={'current': 'volatility'})\n",
    "\n",
    "\n",
    "    # 一档买卖价的数量\n",
    "    bin_ask = subdf['a1_v'].groupby([subdf['date'], subdf['bin_num']]).sum().reset_index()\n",
    "    bin_bid = subdf['b1_v'].groupby([subdf['date'], subdf['bin_num']]).sum().reset_index()\n",
    "    \n",
    "    df = pd.merge(daily_volume, bin_volume, how='outer', on='date')  # Merge daily_volume and bin_volume\n",
    "    subdf1 = pd.merge(df, spread,how='outer',on=['date','bin_num'])\n",
    "    # 将 volatility 数据框与 subdf1 数据框合并\n",
    "    subdf1 = pd.merge(subdf1, volatility, how='outer', on=['date', 'bin_num'])\n",
    "    \n",
    "    # imbalance\n",
    "    \n",
    "    subdf1['quote_imbalance'] = (bin_bid['b1_v'] - bin_ask['a1_v']) / (bin_bid['b1_v'] + bin_ask['a1_v'])\n",
    "    return subdf1\n",
    "\n",
    "\n",
    "def get_df(data_home, data_type, venue, year, month, day,ticker, bin_num, is_filter=1):\n",
    "    stock_data = read_stock_data(data_home, data_type, venue, year, month, day, ticker, is_filter=1)\n",
    "    stock_data = stock_data.reset_index(drop=True)\n",
    "    transdate = trans_date(stock_data['time'])\n",
    "    transtime = trans_time(stock_data['time'])\n",
    "    stock_data.loc[:, 'date'] = transdate  # replace the original data by transformed data\n",
    "    stock_data.loc[:, 'timet'] = transtime\n",
    "    bin_nums = divide_bin(time=stock_data['timet'], binnumber=bin_num)\n",
    "    stock_data.loc[:, 'bin_num'] = bin_nums\n",
    "    vol_df = cal_bin_volume(subdf=stock_data, binnumber=bin_num)\n",
    "    vol_df = vol_df.rename(columns={'volume_x': 'daily_volume'})\n",
    "    vol_df = vol_df.rename(columns={'volume_y': 'bin_volume'})\n",
    "    vol_df['bin_volume'] = vol_df['bin_volume'].fillna(1)  # 空值用1填充\n",
    "    vol_df['daily_volume'] = vol_df['daily_volume'].fillna(method = 'bfill')  # 空值用向上填充\n",
    "    return vol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42ffa33f-e84f-46f0-b2c6-19ad2a989db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_number, is_filter):\n",
    "    '''\n",
    "    读取一只股票所有日期的数据，from start_date to end_date\n",
    "    '''\n",
    "\n",
    "    data_concat = pd.DataFrame(columns=['date', 'daily_volume', 'bin_num', 'bin_volume'])\n",
    "\n",
    "    start_date1 = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date1 = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    interval_day = (end_date1-start_date1).days\n",
    "\n",
    "    ##遍历日期\n",
    "\n",
    "    for i in range(interval_day+1):\n",
    "        date = datetime.datetime.strptime(start_date,'%Y-%m-%d') + datetime.timedelta(days=i)\n",
    "\n",
    "        date2 = datetime.datetime.strftime(date, '%Y-%m-%d')\n",
    "        year = date2[0:4]\n",
    "        month = date2[5:7]\n",
    "        day = date2[8:10]\n",
    "\n",
    "        if len(str(month)) < 2:\n",
    "            month = str(0) + str(month)\n",
    "        else:\n",
    "            month = str(month)\n",
    "        if len(str(day)) < 2:\n",
    "            day = str(0) + str(day)\n",
    "        else:\n",
    "            day = str(day)\n",
    "\n",
    "        dirs = str(data_home) + '/' + str(data_type) + '/' + str(venue) + '/' + str(year) + '/' + str(\n",
    "                    month) + '/' + str(day) + '/'\n",
    "\n",
    "\n",
    "        if not (os.path.exists(dirs)):\n",
    "            continue\n",
    "        else:\n",
    "            print(year, month, day)\n",
    "            data = get_df(data_home, data_type, venue, year, month, day, ticker, bin_number, is_filter)\n",
    "            frames = [data_concat, data]\n",
    "            data_concat = pd.concat(frames)  # 将一只股票多天的数据合并到一个数据框里\n",
    "\n",
    "    return data_concat.reset_index(drop=True)  # 返回合并后的数据框并重新设置下标\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2418c640-f14f-4e2e-86ba-a4c1a132c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 09 01\n",
      "2020 09 02\n",
      "2020 09 03\n",
      "2020 09 04\n",
      "2020 09 07\n",
      "2020 09 08\n",
      "2020 09 09\n",
      "2020 09 10\n",
      "2020 09 11\n",
      "2020 09 14\n",
      "2020 09 15\n",
      "2020 09 16\n",
      "2020 09 17\n",
      "2020 09 18\n",
      "2020 09 21\n",
      "2020 09 22\n",
      "2020 09 23\n",
      "2020 09 24\n",
      "2020 09 25\n",
      "2020 09 28\n",
      "2020 09 29\n",
      "2020 09 30\n",
      "2020 10 09\n",
      "2020 10 12\n",
      "2020 10 13\n",
      "2020 10 14\n",
      "2020 10 15\n",
      "2020 10 16\n",
      "2020 10 19\n",
      "2020 10 20\n",
      "2020 10 21\n",
      "2020 10 22\n",
      "2020 10 23\n",
      "2020 10 26\n",
      "2020 10 27\n",
      "2020 10 28\n",
      "2020 10 29\n",
      "2020 10 30\n",
      "2020 11 02\n",
      "2020 11 03\n",
      "2020 11 04\n",
      "2020 11 05\n",
      "2020 11 06\n",
      "2020 11 09\n",
      "2020 11 10\n",
      "2020 11 11\n",
      "2020 11 12\n",
      "2020 11 13\n",
      "2020 11 16\n",
      "2020 11 17\n",
      "2020 11 18\n",
      "2020 11 19\n",
      "2020 11 20\n",
      "2020 11 23\n",
      "2020 11 24\n",
      "2020 11 25\n",
      "2020 11 26\n",
      "2020 11 27\n",
      "2020 11 30\n",
      "2020 12 01\n",
      "2020 12 02\n",
      "2020 12 03\n",
      "2020 12 04\n",
      "2020 12 07\n",
      "2020 12 08\n",
      "2020 12 09\n",
      "2020 12 10\n",
      "2020 12 11\n",
      "2020 12 14\n",
      "2020 12 15\n",
      "2020 12 16\n",
      "2020 12 17\n",
      "2020 12 18\n",
      "2020 12 21\n",
      "2020 12 22\n",
      "2020 12 23\n",
      "2020 12 24\n",
      "2020 12 25\n",
      "2020 12 28\n",
      "2020 12 29\n",
      "2020 12 30\n",
      "2020 12 31\n",
      "2021 01 04\n",
      "2021 01 05\n",
      "2021 01 06\n",
      "2021 01 07\n",
      "2021 01 08\n",
      "2021 01 11\n",
      "2021 01 12\n",
      "2021 01 13\n",
      "2021 01 14\n",
      "2021 01 15\n",
      "2021 01 18\n",
      "2021 01 19\n",
      "2021 01 20\n",
      "2021 01 21\n",
      "2021 01 22\n",
      "2021 01 25\n",
      "2021 01 26\n",
      "2021 01 27\n",
      "2021 01 28\n",
      "2021 01 29\n",
      "2021 02 01\n",
      "2021 02 02\n",
      "2021 02 03\n",
      "2021 02 04\n",
      "2021 02 05\n",
      "2021 02 08\n",
      "2021 02 09\n",
      "2021 02 10\n",
      "2021 02 18\n",
      "2021 02 19\n",
      "2021 02 22\n",
      "2021 02 23\n",
      "2021 02 24\n",
      "2021 02 25\n",
      "2021 02 26\n",
      "2021 03 01\n",
      "2021 03 02\n",
      "2021 03 03\n",
      "2021 03 04\n",
      "2021 03 05\n",
      "2021 03 08\n",
      "2021 03 09\n",
      "2021 03 10\n",
      "2021 03 11\n",
      "2021 03 12\n",
      "2021 03 15\n",
      "2021 03 16\n",
      "2021 03 17\n",
      "2021 03 18\n",
      "2021 03 19\n",
      "2021 03 22\n",
      "2021 03 23\n",
      "2021 03 24\n",
      "2021 03 25\n",
      "2021 03 26\n",
      "2021 03 29\n",
      "2021 03 30\n",
      "2021 03 31\n",
      "2021 04 01\n",
      "2021 04 02\n",
      "2021 04 06\n",
      "2021 04 07\n",
      "2021 04 08\n",
      "2021 04 09\n",
      "2021 04 12\n",
      "2021 04 13\n",
      "2021 04 14\n",
      "2021 04 15\n",
      "2021 04 16\n",
      "2021 04 19\n",
      "2021 04 20\n",
      "2021 04 21\n",
      "2021 04 22\n",
      "2021 04 23\n",
      "2021 04 26\n",
      "2021 04 27\n",
      "2021 04 28\n",
      "2021 04 29\n",
      "2021 04 30\n",
      "2021 05 06\n",
      "2021 05 07\n",
      "2021 05 10\n",
      "2021 05 11\n",
      "2021 05 12\n",
      "2021 05 13\n",
      "2021 05 14\n",
      "2021 05 17\n",
      "2021 05 18\n",
      "2021 05 19\n",
      "2021 05 20\n",
      "2021 05 21\n",
      "2021 05 24\n",
      "2021 05 25\n",
      "2021 05 26\n",
      "2021 05 27\n",
      "2021 05 28\n",
      "2021 05 31\n",
      "2021 06 01\n",
      "2021 06 02\n",
      "2021 06 03\n",
      "2021 06 04\n",
      "2021 06 07\n",
      "2021 06 08\n",
      "2021 06 09\n",
      "2021 06 10\n",
      "2021 06 11\n",
      "2021 06 15\n",
      "2021 06 16\n",
      "2021 06 17\n",
      "2021 06 18\n",
      "2021 06 21\n",
      "2021 06 22\n",
      "2021 06 23\n",
      "2021 06 24\n",
      "2021 06 25\n",
      "2021 06 28\n",
      "2021 06 29\n",
      "2021 06 30\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/volume1/home/rzhu/ste'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_974824/3629660354.py\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdata_generating_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvenues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbin_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_974824/3629660354.py\u001b[0m in \u001b[0;36mdata_generating_all\u001b[0;34m(data_home, data_types, venues, tickers, start_date, end_date, bin_num, is_filter)\u001b[0m\n\u001b[1;32m     15\u001b[0m         filename_basic = '/volume1/home/rzhu/ste/' + str(ticker) + '_' + str(venue) + '_' + str(\n\u001b[1;32m     16\u001b[0m             bin_num) +'_'+'daily.csv'\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdata_generating_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvenues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbin_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3718\u001b[0m         )\n\u001b[1;32m   3719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3720\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         )\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/volume1/home/rzhu/ste'"
     ]
    }
   ],
   "source": [
    "data_home = '/volume1/sinoalgo/data/sinoalgo/JQMarketData'\n",
    "data_types = ['STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK', 'STOCK']\n",
    "venues = ['XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE', 'XSHE']\n",
    "tickers = ['000725', '300015', '300185', '000002', '000807', '002340', '000001','300750','300059','000166','000009']\n",
    "start_date = \"2020-09-01\"\n",
    "end_date = \"2021-06-30\"\n",
    "bin_num = 25\n",
    "def data_generating_all(data_home, data_types, venues, tickers,start_date,end_date,bin_num, is_filter=1):\n",
    "    data_home = data_home\n",
    "    for i in range(len(data_types)):\n",
    "        data_type = data_types[i]\n",
    "        venue = venues[i]\n",
    "        ticker = tickers[i]\n",
    "        result_df = read_stock_data_all(data_home, data_type, venue, start_date, end_date, ticker, bin_num, is_filter=0)\n",
    "        filename_basic = '/volume1/home/rzhu/test/' + str(ticker) + '_' + str(venue) + '_' + str(\n",
    "            bin_num) +'_'+'daily.csv'\n",
    "        result_df.to_csv(filename_basic, index=False)\n",
    "\n",
    "data_generating_all(data_home,data_types,venues,tickers,start_date,end_date,bin_num,is_filter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aff2168-5b0d-4f18-84a5-700c03df3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.000000\n",
      "1    1.666667\n",
      "2    2.428571\n",
      "3    3.266667\n",
      "4    4.161290\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series([1, 2, 3, 4, 5])\n",
    "\n",
    "ewm_avg = data.ewm(alpha=0.5).mean()\n",
    "\n",
    "print(ewm_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb91d4e1-9f85-4532-a1f7-d8cf6622d3e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_974824/72142530.py\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 按日期分组，并计算每个组的几何平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mspread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spread'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bin_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeometric_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'subdf' is not defined"
     ]
    }
   ],
   "source": [
    "    def geometric_mean(data):\n",
    "        # 对数据加1\n",
    "        data_add1 = data + 1\n",
    "\n",
    "        # 将0值替换为非零小值\n",
    "        data_add1[data_add1 == 0] = 1e-9\n",
    "\n",
    "        # 计算几何平均\n",
    "        geometric_mean = np.exp(np.mean(np.log(data_add1)))\n",
    "\n",
    "        # 减去1得到最终结果\n",
    "        result = geometric_mean - 1\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    # 按日期分组，并计算每个组的几何平均\n",
    "    spread = subdf['spread'].groupby([subdf['date'], subdf['bin_num']]).apply(geometric_mean).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e43de89b-7b6b-4b79-8c53-ae2783c99cf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_974824/72142530.py\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 按日期分组，并计算每个组的几何平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mspread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spread'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bin_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeometric_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'subdf' is not defined"
     ]
    }
   ],
   "source": [
    "    def geometric_mean(data):\n",
    "        # 对数据加1\n",
    "        data_add1 = data + 1\n",
    "\n",
    "        # 将0值替换为非零小值\n",
    "        data_add1[data_add1 == 0] = 1e-9\n",
    "\n",
    "        # 计算几何平均\n",
    "        geometric_mean = np.exp(np.mean(np.log(data_add1)))\n",
    "\n",
    "        # 减去1得到最终结果\n",
    "        result = geometric_mean - 1\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    # 按日期分组，并计算每个组的几何平均\n",
    "    spread = subdf['spread'].groupby([subdf['date'], subdf['bin_num']]).apply(geometric_mean).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfeb8c-c2b7-4285-9103-e5859af8e8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
