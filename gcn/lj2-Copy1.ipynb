{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e8d98d-db86-4f29-8431-ac53b676cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42022fb6-50b9-43db-bc2b-37f924049d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d201a8e-fa1c-4075-9fb5-527d5e1ff46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入特征张量 x 根据邻接矩阵 A 进行乘法操作\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56205be8-0b1c-48b5-a586-79b97f736f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.A = nn.Parameter(torch.zeros(size=(2*out_features, 16)))\n",
    "        nn.init.xavier_uniform_(self.A.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "      \n",
    "        E = []\n",
    "        for i in range(16):\n",
    "            a = self.A[:,i].unsqueeze(1)\n",
    "            e_col = torch.matmul(a_input.double(), a.double()).squeeze(3)[:,:,i]\n",
    "            E.append(e_col)\n",
    "\n",
    "        e = self.leakyrelu(torch.stack(E, dim=2))\n",
    "        # print(e.shape)\n",
    "\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        # print(attention.shape)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb0c5614-a33f-4f74-99af-b3d569e3e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        #x = F.log_softmax(x, dim=2)[:, -1, :]\n",
    "        # print(x)\n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "\n",
    "    \n",
    "\n",
    "def train(model, train_loader,hyperparams):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0015,weight_decay=5e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for inputs, graph_input, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.squeeze(dim=1), graph_input)\n",
    "        # print('train:',outputs)\n",
    "        batch_loss = criterion(outputs, targets)\n",
    "        loss += batch_loss.item()\n",
    "        batch_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1) # 梯度裁剪\n",
    "        optimizer.step()\n",
    "    return loss / len(train_loader)\n",
    "\n",
    "def val(model, val_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "            val_outputs = model(val_inputs, val_graph_input)\n",
    "            # print('val:',val_outputs)\n",
    "            batch_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            val_loss += batch_loss.item()\n",
    "    return val_loss / len(val_loader)\n",
    "            \n",
    "\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for test_inputs, test_graph_input, _ in test_loader:\n",
    "        batch_predictions = model(test_inputs, test_graph_input)\n",
    "        predictions.append(batch_predictions)\n",
    "    predictions = torch.cat(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "146c3895-c79f-475f-9f7b-42dd4d9ccff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['603359_XSHG', '603095_XSHG', '000046_XSHE', '000753_XSHE', '600622_XSHG', '000998_XSHE', '300540_XSHE', '300343_XSHE', '002282_XSHE', '300174_XSHE', '000951_XSHE', '603053_XSHG', '002882_XSHE', '300263_XSHE', '002841_XSHE', '300133_XSHE', '002679_XSHE', '300433_XSHE']\n",
      ">>>>>>>>>>>>>>>>>>>>603359_XSHG>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch [2/300], Train Loss: 3286678781756.6660, Val Loss: 538377106009.7814\n",
      "Epoch [4/300], Train Loss: 747273477548.8772, Val Loss: 141929114444.0784\n",
      "Epoch [6/300], Train Loss: 213168530764.4167, Val Loss: 23675547993.2062\n",
      "Epoch [8/300], Train Loss: 44723553268.2044, Val Loss: 8495704852.2178\n",
      "Epoch [10/300], Train Loss: 18288544219.6943, Val Loss: 10756284856.6598\n",
      "Epoch [12/300], Train Loss: 15049878411.5586, Val Loss: 10077934100.8120\n",
      "Epoch [14/300], Train Loss: 14486733179.8047, Val Loss: 9734684719.2123\n",
      "Epoch [16/300], Train Loss: 14191560933.4112, Val Loss: 8607864839.2934\n",
      "Epoch [18/300], Train Loss: 14660940121.7720, Val Loss: 8530565337.7319\n",
      "Epoch [20/300], Train Loss: 14158259375.1398, Val Loss: 5953059829.3053\n",
      "Epoch [22/300], Train Loss: 14534194990.9676, Val Loss: 7061074131.6769\n",
      "Epoch [24/300], Train Loss: 14876762015.2498, Val Loss: 7845386354.6891\n",
      "Epoch [26/300], Train Loss: 16052060239.7737, Val Loss: 11347003479.2224\n",
      "Epoch [28/300], Train Loss: 14135700008.6448, Val Loss: 8068435999.4092\n",
      "Epoch [30/300], Train Loss: 14549753418.9979, Val Loss: 9708533602.4109\n",
      "Epoch [32/300], Train Loss: 14850157684.2659, Val Loss: 10914671820.1964\n",
      "Epoch [34/300], Train Loss: 14313045276.4432, Val Loss: 8467075600.6098\n",
      "Epoch [36/300], Train Loss: 14146153640.4085, Val Loss: 7695926552.1814\n",
      "Epoch [38/300], Train Loss: 14494770944.5768, Val Loss: 6519970785.9129\n",
      "Epoch [40/300], Train Loss: 14802334017.2660, Val Loss: 7814213517.0841\n",
      "Epoch [42/300], Train Loss: 14126853373.3224, Val Loss: 10036017827.0583\n",
      "Epoch [44/300], Train Loss: 14009902429.8349, Val Loss: 9769560113.7281\n",
      "Epoch [46/300], Train Loss: 14418158170.1166, Val Loss: 7786406380.8720\n",
      "Epoch [48/300], Train Loss: 14220288139.4821, Val Loss: 6237512158.3624\n",
      "Epoch [50/300], Train Loss: 14006800643.5196, Val Loss: 7958855241.1830\n",
      "Epoch [52/300], Train Loss: 14671257894.6700, Val Loss: 8309516153.7764\n",
      "Epoch [54/300], Train Loss: 14169817575.2109, Val Loss: 7266799639.2490\n",
      "Epoch [56/300], Train Loss: 14214036004.3659, Val Loss: 8334872423.3661\n",
      "Epoch [58/300], Train Loss: 14110920718.2639, Val Loss: 6784996508.6074\n",
      "Epoch [60/300], Train Loss: 14539727936.3925, Val Loss: 5360457826.6764\n",
      "Epoch [62/300], Train Loss: 14842101397.1327, Val Loss: 7691989431.4870\n",
      "Epoch [64/300], Train Loss: 14403529152.1531, Val Loss: 7415776454.4553\n",
      "Epoch [66/300], Train Loss: 14336763145.4836, Val Loss: 9218839353.1692\n",
      "Epoch [68/300], Train Loss: 14140517877.1890, Val Loss: 7449582297.3515\n",
      "Epoch [70/300], Train Loss: 15067790928.3722, Val Loss: 8350109552.3656\n",
      "Epoch [72/300], Train Loss: 14061341250.4981, Val Loss: 9220670402.4442\n",
      "Epoch [74/300], Train Loss: 14099974700.7724, Val Loss: 7914569641.4204\n",
      "Epoch [76/300], Train Loss: 14813397403.2545, Val Loss: 7613648729.5827\n",
      "Epoch [78/300], Train Loss: 14063030494.6365, Val Loss: 9802727284.0201\n",
      "Epoch [80/300], Train Loss: 14489636725.2483, Val Loss: 9158480512.6001\n",
      "Epoch [82/300], Train Loss: 14460909581.8886, Val Loss: 9576376835.3988\n",
      "Epoch [84/300], Train Loss: 14528790769.3051, Val Loss: 8925881268.8267\n",
      "Epoch [86/300], Train Loss: 14456220321.4698, Val Loss: 9092312322.5099\n",
      "Epoch [88/300], Train Loss: 14638254256.4246, Val Loss: 9149208752.0141\n",
      "Epoch [90/300], Train Loss: 14836341109.7648, Val Loss: 8891916258.3845\n",
      "Epoch [92/300], Train Loss: 14654450703.5660, Val Loss: 6794426504.8066\n",
      "Epoch [94/300], Train Loss: 13837487033.7880, Val Loss: 8968202094.8282\n",
      "Epoch [96/300], Train Loss: 13900308408.6233, Val Loss: 8680214659.3227\n",
      "Epoch [98/300], Train Loss: 14193517499.8856, Val Loss: 7043172015.3694\n",
      "Epoch [100/300], Train Loss: 13966771298.5200, Val Loss: 8434101874.1405\n",
      "Epoch [102/300], Train Loss: 14336176462.0158, Val Loss: 6659981112.3377\n",
      "Epoch [104/300], Train Loss: 14493337496.5254, Val Loss: 7454098649.8221\n",
      "Epoch [106/300], Train Loss: 14571373154.3381, Val Loss: 7619424823.6226\n",
      "Epoch [108/300], Train Loss: 14113954198.3336, Val Loss: 7613657219.3411\n",
      "Epoch [110/300], Train Loss: 14119174314.2334, Val Loss: 7402479989.0253\n",
      "Epoch [112/300], Train Loss: 14340859566.9976, Val Loss: 8781703085.6747\n",
      "Epoch [114/300], Train Loss: 13995275059.1229, Val Loss: 7388424168.6140\n",
      "Epoch [116/300], Train Loss: 14758247531.2097, Val Loss: 9738967824.5146\n",
      "Epoch [118/300], Train Loss: 13760266689.7873, Val Loss: 8399662887.7896\n",
      "Epoch [120/300], Train Loss: 13795975056.5877, Val Loss: 8471833778.3190\n",
      "Epoch [122/300], Train Loss: 13788862289.3981, Val Loss: 5988628939.4277\n",
      "Epoch [124/300], Train Loss: 13800761950.1998, Val Loss: 7059615417.6676\n",
      "Epoch [126/300], Train Loss: 13899059171.2994, Val Loss: 6938286448.9887\n",
      "Epoch [128/300], Train Loss: 14166042679.2031, Val Loss: 7397365132.0760\n",
      "Epoch [130/300], Train Loss: 14706363920.7184, Val Loss: 7124411988.7228\n",
      "Epoch [132/300], Train Loss: 14269342260.4991, Val Loss: 9128160290.5689\n",
      "Epoch [134/300], Train Loss: 14846889402.0102, Val Loss: 8766180751.6228\n",
      "Epoch [136/300], Train Loss: 14755461728.3911, Val Loss: 10066169650.3249\n",
      "Epoch [138/300], Train Loss: 14471633172.0838, Val Loss: 6709596460.5145\n",
      "Epoch [140/300], Train Loss: 14522643927.3362, Val Loss: 7872470056.5413\n",
      "Epoch [142/300], Train Loss: 14331885461.9045, Val Loss: 8524180693.9728\n",
      "Epoch [144/300], Train Loss: 16423834635.7582, Val Loss: 11101129767.8070\n",
      "Epoch [146/300], Train Loss: 16339512489.2442, Val Loss: 10712207383.3416\n",
      "Epoch [148/300], Train Loss: 15317095647.8904, Val Loss: 9076221807.4255\n",
      "Epoch [150/300], Train Loss: 14928301450.1653, Val Loss: 8291644966.3909\n",
      "Epoch [152/300], Train Loss: 14695068814.0971, Val Loss: 7871289826.8795\n",
      "Epoch [154/300], Train Loss: 15140396342.0719, Val Loss: 7771540452.6759\n",
      "Epoch [156/300], Train Loss: 14229197666.6931, Val Loss: 7521154182.5099\n",
      "Epoch [158/300], Train Loss: 14087172759.6231, Val Loss: 8085483585.5684\n",
      "Epoch [160/300], Train Loss: 14731170257.9981, Val Loss: 8217687156.9356\n",
      "Early stopping at epoch 160\n",
      "\n",
      "Fold number: 0\n",
      "[  1808.87157238  28994.81918228  36818.59808339   4071.51231985\n",
      "  29391.18449289  33465.21403252  38251.10355768  52783.6283786\n",
      "   3640.15151024   3933.30997184  60966.57470884   2535.30456259\n",
      "  54562.63319296   1563.0443696  147128.94382252   1788.63907037\n",
      "  85058.20337205  44665.12534408 132827.22324333    965.79092712\n",
      "   1255.93108213  33854.75390083  98794.62517164   3252.25565607\n",
      "  46491.32368373  36570.8968979   52557.10197467  25915.36904685\n",
      "  92714.51580336  55021.75588076   1365.08050251  32205.56739272\n",
      "   4403.29486443   2052.52919509 232585.93281651 204125.08333425\n",
      "  68588.00559943   3020.32832508   3864.83961273  63804.92359014\n",
      "   5082.38970172  90716.50260542  31594.7915816   28792.42249324\n",
      "  59108.91897189  24664.15728125  63840.88602566   1784.8883702\n",
      "  28593.08987891   3477.43928858   3761.49903044   2249.01489539\n",
      "  41569.1614301   32297.73481331  23236.46435453  25517.28892138\n",
      "  30878.03760133  65354.42532394  44778.27635249  61827.29936973\n",
      "  30086.13045038  61548.18379992  39015.54529397  23568.08130077\n",
      "   5866.33956252   1297.97817756  33703.9874215    3270.58088983\n",
      "  33759.19737775 129230.59931148   3330.55179355  22530.43614401\n",
      "  38119.73278671   2265.6071523    1660.01953235  52832.03389323\n",
      "  60268.00042318  65533.29309649  33993.40949951 117013.19264722\n",
      "  46947.19391643   1555.84836658  37419.70986975  63133.49450482\n",
      "  75371.21656705   1071.34557804  15430.99008675  16530.36042476\n",
      "  31597.31841266  47533.46940254  80750.61651409  23428.70631877\n",
      "  19558.99533366  45795.55378196  36135.62674327   1628.6019766\n",
      "   2045.86811687   3516.28698491  52633.65635533  93925.5332067\n",
      "  43917.67204177  66039.29083388  30898.89781969  31022.30710233\n",
      "  34391.11792983  17800.73404294   2676.00799226  23375.78802428\n",
      "  93170.68123522  48646.01101054   1185.68779033  24937.41946194\n",
      "  31464.66902237  16012.25455894   1058.75301134   2158.97284844\n",
      "   1458.47795908 136203.99910499   1267.18251921   3478.13338462\n",
      "  35669.53591109  55794.44499614  24963.47356735   1254.93703451\n",
      "  29838.82597377  30126.26583793  54827.32346047  31277.02110962\n",
      "  44385.09063434   1347.27589779  51843.22208138  54903.0506819\n",
      "  24921.54037745  52057.89267565  22976.08978887  72245.55937598\n",
      "  65927.68915877   2546.13711606   1473.00928748   5789.26251226\n",
      "  53027.45893228  21943.3355699    1394.65914203    976.95665188\n",
      "  42721.60417915   5477.86018098  39743.84949328  85150.68831817\n",
      "  90116.17591299 120708.61695437    787.13211453   3049.18665887\n",
      "  50108.03525394  28864.74960717  28710.80074968  54878.94029611\n",
      "   4074.13880127  69145.78038571  88638.5391513    1057.37022453\n",
      "   2183.72076035  62255.53803142   2501.20947405  71549.05679977\n",
      "   1347.04818228   3006.31194743 117492.74249229   1170.78734515\n",
      "  12158.87500011  45570.23796017  28291.06323823  27933.10330996\n",
      "   9294.82856068  78107.70657107  63156.81639853  53761.50879223\n",
      "  39299.91875481  62971.55247579   1339.58568526    688.86603724\n",
      "  56563.50879609    856.44589257  51194.46905834   1603.51598487\n",
      "   1902.37480063  22738.59894425  49535.51070358  87723.13877579\n",
      "  83967.13551264  67780.12776144  48158.91393707  84580.9719505\n",
      "  23183.42315857 132319.79538815  66365.07003734   1330.05664386\n",
      "  74562.4604854   84748.01622983   1798.8875986    1006.6177833\n",
      " 162165.72743769   3426.38797383  29683.70689254   1111.42614071\n",
      "  96090.55172744   1715.85917728   1241.73908429  35425.23970064\n",
      "   2715.15866737  24091.91939372 100069.21807323  51971.86426894\n",
      "   2110.23814359  37596.89511884  22069.49955129   1279.72243789\n",
      "  72832.3400137   64583.8847013     884.15064691  75751.94147326\n",
      "  19568.57303614  70324.24834867  84184.65249755  25564.18448852\n",
      "  23628.09100038  40486.52477469   2122.78089419   2668.74351277\n",
      "  14897.03026051  29181.15864108  41881.31481595  61637.31568067\n",
      "   1256.30120284  44319.61917412  34855.67194137   1455.0931699\n",
      "  31767.84577016 119320.3374081    1157.03918517  60427.86627589\n",
      "  80147.44749344  64858.18556811  53346.75335068  36902.45313831\n",
      "   2246.17846176  44708.59927406   1996.08450455  71945.76503487\n",
      "  57031.36690578  36042.84212884  32381.55634728  39794.74980469\n",
      "  33423.3022596    1976.13631785  57576.03274372  35474.32650234\n",
      "  38961.14365528  99198.15899751   2490.16893706   3683.40390292\n",
      "  20050.97547268  37840.9435627   20612.72507725   4821.11931096\n",
      "   1096.383605     1278.29421803 103108.32819771  38110.7913356\n",
      "  42643.94582121  59884.7241421   22921.89465474 102734.74975897\n",
      "  40047.45590907  41973.91855803  38912.38926702  51887.18941578\n",
      "  56892.74327324  51029.13622405  25628.56775638  46886.65209349\n",
      "  34395.47028472  26266.69954792  76172.66347642   1050.66273189\n",
      "  43910.21235038  45945.95298887  73993.48573913   2145.8789188\n",
      "  82657.17401951   2400.77345073   9676.05141622   4250.60378526\n",
      "  51695.47454648  36555.36404339   1584.08605908  37217.21780611\n",
      "  43389.36089894  33062.60226703  44461.96135232   4588.13696242\n",
      "  22589.27839502  55422.89510032 104031.75540422  73605.20035906\n",
      "  40616.03320688   1827.51069513  19630.61387633  21727.73584753\n",
      "  37884.56688311  59443.72460589  67268.82070455  29492.15349876\n",
      "  49677.15138347   2398.43846301   3151.91869779   1240.93940437\n",
      "  18179.12682321  21042.69207958   1488.14195017  33499.03274725\n",
      "   2086.04662964  45623.4018508   20850.76002768  15205.98242013\n",
      "   5540.07868143  56027.22004226  42053.07794807  22885.18744361\n",
      "  24144.38000391   1774.43312786  61776.08641633  79318.57898617\n",
      "   3279.59823352  22615.67187453  30576.97540024  27164.803774\n",
      " 144402.96027429  39072.88263874  56126.2920029   31242.50329786\n",
      "  42973.3988393  118690.61801277  54971.92528383  12925.24323041\n",
      "   4413.43945525  82036.31772368  22550.05287575   1883.28722794\n",
      "   2274.76529049  58152.08629499  39562.24166516  14745.07332528\n",
      "  28564.17596831  31608.97323372   6632.39977179   1979.33064864\n",
      "  13938.39602953  33786.33887158 189776.86765728  32888.34406304]\n",
      "[0.6409719282705643]\n",
      "MAPE =  0.6409719282705643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Train Loss: 3163062891234.0239, Val Loss: 485972047594.4319\n",
      "Epoch [4/300], Train Loss: 763199707255.8934, Val Loss: 105225884863.6493\n",
      "Epoch [6/300], Train Loss: 225385145976.5112, Val Loss: 28594230780.8580\n",
      "Epoch [8/300], Train Loss: 64389730962.3696, Val Loss: 16312614734.7734\n",
      "Epoch [10/300], Train Loss: 20613954827.3697, Val Loss: 9404573621.8939\n",
      "Epoch [12/300], Train Loss: 15213917713.3639, Val Loss: 9312568413.3236\n",
      "Epoch [14/300], Train Loss: 16063955310.6554, Val Loss: 11895492648.2221\n",
      "Epoch [16/300], Train Loss: 16266544600.9591, Val Loss: 11201035217.0159\n",
      "Epoch [18/300], Train Loss: 16547528468.5113, Val Loss: 9985235265.1630\n",
      "Epoch [20/300], Train Loss: 14749533829.7326, Val Loss: 9344567457.4802\n",
      "Epoch [22/300], Train Loss: 14639011392.8350, Val Loss: 6795987652.6507\n",
      "Epoch [24/300], Train Loss: 14888624433.1218, Val Loss: 8764204522.4808\n",
      "Epoch [26/300], Train Loss: 15882541320.7755, Val Loss: 9750330075.1395\n",
      "Epoch [28/300], Train Loss: 16030823165.6023, Val Loss: 9944142971.0068\n",
      "Epoch [30/300], Train Loss: 16705355385.8509, Val Loss: 11894719712.9517\n",
      "Epoch [32/300], Train Loss: 14690243739.4911, Val Loss: 6070136397.0621\n",
      "Epoch [34/300], Train Loss: 14565953893.0610, Val Loss: 7500369064.7837\n",
      "Epoch [36/300], Train Loss: 14758813005.6336, Val Loss: 8457470489.2507\n",
      "Epoch [38/300], Train Loss: 15569225786.1029, Val Loss: 10047162536.1071\n",
      "Epoch [40/300], Train Loss: 14361143541.2630, Val Loss: 9547587652.6977\n",
      "Epoch [42/300], Train Loss: 14043204488.1747, Val Loss: 9832620757.8426\n",
      "Epoch [44/300], Train Loss: 14473006000.8163, Val Loss: 9388853258.8767\n",
      "Epoch [46/300], Train Loss: 14234648778.1530, Val Loss: 7876356268.5503\n",
      "Epoch [48/300], Train Loss: 14653999514.2901, Val Loss: 9077542079.1170\n",
      "Epoch [50/300], Train Loss: 14558700697.1309, Val Loss: 9575699425.1672\n",
      "Epoch [52/300], Train Loss: 13590194560.0171, Val Loss: 7687216765.0182\n",
      "Epoch [54/300], Train Loss: 14129332562.1085, Val Loss: 6667109050.9291\n",
      "Epoch [56/300], Train Loss: 14294434787.4661, Val Loss: 9110082131.3719\n",
      "Epoch [58/300], Train Loss: 13930882644.4970, Val Loss: 9388444763.6617\n",
      "Epoch [60/300], Train Loss: 14623902247.5580, Val Loss: 6980215451.7066\n",
      "Epoch [62/300], Train Loss: 15169661988.3564, Val Loss: 5801303847.6275\n",
      "Epoch [64/300], Train Loss: 13948069203.9646, Val Loss: 6841369468.1930\n",
      "Epoch [66/300], Train Loss: 14911495344.4785, Val Loss: 8778158085.2329\n",
      "Epoch [68/300], Train Loss: 14623020843.6813, Val Loss: 8067613442.3380\n",
      "Epoch [70/300], Train Loss: 14542083992.0459, Val Loss: 9614875329.1073\n",
      "Epoch [72/300], Train Loss: 13967299792.9982, Val Loss: 6531716459.7571\n",
      "Epoch [74/300], Train Loss: 14343836919.7283, Val Loss: 9869569904.8035\n",
      "Epoch [76/300], Train Loss: 15337260988.7638, Val Loss: 7364203020.0487\n",
      "Epoch [78/300], Train Loss: 14192372975.4888, Val Loss: 7528198103.4477\n",
      "Epoch [80/300], Train Loss: 17014996580.5198, Val Loss: 11310697609.1598\n",
      "Epoch [82/300], Train Loss: 14411827921.8117, Val Loss: 9339692569.8702\n",
      "Epoch [84/300], Train Loss: 14793947769.3261, Val Loss: 8890639565.7454\n",
      "Epoch [86/300], Train Loss: 14325632093.8832, Val Loss: 8870620793.4647\n",
      "Epoch [88/300], Train Loss: 14150284419.8190, Val Loss: 10051400267.5090\n",
      "Epoch [90/300], Train Loss: 14621658207.2344, Val Loss: 8873429035.9826\n",
      "Epoch [92/300], Train Loss: 14593692558.2101, Val Loss: 8304820924.5265\n",
      "Epoch [94/300], Train Loss: 14180715098.1648, Val Loss: 9946752196.3959\n",
      "Epoch [96/300], Train Loss: 13971525147.6035, Val Loss: 7634647129.3663\n",
      "Epoch [98/300], Train Loss: 14922944265.3434, Val Loss: 9773913493.5070\n",
      "Epoch [100/300], Train Loss: 14368536462.7829, Val Loss: 8019801010.4748\n",
      "Epoch [102/300], Train Loss: 13897649852.2994, Val Loss: 7034658146.3070\n",
      "Epoch [104/300], Train Loss: 13673905258.3059, Val Loss: 6882710899.0521\n",
      "Epoch [106/300], Train Loss: 14556160841.7420, Val Loss: 7952025078.0168\n",
      "Epoch [108/300], Train Loss: 14296203879.8906, Val Loss: 8906869260.5837\n",
      "Epoch [110/300], Train Loss: 14332384000.6981, Val Loss: 8169033931.6007\n",
      "Epoch [112/300], Train Loss: 14424679424.0813, Val Loss: 7239710625.8581\n",
      "Epoch [114/300], Train Loss: 14305631433.9680, Val Loss: 7719353539.0378\n",
      "Epoch [116/300], Train Loss: 14517086274.9553, Val Loss: 6946655417.9815\n",
      "Epoch [118/300], Train Loss: 15838166564.5333, Val Loss: 9883004942.5932\n",
      "Epoch [120/300], Train Loss: 15026718953.8803, Val Loss: 6556622557.6538\n",
      "Epoch [122/300], Train Loss: 14732206687.7919, Val Loss: 10181765989.9873\n",
      "Epoch [124/300], Train Loss: 13938996496.3472, Val Loss: 6459990671.5252\n",
      "Epoch [126/300], Train Loss: 13926279762.2053, Val Loss: 6910205496.1481\n",
      "Epoch [128/300], Train Loss: 14327256511.2595, Val Loss: 7521508790.5345\n",
      "Epoch [130/300], Train Loss: 14722671451.6168, Val Loss: 7641597171.2674\n",
      "Epoch [132/300], Train Loss: 13616829834.8080, Val Loss: 8349300104.8289\n",
      "Epoch [134/300], Train Loss: 14554973032.7942, Val Loss: 7829743527.6428\n",
      "Epoch [136/300], Train Loss: 14508756779.6947, Val Loss: 7515676386.0643\n",
      "Epoch [138/300], Train Loss: 14245571226.2677, Val Loss: 6651625714.3785\n",
      "Epoch [140/300], Train Loss: 14746577223.6873, Val Loss: 8034247810.2186\n",
      "Epoch [142/300], Train Loss: 17448140127.4702, Val Loss: 7799440916.1071\n",
      "Epoch [144/300], Train Loss: 14078737556.9437, Val Loss: 6050372388.2471\n",
      "Epoch [146/300], Train Loss: 13940599467.7278, Val Loss: 6905231519.1429\n",
      "Epoch [148/300], Train Loss: 13991018548.8845, Val Loss: 8914518851.6403\n",
      "Epoch [150/300], Train Loss: 14136189890.3217, Val Loss: 7998784453.0364\n",
      "Epoch [152/300], Train Loss: 14299197105.0557, Val Loss: 6802435359.4491\n",
      "Epoch [154/300], Train Loss: 14463905961.4341, Val Loss: 8027241019.5859\n",
      "Epoch [156/300], Train Loss: 13942742605.5635, Val Loss: 6670013036.5885\n",
      "Epoch [158/300], Train Loss: 13618438846.6971, Val Loss: 6032394551.3435\n",
      "Epoch [160/300], Train Loss: 14434821599.9906, Val Loss: 6675452527.0060\n",
      "Epoch [162/300], Train Loss: 14650038023.5831, Val Loss: 7104677857.4076\n",
      "Early stopping at epoch 162\n",
      "\n",
      "Fold number: 1\n",
      "[ 33416.97836013  57583.17189301  47629.49045914 129194.44279111\n",
      "  39504.77663876  68053.62908754  38647.38601586  41627.68298348\n",
      "  46429.95810941  28978.22290568  67646.73242283  34527.74071885\n",
      "  32249.62303942  35494.35339839   3411.83630457  43692.52452536\n",
      " 110498.40969632  68151.40641299 105392.10522128  11355.49244469\n",
      "  24937.08378115  31443.48738264  68776.95533392  82638.50271453\n",
      "  44047.61835732  21085.64904243  53322.3375456   19218.94669339\n",
      "  54071.76788408  62645.80729148  25881.14519171  24139.74132823\n",
      "  92843.0811268   51954.46039028 232404.9151689  162419.71579023\n",
      "  41165.72695722  37315.8524148   87249.0226853  110887.48556774\n",
      "  86478.28207623  80749.52969463  46128.36074922  30539.93465916\n",
      "  96596.49793173  40809.35975727  73195.51376002  52236.69693857\n",
      "  28932.50490326  61698.3408109   80544.54230725  36332.80917768\n",
      "  24320.92254212  43828.99820794  19621.50847587  21179.7849856\n",
      "   3414.07649785  41285.06663432  35225.00116397  54801.52414921\n",
      "  32684.14485378  39987.26771647  40737.11706693  31614.90585674\n",
      " 112658.71747892  24594.02303231  51551.03614585  34998.19365375\n",
      "  38584.66159492   5345.0966294   63927.18249877   1010.32200957\n",
      "  29756.27341196   1756.7991922   39074.38592249  49379.83319272\n",
      "  41572.24533879   3112.75096686  36663.60593281   8235.89598363\n",
      "  47088.96789607  41489.28992166   2598.35080093  51531.70893764\n",
      "  45388.05260195  19698.04118859  26152.63051758  19758.99606655\n",
      "  10906.82842966  54282.96528671   2482.73466745  23249.8215072\n",
      "  30403.38839151  50908.75276721  48228.83476508  37738.03221893\n",
      "  58917.38136399  75736.19999208  43799.16824965  33666.24731793\n",
      "  30266.36868713  68791.59057791  43330.85707342  25608.93937386\n",
      "   1031.23507832  21578.2724482   61662.75173133  15819.30821302\n",
      "  73959.46544985  43249.30261981  14001.3100596   32339.45985037\n",
      "   1165.91353031  24460.37809538  26692.29983673  46598.6734802\n",
      "  22364.78167272  48233.52479226  15672.53375801  56716.06714033\n",
      "  36767.67170896  85040.53299685  18365.00321474  16461.57338671\n",
      "  27722.56114305  29625.43747656  32923.85619917  42682.42675775\n",
      "  47884.8948709   28548.20167555  42813.12524165  60606.8032911\n",
      "  19898.5040511   41433.10222727  30508.81339336  77713.5347901\n",
      " 141239.06738965  77547.87742799   1459.47859534 103176.72301378\n",
      "  90647.36348057   1282.0860566   33208.49611616  18581.56419048\n",
      "  75970.30309729 121613.20499204  50212.25218938  74339.84776874\n",
      "  68537.93965153 135657.11098219  31240.07987432  94910.42999281\n",
      "   4174.34070536  42865.82733662   1823.22728158  40441.75313303\n",
      "  60632.42219196 149033.67066996  61901.15682133  21075.8337387\n",
      "  53594.66214159 106011.84485624  37273.8278323   91219.11305435\n",
      "  24045.95121119  51445.18065041   3700.79874756    934.07938313\n",
      "  60243.59943628   1820.62794756  17892.21402632   1395.7226228\n",
      "  16634.88790606  67498.43094901  74347.34160674  46212.18454988\n",
      "  30337.04466165  53539.95551847  27164.93085869  19929.11507063\n",
      "   2662.99704901  10039.08692325  55248.20240274  18124.92221666\n",
      "  29596.62803425  25069.53260356  31111.0972075   63847.9358291\n",
      "   4939.98612791  78508.24926876  47591.93704705  76648.50755396\n",
      "  58677.94213242 172223.19413621  51907.01719505    985.12128577\n",
      "  76640.38199498  53373.75673168   1846.23732061  27312.62252572\n",
      "  56250.82053141  59199.42848745  28169.14470923  25176.10433312\n",
      " 153655.60832005  28660.83122396  44387.97157492  20228.18769732\n",
      "  51190.93450372   1823.24758429   4103.68211234  44479.30661614\n",
      "  23544.53651656  31437.77156112  21807.36147012  20602.34806506\n",
      "   3457.46388939  45720.29306449  16337.17426288  91637.51155543\n",
      "  44777.57716497  30666.7755541   66022.51331254  33601.43870454\n",
      "  60882.11725368  52769.25513481  24837.08595325  58807.90844108\n",
      "  14785.65118552  19345.71030825  24709.88703128  67694.28576258\n",
      "  24945.90482238  68543.61395118  25802.84218584   6861.26356324\n",
      "   2870.50097856 113043.38117014  10059.57011851  34372.41383936\n",
      "  94285.70664311 105849.56254935  32288.03884788  33192.65810747\n",
      "  36488.0351381   37772.82685897  46535.65857204  62603.84010554\n",
      "  64657.55117981   3389.56072152  19841.72659275  40276.40927895\n",
      "  33474.87582142  43271.78211914  41888.27138265  18123.15239765\n",
      "  47475.31150867   4053.84087291  49460.81344651  42713.67986405\n",
      "  13691.86550355  51078.2339997   10241.272933    97651.99733802\n",
      "  13488.35926821   1260.83150551  73768.98538129   2623.96349586\n",
      "  38028.88581015  58224.87810496  38903.35370265 188649.06013526\n",
      "  40048.27290392  37221.80107969   1698.48328043  57826.93322756\n",
      "  41686.96242941  42628.12686376  25538.64270453  96629.48653461\n",
      "  43663.03638983   1681.50648368  76314.52597676  21627.51599225\n",
      "  34401.91504129  44200.0668055   75149.70931695  37078.08206558\n",
      "  19306.21122187  69642.70335696  38358.81703993  57513.76061712\n",
      "  66056.29325107  41463.74595656  37372.10028395   1834.34556704\n",
      "  25781.3155551   62921.87081477  36043.93661439  52005.15391232\n",
      "  41120.97667609 101699.99852015 110117.08506675  91602.52433176\n",
      "  85614.65988814  17382.94661547  23882.06686585    964.28293604\n",
      "   2017.57766638  32524.67159723   4990.0077752   32636.6914203\n",
      "  44782.92913361  55033.41351192  76227.23248701  24023.51508313\n",
      "  21763.65007571  42349.91954889  22002.68129941  27379.43255615\n",
      "   2365.89604506  61475.35273716  33067.69307065  14679.36459305\n",
      "  93738.837248    50368.0728897   38027.69474588  25834.6583841\n",
      "  29715.64332415  23887.71145279  26857.23867538   2197.31516807\n",
      "  36458.73099478  20462.62013608  27275.45723439  36128.31666639\n",
      " 122251.83956397   2064.76298429 103298.63132847  41367.71138633\n",
      "  40379.27209964  97097.41590379 103987.66099229  29579.37773493\n",
      "  63895.98100244  57735.74003413  29446.47470041   1554.87196022\n",
      "    921.89688248  33630.45412567   1691.95682215    527.21672219\n",
      " 117409.81145738  34483.90667095 112162.73567875  16727.47775445\n",
      "  23916.69129667  20752.75852042 188714.67811204  37857.10790829]\n",
      "[0.5469233897036649]\n",
      "MAPE =  0.5469233897036649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Train Loss: 2920001628524.2837, Val Loss: 679940952163.9865\n",
      "Epoch [4/300], Train Loss: 679005737666.2915, Val Loss: 116381442502.7744\n",
      "Epoch [6/300], Train Loss: 230927137945.7052, Val Loss: 33882853885.8984\n",
      "Epoch [8/300], Train Loss: 62137955038.0596, Val Loss: 14414511600.3306\n",
      "Epoch [10/300], Train Loss: 20260880483.1250, Val Loss: 11092257780.5148\n",
      "Epoch [12/300], Train Loss: 14990942136.7204, Val Loss: 12545762789.9857\n",
      "Epoch [14/300], Train Loss: 14453160737.6271, Val Loss: 10251059624.6563\n",
      "Epoch [16/300], Train Loss: 14432123227.7560, Val Loss: 9332421850.1904\n",
      "Epoch [18/300], Train Loss: 14601556636.6214, Val Loss: 9568426857.8472\n",
      "Epoch [20/300], Train Loss: 14604773985.1542, Val Loss: 12490824532.1587\n",
      "Epoch [22/300], Train Loss: 14184807304.8029, Val Loss: 10947144078.9633\n",
      "Epoch [24/300], Train Loss: 14460258523.4584, Val Loss: 9927694612.4449\n",
      "Epoch [26/300], Train Loss: 14334721840.2279, Val Loss: 8579341093.4288\n",
      "Epoch [28/300], Train Loss: 13962388329.3800, Val Loss: 10669774337.8297\n",
      "Epoch [30/300], Train Loss: 14044584046.2559, Val Loss: 10067210902.6613\n",
      "Epoch [32/300], Train Loss: 14005944433.7032, Val Loss: 11880772710.9795\n",
      "Epoch [34/300], Train Loss: 14337084258.0112, Val Loss: 10986824719.8394\n",
      "Epoch [36/300], Train Loss: 14398371813.8120, Val Loss: 10172116025.7415\n",
      "Epoch [38/300], Train Loss: 13850714652.8116, Val Loss: 10224275194.4130\n",
      "Epoch [40/300], Train Loss: 13873606046.1912, Val Loss: 9756595089.5855\n",
      "Epoch [42/300], Train Loss: 14015122032.1700, Val Loss: 9675992974.1626\n",
      "Epoch [44/300], Train Loss: 14048818586.0966, Val Loss: 9355757735.0820\n",
      "Epoch [46/300], Train Loss: 13908585024.5068, Val Loss: 8674164146.0755\n",
      "Epoch [48/300], Train Loss: 13494817195.0628, Val Loss: 8345407438.0302\n",
      "Epoch [50/300], Train Loss: 13681196371.3453, Val Loss: 8728913122.3901\n",
      "Epoch [52/300], Train Loss: 14156632966.1205, Val Loss: 10322909402.3736\n",
      "Epoch [54/300], Train Loss: 13937266454.0939, Val Loss: 10587284216.5234\n",
      "Epoch [56/300], Train Loss: 14150307472.9429, Val Loss: 11123720855.4257\n",
      "Epoch [58/300], Train Loss: 14059872734.8726, Val Loss: 12294602304.5899\n",
      "Epoch [60/300], Train Loss: 13959478865.0965, Val Loss: 9821238099.3874\n",
      "Epoch [62/300], Train Loss: 15105359157.8424, Val Loss: 10708867453.4049\n",
      "Epoch [64/300], Train Loss: 13609480839.0393, Val Loss: 9488706308.7473\n",
      "Epoch [66/300], Train Loss: 13816570135.8946, Val Loss: 8442076859.2552\n",
      "Epoch [68/300], Train Loss: 13496201127.8044, Val Loss: 6145110260.7620\n",
      "Epoch [70/300], Train Loss: 13608713631.0502, Val Loss: 8593162670.4015\n",
      "Epoch [72/300], Train Loss: 13752307357.0419, Val Loss: 9751433351.6764\n",
      "Epoch [74/300], Train Loss: 13602830080.3074, Val Loss: 10990141992.1454\n",
      "Epoch [76/300], Train Loss: 13954574854.2835, Val Loss: 8175260274.2481\n",
      "Epoch [78/300], Train Loss: 14262109481.0740, Val Loss: 15047437552.8517\n",
      "Epoch [80/300], Train Loss: 15657556932.5229, Val Loss: 10386536174.2841\n",
      "Epoch [82/300], Train Loss: 13886644272.0601, Val Loss: 8825065119.9088\n",
      "Epoch [84/300], Train Loss: 14053542050.9131, Val Loss: 7335625726.7462\n",
      "Epoch [86/300], Train Loss: 13366828765.6270, Val Loss: 8168507883.4458\n",
      "Epoch [88/300], Train Loss: 13609789224.3548, Val Loss: 9412741756.0797\n",
      "Epoch [90/300], Train Loss: 14079108864.0525, Val Loss: 9585399948.2282\n",
      "Epoch [92/300], Train Loss: 13979208603.2816, Val Loss: 8157471961.1018\n",
      "Epoch [94/300], Train Loss: 13700247087.3678, Val Loss: 8663564614.5679\n",
      "Epoch [96/300], Train Loss: 13693790026.3265, Val Loss: 10576060821.7365\n",
      "Epoch [98/300], Train Loss: 14331107165.1134, Val Loss: 8178056288.2355\n",
      "Epoch [100/300], Train Loss: 15532680599.4632, Val Loss: 11645267086.5305\n",
      "Epoch [102/300], Train Loss: 14274684388.3313, Val Loss: 11119450634.9173\n",
      "Epoch [104/300], Train Loss: 13986090445.3665, Val Loss: 14730555122.2668\n",
      "Epoch [106/300], Train Loss: 14131938088.2685, Val Loss: 10440376714.7417\n",
      "Epoch [108/300], Train Loss: 13789106534.5182, Val Loss: 10308527355.7115\n",
      "Epoch [110/300], Train Loss: 13517765207.6079, Val Loss: 10597421201.6158\n",
      "Epoch [112/300], Train Loss: 13918404800.8375, Val Loss: 8475412261.9702\n",
      "Epoch [114/300], Train Loss: 13462762469.9805, Val Loss: 7890203418.2870\n",
      "Epoch [116/300], Train Loss: 13347054247.7112, Val Loss: 9458904380.4516\n",
      "Epoch [118/300], Train Loss: 13460449503.3593, Val Loss: 8163140983.7309\n",
      "Epoch [120/300], Train Loss: 13714707999.5828, Val Loss: 11396822974.7629\n",
      "Epoch [122/300], Train Loss: 13639384837.8805, Val Loss: 9165672750.6140\n",
      "Epoch [124/300], Train Loss: 13778404499.0469, Val Loss: 9115510553.2511\n",
      "Epoch [126/300], Train Loss: 14708863440.4531, Val Loss: 9453788599.2466\n",
      "Epoch [128/300], Train Loss: 14141849144.8365, Val Loss: 10274860918.4862\n",
      "Epoch [130/300], Train Loss: 13419030980.8603, Val Loss: 6836213528.2209\n",
      "Epoch [132/300], Train Loss: 13766510616.6982, Val Loss: 8046310041.4543\n",
      "Epoch [134/300], Train Loss: 13804311611.3694, Val Loss: 8223415718.5990\n",
      "Epoch [136/300], Train Loss: 13826663366.7517, Val Loss: 8010018572.8851\n",
      "Epoch [138/300], Train Loss: 13482631912.4541, Val Loss: 8957689861.0489\n",
      "Epoch [140/300], Train Loss: 13903599663.9481, Val Loss: 8553137045.7261\n",
      "Epoch [142/300], Train Loss: 13742358532.5764, Val Loss: 10735705272.7225\n",
      "Epoch [144/300], Train Loss: 13975939912.9419, Val Loss: 9018818622.6335\n",
      "Epoch [146/300], Train Loss: 14427308700.9296, Val Loss: 10532973534.9598\n",
      "Epoch [148/300], Train Loss: 13571052848.0935, Val Loss: 9015279661.7101\n",
      "Epoch [150/300], Train Loss: 13412336317.9105, Val Loss: 10940112361.8423\n",
      "Epoch [152/300], Train Loss: 12917623848.2917, Val Loss: 9419405356.3106\n",
      "Epoch [154/300], Train Loss: 13574127972.4350, Val Loss: 8645511245.1899\n",
      "Epoch [156/300], Train Loss: 13408033701.5144, Val Loss: 9352889297.3161\n",
      "Epoch [158/300], Train Loss: 13956203489.9679, Val Loss: 8184305989.6138\n",
      "Epoch [160/300], Train Loss: 13989936196.6775, Val Loss: 8465888348.3090\n",
      "Epoch [162/300], Train Loss: 14325736214.3807, Val Loss: 8581022510.1657\n",
      "Epoch [164/300], Train Loss: 13953670875.3557, Val Loss: 7136892484.2050\n",
      "Epoch [166/300], Train Loss: 13560041570.3292, Val Loss: 8097742917.2807\n",
      "Epoch [168/300], Train Loss: 14705481131.1435, Val Loss: 10279750990.2285\n",
      "Early stopping at epoch 168\n",
      "\n",
      "Fold number: 2\n",
      "[ 22790.70777744  14561.25557474  21066.04107669 146742.52365328\n",
      "  37356.30077325  63456.70974286  23727.24802118  34058.15734973\n",
      "  64637.03742524 136689.46814943  85111.84508829  13363.85612192\n",
      "  52274.40544285  48004.88521366 107165.86100418  25253.59010083\n",
      "  88356.77103814  56910.280615   122225.32577463  20792.60154485\n",
      "  46630.05841303  42667.58919367  61806.3661469    2173.17970902\n",
      "   2263.10097651   1289.86598312  56600.27884947  17964.39420456\n",
      "  38246.10566956  30095.83982614  19434.89480396  31970.35840628\n",
      "  97215.39443368  58598.39873099 288200.16052493 156826.27577612\n",
      "   2657.79292643   2114.22078313  72675.22896925 157257.43726565\n",
      "  85170.66504186  65595.05389394  30331.33093361  53587.3480137\n",
      "  44760.98381736  27717.87938334  56747.4264781   66442.43243943\n",
      "  40161.40831102  45285.63913431  33237.50532375  45932.65761384\n",
      "  52044.14358923   1408.38279851  29109.67508883  25286.92606092\n",
      "  29007.05403989  57974.85913519  32337.27305999  85655.37177913\n",
      "  20113.98380259  56218.26164429  32681.80080605  27175.05286947\n",
      " 116767.20624565  15349.02175638  70497.6967321   40381.9131765\n",
      "  33441.62487056   3385.14343265  95893.9555864   43540.56400719\n",
      "  62108.13422438  73403.68639262  73080.47540775  28276.57194428\n",
      "  67377.92664596  40939.34957422  20073.88396926  49374.81873195\n",
      "  42867.477036    43798.27934311  24097.61471387  55380.41521382\n",
      "   1853.35522836  27166.50275233  33162.68633787  36456.4692599\n",
      "  17488.36523215  22022.82856052  87006.79868678  24118.89516287\n",
      "  27858.04290665  49346.10693225  38406.34096135  58977.25622919\n",
      "  70720.44476199  97180.00332065  33879.51175432 129652.96854866\n",
      "  20678.48910859  58304.05346657  46268.41261599  52237.41958278\n",
      "  39291.2765019     539.86645721  43344.88927941  17078.968912\n",
      " 124125.51100384  47848.98139461  46611.95310757  21731.11116043\n",
      "   1161.54148087  25922.21608148  23600.45262074  74817.18226911\n",
      "  20111.03417655  64038.61213841    748.02513807  56897.44444741\n",
      "  24876.93053841   5200.01874308  23040.62567333  21056.29859796\n",
      "  41657.16412082  24662.95956925  76413.64367234  36429.63641761\n",
      "  72458.21013711  24279.89439658  54932.35501425  43280.17305921\n",
      "  34410.62610251  26270.12857354  18367.19602459  38301.95520397\n",
      " 118753.79115802  46891.51254263  62533.45069976 137024.86405975\n",
      "  86517.57411546   1242.68737295  23815.67009062  21208.14669854\n",
      "  62670.22929619 141236.45900866   1217.08139988  82232.67809197\n",
      "  58922.01568045 118661.99638771  29359.03267207   1398.27328237\n",
      "  21115.34584379  35852.10402653  57210.69337845  91591.06793169\n",
      "  64797.08673175 135857.79558867  74057.57407507  46938.47959372\n",
      "  34703.14513352 162510.19051207  45911.60720265   2291.79962489\n",
      "  12033.15162235  66041.63093428  88838.94453666  31507.19274987\n",
      "  55133.72827628  43952.82966155    907.79456208  15244.85813018\n",
      "  11462.04579632   2403.11422406 105529.70610918  69553.1623411\n",
      "  22295.56615583  43634.59183995  21018.97630023  31888.42324857\n",
      "  39524.4044811   12938.6510004   86631.29689691  18274.0600677\n",
      "  58304.05603902  35119.13076105  40144.82667149  49057.26489185\n",
      " 102275.15310498  45562.94609529  40859.39784907  64159.73748406\n",
      "  41306.94295924 168264.92095159  47645.28133841  27026.9221978\n",
      "  36020.86830653  46473.94803351  29937.56649755  21762.39791782\n",
      "  99316.81661482  81450.99453172  75374.58824821  30215.77137149\n",
      "  97563.12095185  15257.98518737  36703.93574264  45756.4840676\n",
      "  24182.50734144  15944.42683269  55351.28712114  37888.1810049\n",
      "  68014.91472867  65961.04080395  16290.64350916  35955.5785264\n",
      "  41605.18039442  41217.16275226  14692.44949778 111408.03662706\n",
      "  28623.8524508   75119.54668932  61217.05181879  40637.20455451\n",
      "  54861.04354386  50013.13266587  36762.72705488  51688.34287059\n",
      "  22003.61216313  32926.78786905  48166.80493918 103946.18610041\n",
      "  24738.72509476  46186.82277952    961.08522975  25935.4707162\n",
      "  17736.86044626  97008.32329677  10857.91226728  59473.5115204\n",
      "  52052.29949076  76824.04213655   1860.14246199   1301.87695501\n",
      "  26054.13669375  65852.20515429  66352.70152581  82475.81944678\n",
      "  37413.01563383  20032.22090553  53117.45151609  16121.68829127\n",
      "  19384.80348953  64364.47488504  38772.21253958   1400.7060139\n",
      "  26457.88751246   4256.7973898   38799.03491387  88792.44589206\n",
      "   1014.09045684  34190.51134655  31299.50477658   2480.7258365\n",
      "  22831.12335134  25518.51534279  51054.53644165  40142.08578383\n",
      "  32951.21069324  73027.41343597  32756.59392835  64342.61623904\n",
      "  49996.60185492  39176.77292357   1067.15706271  29690.16200006\n",
      "   2040.02220538  49727.00859934  41139.03989321  73287.59205922\n",
      "  31156.7613429   26896.00430975  45976.32852396  18245.03104033\n",
      "  29744.31655379  73934.59114677  38761.09585849  38691.86036546\n",
      "   3255.36085468  48625.42796347  34964.30959436 154074.79689985\n",
      "  43759.45817836  29190.38971566  39623.17053834  75896.16367474\n",
      "  25108.13186272  47048.14670768   1364.67767741  68174.22206481\n",
      "  61479.94868576   2015.79645252  90151.92216501  42407.70088577\n",
      "  75211.60842156    823.81164563  23883.7188396   35012.52972684\n",
      "  46825.3391864   96217.23216009   1750.20738175  39963.90105629\n",
      "  80795.22998453  39649.20934298  88305.89703654  35692.68957112\n",
      "  30528.13823046  21494.62883852  58591.63923143  37359.5923163\n",
      "   2442.85603146  88195.69557729  10684.3637564   46656.9734883\n",
      "  57485.18504725  65201.56649722  50164.3305962   25044.13857212\n",
      "  31941.94429964    970.51405973  39922.90005831  42362.45110139\n",
      "  34452.41318516  21896.0457852   18309.07663879  33685.64466441\n",
      "  62678.93657709  20272.76884868  44112.43184868  54639.92814347\n",
      "  52649.26749677  69774.11859638 110205.66783786  28177.3884131\n",
      "  74095.4282502   64593.9039794   22592.09468085  83146.12659408\n",
      "  28733.65099657   1813.65402513  63542.14218575  26647.76868563\n",
      " 116998.88897545  23723.61582153 108907.85945413  50744.65553328\n",
      "  19590.22768525  25593.88223403 103500.68432511  29149.93018348]\n",
      "[0.5785311254494456]\n",
      "MAPE =  0.5785311254494456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Train Loss: 2682053591630.3643, Val Loss: 703363971228.9657\n",
      "Epoch [4/300], Train Loss: 802625714085.4012, Val Loss: 193841117100.9097\n",
      "Epoch [6/300], Train Loss: 169886308675.1985, Val Loss: 48427283529.8082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m val(model, val_loader)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[47], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, graph_input, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# print('train:',outputs)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[47], line 21\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([att(x, adj) \u001b[38;5;28;01mfor\u001b[39;00m att \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 将每个head得到的表示进行拼接\u001b[39;00m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)   \u001b[38;5;66;03m# dropout，防止过拟合\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_att\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(x)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 39\u001b[0m, in \u001b[0;36mGraphAttentionLayer.forward\u001b[0;34m(self, inp, adj)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m):\n\u001b[1;32m     38\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA[:,i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     e_col \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(a_input\u001b[38;5;241m.\u001b[39mdouble(), \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)[:,:,i]\n\u001b[1;32m     40\u001b[0m     E\u001b[38;5;241m.\u001b[39mappend(e_col)\n\u001b[1;32m     42\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu(torch\u001b[38;5;241m.\u001b[39mstack(E, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/fx/traceback.py:57\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_stack\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/traceback.py:197\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformat_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/traceback.py:39\u001b[0m, in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_list\u001b[39m(extracted_list):\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format a list of tuples or FrameSummary objects for printing.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Given a list of tuples or FrameSummary objects as returned by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    whose source text line is not None.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/traceback.py:423\u001b[0m, in \u001b[0;36mStackSummary.format\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    422\u001b[0m row \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 423\u001b[0m row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m  File \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, line \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, in \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mline:\n\u001b[1;32m    426\u001b[0m     row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(frame\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = list(set(s.split('_25')[0] for s in files))\n",
    "    print(stocks_info)\n",
    "    num_epochs=300\n",
    "    patience=100\n",
    "    for stock_info in stocks_info[0:2]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        mape_list = []\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(testInputs), torch.tensor(testgraphInput), torch.tensor(testTargets))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "        K = 5  # Number of folds\n",
    "        for k in range(K):\n",
    "            torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "            trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "            trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "            trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "            valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "            valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "            trainInputs = normalize(trainInputsAll[:, :])\n",
    "            valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "            # Assuming trainInputs, trainGraphInput, trainGraphFeatureInput, trainTargets are PyTorch tensors\n",
    "            train_dataset = TensorDataset(torch.tensor(trainInputs), torch.tensor(trainGraphInput),torch.tensor(trainTargets))\n",
    "            val_dataset = TensorDataset(torch.tensor(valInputs), torch.tensor(valGraphInput),torch.tensor(valTargets))\n",
    "\n",
    "            # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "            # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "        \n",
    "            model = GAT(7,8,1, 0.3, 1,2)\n",
    "            best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "            patience_counter = 0\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.0015,weight_decay=5e-4)\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                loss = train(model, train_loader)\n",
    "                val_loss = val(model, val_loader)\n",
    "                if (epoch + 1) % 2 == 0:\n",
    "                    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f'Early stopping at epoch {epoch+1}')\n",
    "                        break\n",
    "\n",
    "            predictions=predict(model, test_loader)\n",
    "            torch.save(model.state_dict(), f'models/gat_{stock_info}_{lag_bin}_{lag_day}_gcn_model_iteration_{k}.pt')\n",
    "    \n",
    "            print()\n",
    "            print('Fold number:', k)\n",
    "\n",
    "            new_predictions = np.array([item.detach().numpy() for item in predictions]).flatten()\n",
    "            MAPE = []\n",
    "\n",
    "            MAPE.append(mean_absolute_percentage_error(testTargets[:], new_predictions[:]))\n",
    "            print(new_predictions)\n",
    "            print(MAPE)\n",
    "            testTargets0 = list(testTargets)\n",
    "\n",
    "            res = {\n",
    "                'testTargets': testTargets0,\n",
    "                'new_predictions': new_predictions\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame(res)\n",
    "            res_df.to_csv(f'./result/gat_{stock_info}_{lag_bin}_{lag_day}_res_test_MAPE{k}.csv', index=False)\n",
    "\n",
    "            print('MAPE = ', np.array(MAPE).mean())\n",
    "            MAPE_mean = np.array(MAPE).mean()\n",
    "            mape_list.append(MAPE)\n",
    "\n",
    "        print('-')\n",
    "        print('mape score = ', mape_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd63c7a-d5a4-49cb-8983-9e83659a3878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e40f93-e0c6-4d7a-af6a-b3fe8ab0f379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c295f-9759-4a21-9848-912604869f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
