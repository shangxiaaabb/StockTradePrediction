{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e99867e-b47b-4bd1-b701-6860aee8f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\")\n",
    "import torch\n",
    "from torch_geometric.nn import NNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf07276-d815-4ae6-8bbe-c1853df92f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7749e+00, -5.6626e-01, -8.0984e-01, -7.1449e-02,  2.8582e-01,\n",
      "         -3.8365e-01,  7.5139e-02],\n",
      "        [ 7.0312e-02, -1.5984e-01,  8.4776e-02, -9.3610e-01,  8.9785e-01,\n",
      "         -8.2540e-01,  1.4266e-01],\n",
      "        [ 2.6638e-02, -2.3671e-01,  1.5674e-01,  8.1363e-01, -2.0409e-01,\n",
      "         -3.1345e-01,  1.0280e+00],\n",
      "        [ 5.7172e-01, -1.9054e+00,  9.1754e-01, -9.6629e-01,  4.9989e-01,\n",
      "         -2.1724e+00,  1.0834e+00],\n",
      "        [-1.1801e-01, -2.5599e-01,  6.2659e-02, -6.9156e-01,  5.2550e-01,\n",
      "          4.2122e-01,  2.0988e-01],\n",
      "        [-3.5079e-01, -2.4010e-01, -3.0144e-01, -7.8398e-01,  6.8617e-01,\n",
      "          3.8327e-01, -2.0546e-01],\n",
      "        [-2.0752e-01,  7.2908e-01,  2.7051e-01, -2.1384e-01,  5.2462e-01,\n",
      "          4.0481e-01,  2.1597e-01],\n",
      "        [ 1.2449e-01,  1.5051e-01, -1.3724e-01,  2.4650e-01, -6.2854e-01,\n",
      "          1.8358e-01,  2.5162e-01],\n",
      "        [-1.3348e-03, -2.9526e-01, -6.4837e-01, -5.4594e-01, -1.0584e-01,\n",
      "          2.4330e-01,  2.3672e-02],\n",
      "        [-1.0562e-01, -7.6398e-01,  3.7003e-01,  2.7156e-01,  5.1140e-02,\n",
      "         -4.4758e-01, -3.5205e-01],\n",
      "        [ 4.9960e-02,  1.7305e-01, -5.2039e-01, -8.3642e-01,  2.2278e-01,\n",
      "          8.9327e-01,  1.3236e-01],\n",
      "        [-5.0937e-01, -1.6108e-01, -1.7762e-01, -2.1240e-01,  8.7808e-02,\n",
      "         -4.5712e-01,  9.1166e-01],\n",
      "        [ 9.2813e-01,  5.3755e-01,  4.6891e-01,  4.6850e-01, -1.0803e-01,\n",
      "         -2.1708e-01,  9.7082e-02],\n",
      "        [ 4.4951e-01, -6.2035e-01,  2.1271e-01,  2.1495e-01,  1.7646e-01,\n",
      "          7.4921e-02, -1.4298e+00],\n",
      "        [ 5.5485e-01,  1.0174e+00,  2.1573e-01, -2.5215e-01,  2.7308e-01,\n",
      "          6.8831e-01,  2.2911e-01],\n",
      "        [ 4.7957e-01, -9.5723e-01,  1.3137e-01,  3.6309e-01, -3.2064e-01,\n",
      "         -3.2841e-01, -6.7073e-01]], grad_fn=<AddBackward0>)\n",
      "torch.Size([16, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as f\n",
    "\n",
    "# 生成一些示例数据\n",
    "x = torch.randn(16, 7)\n",
    "# 4 个节点，每个具有 16 个特征\n",
    "edge_index = torch.tensor([[0,1, 1, 2, 2, 3],[1, 0, 2, 1, 3, 2]], dtype=torch.long)\n",
    "# 边的索\n",
    "e = torch.randn(6, 5)\n",
    "# 6 条边，每边具有 4 个特征\n",
    "\n",
    "# 定义 NNConv 层\n",
    "model_nn = nn.Sequential(nn.Linear(5, 7), nn.ReLU(), nn.Linear(7, 7*7))\n",
    "conv = NNConv(7, 7, model_nn, aggr='mean')\n",
    "\n",
    "# 运行 NNConv 层\n",
    "out = conv(x, edge_index, e)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160f4062-acf2-4d2d-bb5e-8a3d4865f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.A = nn.Parameter(torch.zeros(size=(2*out_features, 16)))\n",
    "        nn.init.xavier_uniform_(self.A.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        edge_index = torch.stack([np.nonzero(t)for t in torch.unbind(adj,dim=0)],dim=0).permute(0,2,1)\n",
    "        edge_index=edge_index[0][[1, 0], :]\n",
    "        \n",
    "        diff = edge_index[0] - edge_index[1]\n",
    "        unique_diff = torch.unique(diff)\n",
    "        # 创建一个字典，用于将差值映射到 one-hot 编码\n",
    "        mapping = {val.item(): torch.eye(len(unique_diff))[i] for i, val in enumerate(unique_diff)}\n",
    "        e = torch.stack([mapping[val.item()] for val in diff])# 42条边，每边具有 4 个特征\n",
    "\n",
    "        # 定义 NNConv 层\n",
    "        model_nn = nn.Sequential(nn.Linear(5, 7), nn.ReLU(), nn.Linear(7, 7*7))\n",
    "        conv = NNConv(7, 7, model_nn, aggr='mean')\n",
    "\n",
    "        # 运行 NNConv 层\n",
    "        out = conv(x, edge_index, e)\n",
    "\n",
    "        \n",
    "        \n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "      \n",
    "        E = [torch.matmul(a_input.double(), self.A[:,i].unsqueeze(1).double()).squeeze(3)[:,:,i] for i in range(N)]\n",
    "\n",
    "        e = self.leakyrelu(torch.stack(E, dim=2))\n",
    "        # print(e.shape)\n",
    "\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        # print(attention.shape)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.mean([att(x, adj) for att in self.attentions], dim=0)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "        \n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        super(GATModel, self).__init__()   \n",
    "        self.gat1 = GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True)\n",
    "        self.gat2 = GraphAttentionLayer(n_hid, n_hid, dropout=dropout, alpha=alpha, concat=True)\n",
    "        self.gat3 = GraphAttentionLayer(n_hid, n_hid, dropout=dropout, alpha=alpha, concat=False)\n",
    "        self.linear2 = torch.nn.Linear(n_hid, n_hid).double() \n",
    "        self.linear3 = torch.nn.Linear(n_hid, n_class).double() \n",
    "    def forward(self, x, adj):\n",
    "        h = self.gat1(x, adj)\n",
    "        h = F.leaky_relu(h, negative_slope=0.2)\n",
    "        h = self.gat2(h, adj)\n",
    "        h = F.leaky_relu(h, negative_slope=0.2)\n",
    "        h = self.gat3(h, adj)\n",
    "        last_column = h[:, -1]\n",
    "        conv1_new = last_column.unsqueeze(-1)\n",
    "        #conv1_new = torch.nn.Flatten(conv1_new)\n",
    "        #conv1_new = F.dropout(conv1_new,p=0.3)\n",
    "        conv1_new = F.dropout(conv1_new.view(conv1_new.size(0), -1), p=0.3, training=self.training) \n",
    "        conv1_new = self.linear2(conv1_new)\n",
    "        y_hat = self.linear3(conv1_new)\n",
    "        return y_hat\n",
    "   \n",
    "    def fit(self,train_loader, val_loader,num_epochs=1000,patience=100):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.000015,weight_decay=5e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0 \n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs.squeeze(dim=1), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "            #     train_loss += loss.item()\n",
    "            # avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            #         val_loss += val_loss.item()\n",
    "            # avg_val_loss=val_loss / len(val_loader)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return\n",
    "                \n",
    "                \n",
    "    def test(self,test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110decbd-cfe6-4c03-934f-eab8843b749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000046_XSHE', '000753_XSHE', '000951_XSHE', '000998_XSHE', '002282_XSHE', '002679_XSHE', '002841_XSHE', '002882_XSHE', '300133_XSHE', '300174_XSHE', '300263_XSHE', '300343_XSHE', '300433_XSHE', '300540_XSHE', '600622_XSHG', '603053_XSHG', '603095_XSHG', '603359_XSHG']\n",
      ">>>>>>>>>>>>>>>>>>>>000046_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch [20/1000], Train Loss: 475836380511.7301, Val Loss: 443974706385.4072\n",
      "Epoch [40/1000], Train Loss: 235862698601.6849, Val Loss: 412908789462.9811\n",
      "Epoch [60/1000], Train Loss: 215123638986.0860, Val Loss: 428630938889.1474\n",
      "Epoch [80/1000], Train Loss: 507026163212.9767, Val Loss: 424892553006.0197\n",
      "Epoch [100/1000], Train Loss: 434164345730.8855, Val Loss: 421780609357.3370\n",
      "Epoch [120/1000], Train Loss: 145989909904.2234, Val Loss: 364586540858.0689\n",
      "Early stopping at epoch 130\n",
      "\n",
      "Fold number: 0\n",
      "[2.19116064e+03 2.14945492e+04 7.18558496e+04 1.14252392e+04\n",
      " 8.79791653e+02 6.12865218e+05 7.69913089e+04 3.55127885e+02\n",
      " 6.88816200e+05 4.79325908e+04 5.24102372e+04 1.54373810e+05\n",
      " 1.36582910e+04 1.52588716e+04 2.29535354e+05 1.87119242e+05\n",
      " 1.34261099e+04 4.20305496e+02 6.05755049e+05 8.31543801e+04\n",
      " 8.09797684e+04 5.58543970e+02 2.20953342e+05 1.90110575e+04\n",
      " 1.17418088e+03 6.15403332e+03 5.33883371e+03 2.77273706e+05\n",
      " 3.32603151e+03 7.77915578e+02 1.65549413e+04 9.24832409e+03\n",
      " 3.64691407e+05 3.56368064e+05 4.32250235e+04 4.38766451e+05\n",
      " 4.88166316e+04 3.21017030e+05 5.58046126e+05 6.93442287e+04\n",
      " 8.36587798e+03 2.08346997e+05 2.64871717e+05 5.89821522e+04\n",
      " 4.05241647e+02 1.10054856e+03 4.72436170e+02 2.61209692e+04\n",
      " 1.93044326e+03 5.48958451e+04 5.15944457e+03 2.76248264e+04\n",
      " 8.29516944e+05 1.20450512e+04 1.32636541e+04 1.98684547e+04\n",
      " 4.59472727e+04 1.04795250e+05 1.50756170e+04 6.30629428e+04\n",
      " 1.65711288e+05 3.03723825e+05 3.01658520e+05 3.78886865e+04\n",
      " 3.95475704e+05 2.95368821e+02 2.16254648e+06 3.77043221e+04\n",
      " 9.91076219e+04 2.39679675e+04 2.78744240e+04 1.74251122e+05\n",
      " 3.21788625e+05 2.16408535e+05 6.60774290e+04 1.07546685e+05\n",
      " 6.23449917e+02 1.81282795e+05 5.14787503e+04 1.05652134e+05\n",
      " 5.06176237e+05 1.86038697e+04 1.13860291e+03 3.02815964e+05\n",
      " 3.07247434e+04 3.43678538e+05 3.01270818e+05 1.16444882e+04\n",
      " 1.21552664e+03 5.46966756e+04 4.86650570e+02 6.16148588e+02\n",
      " 1.18756683e+04 5.18488673e+04 6.53888037e+03 7.26067940e+04\n",
      " 5.03585337e+04 2.85344915e+04 1.78685775e+04 2.71857291e+03\n",
      " 7.83433590e+05 2.34876269e+04 8.04064831e+05 2.38103647e+05\n",
      " 1.49024593e+04 1.41171478e+05 1.30463945e+05 3.97061039e+05\n",
      " 3.72619856e+04 3.56005545e+05 3.03685228e+03 8.76079857e+05\n",
      " 3.10379321e+04 2.81421913e+03 2.23807583e+05 3.42103774e+04\n",
      " 5.24435558e+04 6.71472006e+05 3.57624453e+04 1.03531116e+06\n",
      " 1.80066257e+05 1.94249031e+04 1.43296811e+05 3.21278492e+05\n",
      " 3.47848408e+04 3.97645564e+05 2.17997323e+04 2.10019714e+04\n",
      " 1.81311427e+04 1.21513869e+04 5.74034726e+05 4.00889436e+04\n",
      " 8.60230026e+04 6.40690275e+04 8.64163875e+05 2.96723526e+02\n",
      " 2.47945765e+04 2.39581815e+06 7.70833717e+05 1.70758624e+04\n",
      " 2.69906446e+05 2.87480561e+03 3.00720704e+04 1.75119552e+03\n",
      " 5.04279935e+04 5.11738180e+04 3.73310301e+04 3.54068404e+05\n",
      " 2.44608080e+05 6.58199780e+04 1.63818306e+05 1.84619933e+05\n",
      " 6.60184389e+04 8.78855218e+04 4.12841606e+04 6.12019239e+02\n",
      " 5.41933682e+02 4.21306622e+05 1.86691728e+05 8.65584585e+03\n",
      " 3.50763819e+04 2.86175339e+05 1.55206398e+04 4.00853362e+04\n",
      " 6.76490796e+04 7.13899826e+05 1.82538816e+05 1.24824020e+05\n",
      " 5.58774387e+02 1.13131549e+06 3.10976128e+04 7.04396194e+03\n",
      " 2.02535818e+05 1.83269765e+05 4.12087740e+02 1.06013726e+04\n",
      " 7.91774878e+04 6.76235363e+04 1.44087912e+05 2.02507744e+06\n",
      " 9.39865801e+04 4.44649029e+05 4.24882173e+05 1.65476142e+05\n",
      " 3.49004851e+04 1.97848762e+05 2.34410094e+05 2.77916678e+04\n",
      " 1.49262287e+05 2.17205618e+04 3.16721271e+05 1.75373495e+05\n",
      " 4.63259034e+05 2.32082323e+03 1.64122651e+05 6.05262648e+04\n",
      " 5.21905598e+03 8.29287397e+03 2.31237378e+05 3.65814797e+03\n",
      " 4.93749092e+04 2.30170519e+03 5.24727970e+03 2.37762313e+05\n",
      " 3.68131981e+05 3.98208545e+04 9.05632799e+05 4.71566522e+05\n",
      " 1.70631738e+03 4.37293442e+04 1.27132497e+03 7.67761402e+05\n",
      " 1.46975767e+05 1.90384179e+05 1.05443263e+04 2.28878293e+02\n",
      " 2.80808886e+04 1.01679187e+05 6.24104816e+03 9.35528263e+03\n",
      " 3.83731050e+04 1.70934554e+05 3.36411604e+05 2.78324688e+05\n",
      " 3.35485622e+05 3.15948795e+04 7.50789798e+04 4.92996780e+03\n",
      " 7.63120455e+05 1.20115845e+04 2.72507072e+02 3.13889436e+04\n",
      " 8.08710821e+04 1.39149431e+05 5.64062933e+05 5.05455456e+05\n",
      " 1.08740934e+05 2.64374077e+03 8.19092414e+02 6.37810277e+02\n",
      " 2.42134279e+05 1.42849620e+04 1.54584100e+05 2.90390959e+05\n",
      " 8.25518215e+03 2.88435836e+04 4.05205110e+05 5.29444817e+04\n",
      " 5.81796539e+04 1.63050453e+05 5.84385507e+03 2.19979750e+04\n",
      " 1.49350486e+05 3.21950415e+02 1.54611965e+05 1.55324771e+05\n",
      " 8.42522811e+05 2.74550265e+05 1.44716991e+05 2.48165832e+02\n",
      " 1.26022355e+04 3.94207670e+04 3.05914470e+04 2.76042963e+05\n",
      " 5.77439698e+04 2.44547336e+05 2.91644513e+05 7.27992552e+02\n",
      " 3.70269547e+02 1.56868865e+04 5.01117887e+02 3.44737114e+04\n",
      " 3.98656311e+04 1.98651863e+05 5.47860449e+04 3.36789705e+04\n",
      " 2.11282209e+05 6.51921637e+03 2.62491606e+05 3.22114920e+04\n",
      " 2.41742249e+04 2.32072108e+03 1.63559956e+05 3.65997441e+04\n",
      " 1.62038126e+03 2.13460320e+05 5.17325894e+02 1.18570791e+04\n",
      " 1.81496483e+03 5.98399875e+02 1.80733933e+04 3.62398236e+04\n",
      " 7.91551742e+04 1.89283278e+04 4.14254881e+04 5.17746036e+05\n",
      " 1.09289885e+05 5.76410831e+05 4.47482682e+05 7.07599473e+03\n",
      " 4.62152414e+02 5.66720637e+04 2.79060938e+05 7.55456545e+02\n",
      " 1.45199160e+05 4.58841879e+04 6.77867370e+05 3.88326713e+04\n",
      " 2.84129527e+05 7.58408533e+04 2.20840318e+04 2.61741460e+05\n",
      " 1.25090662e+05 5.48219987e+05 1.86045710e+04 2.13274042e+04\n",
      " 4.10291173e+05 3.47821808e+04 1.71638751e+05 3.12642058e+04\n",
      " 3.88615791e+04 1.24446569e+03 1.59861590e+04 6.50606893e+04\n",
      " 3.82918260e+04 1.16706571e+03 2.46078901e+02 7.19000045e+05\n",
      " 9.56231893e+03 4.81527406e+05 6.30032537e+05 3.94442825e+04\n",
      " 3.93511229e+04 1.94512184e+04 5.15124051e+05 3.10453573e+04\n",
      " 1.69422361e+05 4.72631549e+02 4.84628151e+02 5.62624298e+04\n",
      " 4.62206890e+05 3.49604205e+02 8.60798949e+03 6.03722795e+05\n",
      " 3.71553916e+02 6.11360762e+02 3.22290427e+05 2.29824774e+06\n",
      " 2.57348508e+05 5.14718735e+05 5.52194135e+04 2.62808061e+04\n",
      " 3.33985819e+04 5.94418321e+04 1.08034594e+05 4.70336440e+05\n",
      " 1.68103285e+03 1.07698816e+03 4.46355242e+03 2.18080281e+05]\n",
      "[0.8070348016276856]\n",
      "MAPE =  0.8070348016276856\n",
      "Epoch [20/1000], Train Loss: 410271081205.2051, Val Loss: 1327079817819.3501\n",
      "Epoch [40/1000], Train Loss: 268777655292.5801, Val Loss: 1250667069055.1816\n",
      "Epoch [60/1000], Train Loss: 398263755430.6934, Val Loss: 1429676919092.9866\n",
      "Epoch [80/1000], Train Loss: 261323270531.0754, Val Loss: 1381571124836.9333\n",
      "Epoch [100/1000], Train Loss: 548698942301.4158, Val Loss: 1371156951311.4370\n",
      "Epoch [120/1000], Train Loss: 149639070492.0868, Val Loss: 1355152781262.5815\n",
      "Epoch [140/1000], Train Loss: 406355977898.9443, Val Loss: 1366882189604.1558\n",
      "Early stopping at epoch 152\n",
      "\n",
      "Fold number: 1\n",
      "[2.13255880e+03 2.09732009e+04 6.98140080e+04 1.15634723e+04\n",
      " 8.62707780e+02 3.75873090e+05 7.51283987e+04 3.58441462e+02\n",
      " 6.86281320e+05 4.82091365e+04 5.27854124e+04 1.55782103e+05\n",
      " 1.38252068e+04 1.73877744e+05 2.34106187e+05 1.82259446e+05\n",
      " 1.36005797e+04 4.09802821e+02 6.13853164e+05 8.08818955e+04\n",
      " 1.02665319e+05 5.43610860e+02 6.03837915e+04 2.74316592e+05\n",
      " 1.14173624e+03 5.99511149e+03 5.62742734e+03 4.77979907e+05\n",
      " 3.35578095e+03 7.92850269e+02 1.67098002e+04 2.84642285e+05\n",
      " 3.68069199e+05 3.43487529e+05 4.35047870e+04 4.37412742e+05\n",
      " 4.79821285e+04 3.21694804e+05 5.39645430e+05 6.82004343e+04\n",
      " 8.48997740e+03 3.40401736e+05 2.58831903e+05 5.73948227e+04\n",
      " 4.10160456e+02 1.11879205e+03 4.82297466e+02 2.59681155e+04\n",
      " 1.88554004e+03 5.57282520e+04 5.08979421e+03 2.72225972e+04\n",
      " 8.38860722e+05 1.22658632e+04 1.33420625e+04 1.93234337e+04\n",
      " 4.48997539e+04 1.02852571e+05 1.53006995e+04 6.24338955e+04\n",
      " 1.61614386e+05 3.07431156e+05 2.95976061e+05 3.68830103e+04\n",
      " 3.66667309e+05 2.96844570e+02 2.19709256e+06 4.88440262e+04\n",
      " 9.76910044e+04 2.39672464e+04 2.82218591e+04 1.75601434e+05\n",
      " 5.28946303e+04 2.20314018e+05 6.58844135e+04 1.08706645e+05\n",
      " 6.33672344e+02 2.97376576e+05 5.01173720e+04 1.07549269e+05\n",
      " 5.11338206e+05 1.89205285e+04 1.14721806e+03 3.03199141e+05\n",
      " 3.01759919e+04 3.48254476e+05 2.96357650e+05 1.13670307e+04\n",
      " 1.23448198e+03 5.55928956e+04 4.88679423e+02 6.23019564e+02\n",
      " 1.19532879e+04 5.15920979e+02 6.43968223e+03 7.37986613e+04\n",
      " 4.95233432e+04 2.87456868e+04 1.80287927e+04 4.09323907e+04\n",
      " 7.82684706e+05 2.39631947e+04 7.96322784e+05 2.98463824e+05\n",
      " 1.50140537e+04 1.43091832e+05 1.32737646e+05 4.02707342e+05\n",
      " 3.76729112e+04 3.64810187e+05 2.98399423e+03 8.90245975e+05\n",
      " 3.16319408e+04 2.86991725e+03 2.26581062e+05 3.36439445e+04\n",
      " 3.11798423e+04 6.82758708e+05 3.06869270e+04 1.04098701e+06\n",
      " 3.53208405e+05 1.97010218e+04 1.43782393e+05 3.11508337e+05\n",
      " 1.83116147e+05 3.88629607e+05 2.01737605e+05 2.10991895e+04\n",
      " 1.82528369e+04 1.22150163e+04 5.81118548e+05 4.00646828e+04\n",
      " 8.82649799e+04 6.46299804e+04 8.79859867e+05 2.99905506e+02\n",
      " 2.50343382e+04 2.43127651e+06 7.45357291e+05 1.70197870e+04\n",
      " 2.75178380e+05 2.81620809e+03 2.92450613e+04 1.76647451e+03\n",
      " 4.91633609e+04 4.96227300e+04 3.78785860e+04 3.46293728e+05\n",
      " 2.47790054e+05 6.52010052e+04 1.64893843e+05 1.80820394e+05\n",
      " 6.58240040e+04 8.89515216e+04 4.18251176e+04 6.25374030e+02\n",
      " 5.45087475e+02 5.36931587e+04 1.88749293e+05 8.77398971e+03\n",
      " 2.67285624e+04 3.33046357e+04 1.58114246e+04 3.97627465e+04\n",
      " 6.87146418e+04 1.22165965e+05 1.78916627e+05 1.26380461e+05\n",
      " 5.63694107e+02 1.09767220e+06 3.79507877e+04 6.86433816e+03\n",
      " 2.03798904e+05 1.80342482e+05 4.20811374e+02 1.06730892e+04\n",
      " 8.08991339e+04 6.89390370e+04 1.45246305e+05 2.04421207e+06\n",
      " 9.22415518e+04 4.47323965e+05 4.28344882e+05 1.66797492e+05\n",
      " 3.43788018e+04 1.98629418e+05 2.35531293e+05 2.82580719e+04\n",
      " 8.08142707e+03 2.11993858e+04 3.19315724e+05 1.71650519e+05\n",
      " 4.49101743e+05 2.29644902e+03 2.32843260e+04 6.11693416e+04\n",
      " 5.21439923e+03 8.44238039e+03 2.35126821e+05 3.71807497e+03\n",
      " 4.92824565e+04 2.31388686e+03 5.10179316e+03 2.42350670e+05\n",
      " 3.57245633e+05 4.03317142e+04 9.21629970e+05 4.77719375e+05\n",
      " 1.73356816e+03 4.25241535e+04 1.28099932e+03 7.75953850e+05\n",
      " 1.44008159e+05 1.91913524e+05 1.85877326e+04 2.30086129e+02\n",
      " 2.83146713e+04 1.00355274e+05 6.28557782e+03 9.45633824e+03\n",
      " 3.73402084e+04 1.66679059e+05 3.42793348e+05 2.80257814e+05\n",
      " 4.41068134e+04 3.08162347e+04 7.63891485e+04 5.00615122e+03\n",
      " 7.57808862e+05 1.21584634e+04 2.73680106e+02 4.28487206e+04\n",
      " 8.26738030e+04 1.41558827e+05 1.24030551e+04 5.11406293e+05\n",
      " 1.06280107e+05 2.66715945e+03 8.26895848e+02 6.20113590e+02\n",
      " 1.90677055e+04 6.99344955e+04 1.56337133e+05 2.84819486e+05\n",
      " 8.31789652e+03 1.76604583e+03 4.12989734e+05 1.82612435e+05\n",
      " 5.86068226e+04 1.57892923e+05 5.89613420e+03 2.17199147e+04\n",
      " 1.48335311e+05 3.27111562e+02 1.52057786e+05 1.56269417e+05\n",
      " 8.31295057e+05 1.26104196e+05 1.45802528e+05 2.53592156e+02\n",
      " 1.26853681e+04 8.45607124e+02 1.11583306e+04 2.67982510e+05\n",
      " 5.84794500e+04 6.01746421e+04 2.84472833e+05 7.43265065e+02\n",
      " 3.74556864e+02 1.57532591e+04 5.04062046e+02 3.48791457e+04\n",
      " 4.00544463e+04 1.93557372e+05 5.57449448e+04 2.21068159e+04\n",
      " 2.13991002e+05 6.59560512e+03 2.55814792e+05 3.25939493e+04\n",
      " 2.36388301e+04 2.33655937e+03 1.60774459e+05 3.62391931e+04\n",
      " 1.57874751e+03 1.34261134e+05 5.25112009e+02 1.21060103e+04\n",
      " 1.76874496e+03 6.11550764e+02 1.76202541e+04 3.53090560e+04\n",
      " 1.71665138e+05 1.88617375e+04 2.57799201e+04 5.30272599e+05\n",
      " 1.10838267e+05 5.82656304e+05 2.14333860e+04 7.15000829e+03\n",
      " 4.71855168e+02 5.73456164e+04 2.74463933e+05 7.62003066e+02\n",
      " 1.41329992e+05 4.53238156e+04 6.84540556e+05 3.95413146e+04\n",
      " 2.93981617e+05 7.43930037e+04 2.16204658e+04 2.66380110e+05\n",
      " 1.17540801e+05 5.55769618e+05 1.87917251e+04 2.07939123e+04\n",
      " 9.06029925e+03 3.37941070e+04 1.74187676e+05 3.15550894e+04\n",
      " 3.81878911e+04 1.26828407e+03 1.61121348e+04 6.37029061e+04\n",
      " 8.98618276e+02 1.17412854e+03 2.41029098e+02 7.01795310e+05\n",
      " 9.31913301e+03 4.83863757e+05 6.33568668e+05 4.00884109e+04\n",
      " 3.84817490e+04 1.64194571e+03 6.20450970e+04 3.08477859e+04\n",
      " 1.71884466e+05 4.78928959e+02 4.91045208e+02 5.49699454e+04\n",
      " 4.52072378e+05 9.59971442e+03 8.76960808e+03 6.08318605e+05\n",
      " 5.74644891e+04 5.94344590e+02 3.04854823e+05 2.33191724e+06\n",
      " 2.58946241e+05 5.00782931e+05 5.39949754e+04 2.67234609e+04\n",
      " 3.36951215e+04 6.05987433e+04 1.09900035e+05 4.77456876e+05\n",
      " 1.70134230e+03 1.08890178e+03 4.40862072e+03 2.19950957e+05]\n",
      "[0.8138826203560301]\n",
      "MAPE =  0.8138826203560301\n",
      "Epoch [20/1000], Train Loss: 401710103602.8629, Val Loss: 315874025823.4235\n",
      "Epoch [40/1000], Train Loss: 1596531463697.2134, Val Loss: 291337109251.0801\n",
      "Epoch [60/1000], Train Loss: 438592922112.3154, Val Loss: 283861784979.0809\n",
      "Epoch [80/1000], Train Loss: 352143975156.6705, Val Loss: 281724126098.2158\n",
      "Epoch [100/1000], Train Loss: 696399172936.7363, Val Loss: 262531695602.6321\n",
      "Epoch [120/1000], Train Loss: 141439479238.8891, Val Loss: 262030830775.7298\n",
      "Epoch [140/1000], Train Loss: 388441583640.5432, Val Loss: 262006864661.4496\n",
      "Epoch [160/1000], Train Loss: 358173548360.8099, Val Loss: 240377940713.9748\n",
      "Epoch [180/1000], Train Loss: 378819621146.5143, Val Loss: 239892872849.8258\n",
      "Epoch [200/1000], Train Loss: 460962311845.8586, Val Loss: 240198573394.6115\n",
      "Epoch [220/1000], Train Loss: 287048386831.6602, Val Loss: 239702753444.2713\n",
      "Epoch [240/1000], Train Loss: 971170045450.8544, Val Loss: 243971884859.3124\n",
      "Epoch [260/1000], Train Loss: 1097703882596.5177, Val Loss: 243138012742.1305\n",
      "Epoch [280/1000], Train Loss: 271457450600.1424, Val Loss: 244259973261.9992\n",
      "Early stopping at epoch 290\n",
      "\n",
      "Fold number: 2\n",
      "[2.05070427e+03 1.91637938e+04 6.17896432e+04 1.06562100e+04\n",
      " 8.11738966e+02 5.50513647e+05 4.21602961e+03 3.52020190e+02\n",
      " 4.77194331e+05 4.63705487e+04 1.69450277e+03 1.42157865e+05\n",
      " 1.27306482e+04 1.67212207e+05 2.27900707e+05 1.68226962e+05\n",
      " 1.25418816e+04 3.76262437e+02 2.64490979e+04 7.28344756e+04\n",
      " 4.58524669e+05 4.90534143e+02 5.54615908e+04 2.43736271e+05\n",
      " 1.09830526e+03 5.77255968e+03 5.45188719e+03 4.22835709e+05\n",
      " 3.26949644e+03 7.54845089e+02 2.30778165e+04 2.91933363e+05\n",
      " 3.27654267e+05 2.96282592e+05 4.22037225e+04 4.45891599e+05\n",
      " 4.45465993e+04 5.51997901e+05 4.72007250e+05 6.37800242e+04\n",
      " 7.87013036e+03 2.02263356e+05 3.22985949e+05 5.23703074e+04\n",
      " 3.77076791e+02 1.05386508e+03 4.66576023e+02 2.47054496e+04\n",
      " 1.81618202e+03 5.17977495e+04 4.90290840e+03 9.94112131e+03\n",
      " 7.63458183e+05 1.17351417e+04 1.29044460e+04 1.69587496e+04\n",
      " 2.86815463e+05 9.59523927e+04 1.42364376e+04 5.92184877e+04\n",
      " 1.48480260e+05 1.84162567e+05 2.76110439e+05 3.35310129e+04\n",
      " 1.38792160e+05 2.86916113e+02 2.09561155e+06 3.22978028e+04\n",
      " 1.20119199e+05 2.31194007e+04 9.96802327e+04 1.69828314e+05\n",
      " 3.34697201e+05 7.85525992e+04 6.33835097e+04 9.91479483e+04\n",
      " 5.89534844e+02 2.87239226e+05 4.53214291e+04 1.02609355e+05\n",
      " 4.73346744e+05 1.79495127e+04 5.82547164e+02 2.93382210e+05\n",
      " 1.85934150e+04 3.33680299e+05 2.74081812e+05 2.70477706e+04\n",
      " 1.14852669e+03 5.43109951e+04 4.71675249e+02 5.70825554e+02\n",
      " 1.15788399e+04 6.76112911e+04 6.20570795e+03 6.88388730e+04\n",
      " 7.94165361e+03 2.58028925e+04 1.76053536e+04 3.70175485e+04\n",
      " 7.56496864e+05 2.30017770e+04 7.43221547e+05 2.89250220e+05\n",
      " 8.23106736e+04 1.35446577e+05 1.26058366e+05 1.89501174e+05\n",
      " 3.41966519e+04 7.36581057e+05 5.76920238e+04 8.46196980e+05\n",
      " 3.54081358e+04 2.79625940e+03 1.03124045e+05 3.11760017e+04\n",
      " 3.70617546e+05 6.50841729e+05 2.83430043e+04 1.00639606e+06\n",
      " 3.41503053e+05 8.55145440e+04 1.37989048e+05 2.94263994e+05\n",
      " 1.74444306e+05 3.44663690e+05 1.96647133e+05 2.24521346e+05\n",
      " 1.76856922e+04 1.17975047e+04 5.35731483e+05 3.87753548e+04\n",
      " 9.45858227e+04 3.95108231e+04 4.30192419e+05 2.74252567e+02\n",
      " 1.54533899e+03 2.27626047e+06 3.36541932e+05 8.96843161e+03\n",
      " 2.64032151e+05 2.65112981e+03 2.77577556e+04 7.98823083e+05\n",
      " 2.53566716e+04 4.42622128e+04 1.62957353e+04 3.32590207e+05\n",
      " 2.28941133e+05 6.27037272e+04 1.47529544e+05 1.73595122e+05\n",
      " 6.35330281e+04 8.68242470e+04 4.03580340e+04 6.10118155e+02\n",
      " 5.27416840e+02 4.76323579e+04 4.15637640e+03 2.06792362e+05\n",
      " 1.67622949e+04 3.11060387e+04 1.49044977e+04 3.83088876e+04\n",
      " 6.60690749e+04 1.10822503e+05 1.67169568e+05 1.13116450e+05\n",
      " 2.54092668e+02 1.40794346e+05 2.86944763e+04 6.26537358e+03\n",
      " 1.82429370e+05 4.03165455e+04 4.04581519e+02 1.03552119e+04\n",
      " 7.83583611e+04 4.06033512e+04 1.00623121e+04 1.82796167e+06\n",
      " 5.40295347e+03 4.33227599e+05 3.72158113e+05 1.49773246e+05\n",
      " 8.23403976e+03 1.89664433e+05 2.27904961e+05 2.30623569e+04\n",
      " 7.79456128e+03 1.91208139e+04 2.03546557e+05 1.62572812e+05\n",
      " 3.66203728e+05 2.21278180e+03 2.07277137e+04 4.76012509e+04\n",
      " 5.02835914e+03 8.02365500e+03 2.23288222e+05 3.45857028e+03\n",
      " 4.72139899e+04 2.23518383e+03 6.25059739e+02 2.33631501e+05\n",
      " 3.01884805e+05 3.88471203e+04 8.91258998e+05 4.66161556e+05\n",
      " 1.61748180e+03 2.20181448e+05 1.14563389e+03 6.95572854e+05\n",
      " 1.34499038e+05 1.74551004e+05 9.87572170e+03 2.22561848e+02\n",
      " 2.75801503e+04 9.25804040e+04 6.09615843e+03 8.64520031e+03\n",
      " 3.41535208e+04 1.60171506e+05 3.27527576e+05 3.59012649e+04\n",
      " 3.95458738e+04 2.85295159e+04 7.23084901e+04 4.65400366e+03\n",
      " 7.18927797e+05 1.11858037e+04 2.64390943e+02 2.97601987e+04\n",
      " 8.90180743e+04 1.35082688e+05 1.46589669e+05 4.70529824e+05\n",
      " 9.61348868e+04 2.39602023e+03 8.07849653e+02 5.92776518e+02\n",
      " 2.56329265e+05 4.98252097e+04 1.43267845e+05 7.36212720e+05\n",
      " 7.46009742e+03 1.70102885e+03 4.02625105e+05 2.85346817e+05\n",
      " 5.68195209e+04 1.34119035e+05 5.75374007e+03 2.09314009e+04\n",
      " 1.39595722e+05 3.04335939e+02 1.39670522e+05 8.89303274e+03\n",
      " 3.83370828e+05 1.20245617e+05 1.40980329e+05 1.63975412e+05\n",
      " 1.22880790e+04 7.84485525e+02 1.02585831e+04 2.39040074e+05\n",
      " 5.01374788e+04 3.87062194e+05 2.56132675e+05 7.17672088e+02\n",
      " 3.43680609e+02 1.39218617e+04 4.88306717e+02 3.41500646e+04\n",
      " 4.71707261e+05 1.75505731e+05 5.28528617e+04 3.12220461e+04\n",
      " 1.79040530e+05 6.46809803e+03 3.56182147e+05 1.99556042e+04\n",
      " 2.19720730e+04 3.84808390e+05 1.47481237e+05 2.63810491e+05\n",
      " 1.43444329e+03 1.29668373e+05 4.87803814e+02 1.49146464e+05\n",
      " 1.60271255e+03 5.95837375e+02 1.60779299e+04 3.29635553e+04\n",
      " 1.66087534e+05 1.80745276e+04 2.50892070e+04 2.35244552e+05\n",
      " 4.62164383e+03 3.48725620e+05 3.85063050e+04 7.02014312e+03\n",
      " 4.57090132e+02 3.00345972e+04 1.20330089e+06 7.41661997e+02\n",
      " 2.28702919e+04 4.36946318e+04 6.18049277e+05 3.74981695e+04\n",
      " 2.84372704e+05 6.91391925e+04 2.40569824e+05 2.56678243e+05\n",
      " 1.78172674e+05 5.16005506e+05 1.37610083e+05 1.99657483e+04\n",
      " 7.96086070e+03 2.98656076e+04 1.63717577e+05 2.33959206e+04\n",
      " 3.48373525e+04 1.21751759e+03 1.49615581e+04 5.81806460e+04\n",
      " 8.65702884e+02 1.13610329e+03 2.26042648e+02 6.40391932e+05\n",
      " 8.91871294e+03 3.68874733e+04 5.53593448e+05 3.83715850e+04\n",
      " 1.44446193e+04 9.59572651e+02 2.36733867e+04 1.45811712e+04\n",
      " 1.16957132e+05 4.41791635e+02 4.52915996e+02 5.02797702e+04\n",
      " 4.24393797e+05 2.18161715e+05 8.32748099e+03 5.90705142e+05\n",
      " 2.04313964e+04 5.35903400e+02 2.74517348e+05 2.18592539e+06\n",
      " 1.58010396e+05 1.67188928e+05 5.19064251e+04 9.04404335e+04\n",
      " 3.28891099e+04 5.81510146e+04 1.74535885e+05 3.18452681e+05\n",
      " 9.28982634e+04 9.98082063e+02 2.10030595e+05 2.93870782e+05]\n",
      "[0.8107344069755698]\n",
      "MAPE =  0.8107344069755698\n",
      "Epoch [20/1000], Train Loss: 700431350419.7426, Val Loss: 319533991950.9412\n",
      "Epoch [40/1000], Train Loss: 1556578059218.3357, Val Loss: 290978924616.2731\n",
      "Epoch [60/1000], Train Loss: 783230696692.4332, Val Loss: 283595229729.6227\n",
      "Epoch [80/1000], Train Loss: 394706396839.9962, Val Loss: 282028150250.9174\n",
      "Epoch [100/1000], Train Loss: 677203752540.9292, Val Loss: 276209647515.9136\n",
      "Epoch [120/1000], Train Loss: 336018071342.9948, Val Loss: 274433120175.3078\n",
      "Epoch [140/1000], Train Loss: 891949982387.9363, Val Loss: 246756280411.4309\n",
      "Epoch [160/1000], Train Loss: 232254713614.6685, Val Loss: 240646472300.8384\n",
      "Epoch [180/1000], Train Loss: 437606862714.6129, Val Loss: 242040013083.5082\n",
      "Epoch [200/1000], Train Loss: 328440583180.0593, Val Loss: 241797441076.6487\n",
      "Epoch [220/1000], Train Loss: 314108764131.8455, Val Loss: 241977813885.4874\n",
      "Epoch [240/1000], Train Loss: 844874812949.1051, Val Loss: 245262325181.8194\n",
      "Epoch [260/1000], Train Loss: 388837563057.6275, Val Loss: 246541193757.6118\n",
      "Early stopping at epoch 261\n",
      "\n",
      "Fold number: 3\n",
      "[2.02117893e+03 1.93045715e+04 6.40517424e+04 1.02295473e+04\n",
      " 8.01717271e+02 5.49873419e+05 5.29238781e+04 3.39405685e+02\n",
      " 4.68440460e+05 4.52870055e+04 1.62386369e+03 1.37037347e+05\n",
      " 1.22197108e+04 1.60605415e+05 2.19947245e+05 1.69003254e+05\n",
      " 1.20601776e+04 3.75625541e+02 5.75110788e+05 7.42304755e+04\n",
      " 9.57060341e+04 4.99382605e+02 2.08706488e+05 2.35035148e+05\n",
      " 1.08286569e+03 5.68829999e+03 5.28444775e+03 2.46497065e+05\n",
      " 3.13830885e+03 7.28520514e+02 2.21965765e+04 2.66668844e+05\n",
      " 3.19826882e+05 3.10535252e+05 4.09446353e+04 4.38325333e+05\n",
      " 4.43898761e+04 5.36305024e+05 4.90651586e+05 6.30371659e+04\n",
      " 7.58381523e+03 3.21959345e+05 3.03762628e+05 5.28745221e+04\n",
      " 3.62444340e+02 1.01401128e+03 4.48942211e+02 2.42224016e+04\n",
      " 1.78834934e+03 4.99852055e+04 4.81931196e+03 9.77247250e+03\n",
      " 7.34290612e+05 1.12779634e+04 1.25730227e+04 1.77538528e+04\n",
      " 2.84202609e+05 9.49991050e+04 1.37128950e+04 5.81709587e+04\n",
      " 1.48511784e+05 2.80262700e+05 2.72982199e+05 3.38935699e+04\n",
      " 3.43786953e+05 2.80198180e+02 1.99709901e+06 3.33263834e+04\n",
      " 1.16525291e+05 2.27166103e+04 2.33972021e+04 1.64854701e+05\n",
      " 4.14475535e+04 2.02259219e+05 2.54884366e+05 9.53089461e+04\n",
      " 5.70387135e+02 2.79825997e+05 4.60732527e+04 9.85269057e+04\n",
      " 4.72539681e+05 1.72381123e+04 1.23211607e+04 2.85892938e+05\n",
      " 1.87262134e+04 3.18297418e+05 2.74790305e+05 2.69225236e+04\n",
      " 1.10925404e+03 5.19343314e+04 4.62747801e+02 5.48755151e+02\n",
      " 1.12364688e+04 3.95542028e+05 6.10393015e+03 6.66511382e+04\n",
      " 4.61502220e+04 2.48111681e+04 1.68875158e+04 3.79678710e+04\n",
      " 7.36909603e+05 2.21809573e+04 6.92917100e+05 3.25149904e+05\n",
      " 1.72391592e+05 1.29419964e+05 1.21736335e+05 1.82475435e+05\n",
      " 3.28937194e+04 3.40120762e+05 5.59491278e+04 2.80811079e+05\n",
      " 2.95512799e+04 2.67207603e+03 2.12564746e+05 3.11530778e+04\n",
      " 3.58702105e+05 6.23613388e+05 1.21036225e+05 9.79772905e+05\n",
      " 3.31783120e+05 2.37221208e+04 1.34826717e+05 3.29133799e+04\n",
      " 1.71664395e+05 3.44598526e+05 1.89874745e+05 2.21258913e+05\n",
      " 1.71535035e+04 1.15345109e+04 5.14564379e+05 3.78635693e+04\n",
      " 8.78711115e+04 3.84654956e+04 4.14822059e+05 2.64591868e+02\n",
      " 1.51247549e+03 2.18862215e+06 6.76386262e+05 8.63363615e+03\n",
      " 2.54157030e+05 2.61906880e+03 2.74671433e+04 7.64243426e+05\n",
      " 2.52759272e+04 4.53404519e+04 3.38360742e+04 3.27319315e+05\n",
      " 2.19979379e+05 6.11798117e+04 1.42553568e+05 1.71010138e+05\n",
      " 6.21783376e+04 8.33468321e+04 3.86410252e+04 5.86740921e+02\n",
      " 5.13206011e+02 4.16775167e+04 4.00384217e+03 4.14929059e+05\n",
      " 1.61113862e+04 2.60440176e+05 1.44400518e+04 3.75993351e+04\n",
      " 6.34469782e+04 1.13157735e+05 1.65361830e+05 1.11966502e+05\n",
      " 2.43648533e+02 1.00248319e+06 2.83622133e+04 6.30668625e+03\n",
      " 1.75385727e+05 1.65553830e+05 3.90917131e+02 1.00300378e+04\n",
      " 1.89825311e+04 3.90195904e+04 9.68465508e+03 1.78335752e+06\n",
      " 8.64293792e+04 4.21442545e+05 1.17910921e+05 1.44508526e+05\n",
      " 8.00440366e+03 1.79727345e+05 2.20715665e+05 2.56142261e+04\n",
      " 7.46486220e+03 1.96416056e+04 2.76556004e+05 1.60662067e+05\n",
      " 3.76955349e+05 2.17646965e+03 2.12513669e+04 5.38617384e+04\n",
      " 4.93665279e+03 7.72243227e+03 2.13664307e+05 3.32003717e+03\n",
      " 4.63691817e+04 2.18290262e+03 4.82521183e+03 2.24932391e+05\n",
      " 3.27003672e+05 3.72356544e+04 8.52704262e+05 4.46730700e+05\n",
      " 1.56133953e+03 2.19118681e+05 1.10713323e+03 6.76214292e+05\n",
      " 1.33075367e+05 1.67739688e+05 9.49092895e+03 2.17127937e+02\n",
      " 2.65108427e+04 9.31623216e+04 5.90049534e+03 8.34311966e+03\n",
      " 3.45660648e+04 1.57782636e+05 3.15485051e+05 3.48784593e+04\n",
      " 4.05937543e+04 2.87343663e+04 6.97883295e+04 4.49465143e+03\n",
      " 7.07195812e+05 1.07420888e+04 2.59070366e+02 2.92373775e+04\n",
      " 8.32368069e+04 1.30264241e+05 2.74893053e+05 4.50931897e+05\n",
      " 9.78201047e+04 2.31138734e+03 7.73776487e+02 5.85337900e+02\n",
      " 2.61739682e+05 1.34986352e+04 1.38165741e+05 7.27996128e+05\n",
      " 7.19528197e+03 1.67584491e+03 3.85468091e+05 2.63541458e+05\n",
      " 5.49755170e+04 1.43831947e+05 5.51446564e+03 2.05848758e+04\n",
      " 1.37564294e+05 2.93829006e+02 1.40593503e+05 1.47576651e+05\n",
      " 3.91218379e+05 1.18648511e+05 1.36969714e+05 1.57531059e+05\n",
      " 1.19345456e+04 7.56494058e+02 1.02441471e+04 2.45092668e+05\n",
      " 4.81665258e+04 3.82206386e+05 2.61009045e+05 6.91223030e+02\n",
      " 3.31123331e+02 1.34175848e+04 4.75023493e+02 3.28964507e+04\n",
      " 3.78324490e+04 1.78199624e+05 5.08452534e+04 2.06884779e+04\n",
      " 1.90103192e+05 6.17944718e+03 2.35054690e+05 3.02426679e+04\n",
      " 2.17827342e+04 3.77130268e+05 1.48516830e+05 1.52111817e+04\n",
      " 1.45278014e+03 1.24730130e+05 4.70387243e+02 1.12734228e+04\n",
      " 1.62803158e+03 5.73352958e+02 1.62105395e+04 3.28097409e+04\n",
      " 1.59177576e+05 1.76421334e+04 2.41019842e+04 2.20872912e+05\n",
      " 4.46537050e+03 3.35768591e+05 1.98243136e+04 6.73849643e+03\n",
      " 4.39684923e+02 2.88189514e+04 1.18377995e+06 7.12933618e+02\n",
      " 1.29966333e+05 4.28382296e+04 5.95968124e+05 3.61613503e+04\n",
      " 2.71746768e+05 6.85440293e+04 2.34036779e+05 2.43108432e+05\n",
      " 1.02295279e+05 4.96218808e+05 1.76786220e+04 1.96668452e+04\n",
      " 8.24462863e+03 3.09988834e+04 1.56934672e+05 2.95986212e+04\n",
      " 3.50820760e+04 1.16934062e+03 1.43986421e+04 5.88174055e+04\n",
      " 1.24524627e+04 1.10517695e+03 2.23548775e+02 6.46970119e+05\n",
      " 1.67720830e+04 4.09944587e+05 5.40780600e+05 3.69577834e+04\n",
      " 1.43190127e+04 1.53696366e+03 2.27764468e+04 5.50139018e+05\n",
      " 1.76541523e+04 4.26930574e+02 4.37321690e+02 5.07825908e+04\n",
      " 4.19955949e+05 8.98790752e+03 8.03834968e+03 5.70722777e+05\n",
      " 2.02460330e+04 5.45792379e+02 2.79456356e+05 2.09958867e+06\n",
      " 2.43858544e+05 1.90428764e+05 5.11381505e+04 2.42458310e+04\n",
      " 3.15452828e+04 5.62965935e+04 1.00418412e+05 4.27975819e+05\n",
      " 8.64397940e+04 9.58848977e+02 2.01515942e+05 2.05654203e+05]\n",
      "[0.8041819547317111]\n",
      "MAPE =  0.8041819547317111\n",
      "Epoch [20/1000], Train Loss: 560029566229.6721, Val Loss: 396113438975.5394\n",
      "Epoch [40/1000], Train Loss: 459183255454.5425, Val Loss: 383490535339.5602\n",
      "Epoch [60/1000], Train Loss: 241042301163.8522, Val Loss: 372393012282.7643\n",
      "Epoch [80/1000], Train Loss: 531303859304.3630, Val Loss: 385215676255.9648\n",
      "Epoch [100/1000], Train Loss: 497734479659.0452, Val Loss: 383302370411.7522\n",
      "Epoch [120/1000], Train Loss: 1091065368588.8909, Val Loss: 397091678363.6884\n",
      "Early stopping at epoch 134\n",
      "\n",
      "Fold number: 4\n",
      "[2.23618434e+03 2.18840679e+04 7.29739601e+04 1.14953274e+04\n",
      " 8.96287723e+02 6.24073608e+05 7.82012283e+04 3.57652238e+02\n",
      " 7.00795390e+05 4.83247232e+04 5.28674767e+04 1.54806994e+05\n",
      " 1.37453788e+04 1.53180310e+04 2.31117271e+05 1.90562529e+05\n",
      " 1.34948021e+04 4.28013692e+02 6.10612351e+05 8.46274831e+04\n",
      " 1.05050173e+05 5.68464372e+02 6.31848824e+04 2.76340584e+05\n",
      " 1.19814734e+03 6.28530243e+03 5.38450876e+03 2.82088360e+05\n",
      " 3.35615734e+03 7.79894367e+02 1.65196355e+04 9.32981401e+03\n",
      " 3.65757203e+05 3.62279508e+05 4.35550240e+04 4.48416512e+05\n",
      " 4.95848147e+04 3.24448768e+05 5.67363739e+05 7.04859245e+04\n",
      " 8.40293748e+03 2.11337036e+05 2.69867533e+05 6.00556482e+04\n",
      " 4.07550265e+02 1.10615716e+03 4.73834408e+02 2.64499954e+04\n",
      " 1.97119696e+03 5.49658315e+04 5.26722214e+03 2.82241761e+04\n",
      " 8.31171690e+05 1.20968300e+04 1.33816074e+04 2.02243154e+04\n",
      " 4.67414293e+04 1.06773024e+05 1.51198012e+04 6.39852497e+04\n",
      " 1.78959654e+05 3.05618001e+05 3.06895909e+05 3.85760675e+04\n",
      " 3.74965090e+05 2.97645564e+02 2.17632533e+06 5.12305882e+04\n",
      " 1.00745121e+05 2.45068832e+04 1.04279473e+05 1.75716810e+05\n",
      " 3.24859959e+05 2.17222949e+05 6.69486708e+04 1.07564314e+05\n",
      " 6.25303777e+02 1.82870529e+05 5.23971840e+04 1.06248791e+05\n",
      " 5.27275264e+05 1.86980322e+04 1.14847430e+03 3.06773909e+05\n",
      " 3.10573290e+04 3.45964390e+05 3.05973404e+05 1.18649080e+04\n",
      " 1.21663476e+03 5.52162309e+04 4.97082847e+02 6.19735060e+02\n",
      " 1.19717770e+04 5.22841145e+04 6.67897207e+03 7.27171731e+04\n",
      " 5.12186523e+04 2.86129054e+04 1.80179749e+04 2.76691398e+03\n",
      " 7.94161844e+05 2.35425519e+04 7.94966783e+05 2.40421163e+05\n",
      " 9.94599046e+04 1.41729894e+05 1.30828382e+05 3.99569178e+05\n",
      " 3.73628969e+04 1.07026967e+03 3.10165337e+03 8.82557474e+05\n",
      " 3.12442493e+04 2.83729944e+03 2.25478825e+05 3.47139011e+04\n",
      " 5.33506830e+04 6.75959388e+05 3.58874040e+04 1.04542287e+06\n",
      " 3.54145186e+05 1.95492344e+04 1.45772645e+05 3.26826738e+05\n",
      " 1.85129209e+05 4.04914254e+05 2.22863006e+04 2.45163114e+05\n",
      " 1.82796336e+04 1.22644957e+04 5.77493024e+05 4.08028187e+04\n",
      " 8.05118766e+04 6.49317410e+04 8.67471145e+05 2.97682809e+02\n",
      " 2.49450779e+04 1.99628318e+06 7.83514057e+05 1.72561704e+04\n",
      " 2.70800580e+05 2.92873441e+03 3.06461340e+04 1.76815471e+03\n",
      " 5.13576942e+04 5.20712571e+04 3.74788341e+04 3.60395532e+05\n",
      " 2.45958994e+05 6.68437477e+04 1.63754931e+05 1.88412020e+05\n",
      " 6.72651262e+04 8.85210393e+04 4.16026499e+04 6.16958412e+02\n",
      " 5.46237019e+02 4.28229053e+05 1.87691289e+05 8.71627593e+03\n",
      " 2.63726662e+04 3.29018632e+04 1.55392606e+04 4.08980744e+04\n",
      " 6.81113991e+04 1.26954017e+05 1.85818043e+05 1.25534793e+05\n",
      " 5.63831718e+02 1.15047021e+06 3.95996756e+04 7.17209943e+03\n",
      " 2.04438641e+05 1.86084455e+05 4.12545241e+02 1.06836466e+04\n",
      " 7.95533351e+04 6.78813174e+04 1.45365318e+05 2.06826220e+06\n",
      " 9.55819659e+04 4.48113442e+05 4.25103768e+05 1.65562913e+05\n",
      " 6.24180929e+04 1.99087095e+05 2.36754881e+05 2.78514289e+04\n",
      " 1.52274303e+05 2.20466642e+04 3.16292587e+05 1.78520395e+05\n",
      " 4.71245316e+05 2.37209018e+03 1.67073260e+05 6.08027904e+04\n",
      " 5.33353487e+03 8.32780326e+03 2.32308672e+05 3.68922877e+03\n",
      " 5.03632752e+04 2.32237794e+03 5.34855136e+03 2.38519655e+05\n",
      " 3.74537589e+05 4.01121691e+04 9.12037336e+05 4.75075993e+05\n",
      " 1.71320409e+03 4.45002743e+04 1.26800445e+03 7.69591059e+05\n",
      " 1.49600626e+05 1.91283109e+05 1.05986595e+04 2.30601622e+02\n",
      " 2.83169233e+04 1.03610083e+05 6.29284058e+03 9.38457042e+03\n",
      " 3.90836689e+04 1.74133979e+05 8.59573905e+05 2.80776352e+05\n",
      " 4.60086039e+04 3.21363316e+04 7.52040532e+04 4.93526832e+03\n",
      " 7.74416909e+05 1.20867994e+04 2.74757603e+02 3.17532197e+04\n",
      " 8.13626379e+04 1.39737625e+05 5.73764804e+05 5.09278992e+05\n",
      " 1.10659415e+05 2.64271308e+03 8.26403517e+02 6.50098917e+02\n",
      " 1.90806224e+04 1.45090077e+04 1.55107842e+05 2.95306418e+05\n",
      " 8.26843564e+03 2.94159796e+04 4.08146544e+05 5.33267513e+04\n",
      " 5.86953696e+04 1.65838381e+05 5.89355090e+03 2.24745502e+04\n",
      " 1.51290842e+05 3.22870187e+02 1.31129203e+05 1.56538073e+05\n",
      " 8.53370859e+05 2.80104046e+05 1.46011406e+05 2.48211462e+02\n",
      " 1.27094066e+04 3.92868149e+04 3.12259459e+04 2.80854428e+05\n",
      " 5.80731731e+04 2.48121919e+05 2.96273601e+05 7.29908393e+02\n",
      " 3.71737265e+02 1.58764076e+04 5.04728629e+02 3.47262564e+04\n",
      " 4.02704399e+04 2.02179165e+05 5.50406241e+04 3.44001342e+04\n",
      " 2.12666475e+05 6.58384276e+03 2.67178319e+05 3.24671095e+04\n",
      " 2.46230385e+04 2.13666863e+05 1.65959047e+05 3.71192291e+04\n",
      " 1.64940281e+03 2.13883090e+05 5.18480125e+02 1.18970468e+04\n",
      " 1.84737333e+03 6.02106777e+02 1.84025377e+04 3.69557691e+04\n",
      " 8.03886900e+04 1.91678249e+04 2.57902293e+04 5.17102983e+05\n",
      " 1.09447328e+05 5.78572901e+05 2.23567605e+04 7.13593083e+03\n",
      " 4.63545050e+02 5.70391416e+04 2.84686226e+05 7.62292715e+02\n",
      " 1.47813862e+05 4.66185476e+04 6.77923152e+05 3.89003962e+04\n",
      " 2.86467578e+05 7.71309665e+04 2.24991411e+04 2.62690151e+05\n",
      " 1.25193307e+05 5.51658285e+05 1.48080252e+05 2.17633513e+04\n",
      " 4.17096797e+05 3.53152996e+04 1.72840417e+05 3.15271859e+04\n",
      " 3.93966181e+04 1.25028807e+03 1.61210831e+04 6.60724903e+04\n",
      " 3.90163509e+04 1.17692510e+03 2.50703430e+02 7.31932990e+05\n",
      " 9.74501281e+03 4.89172577e+05 6.38762023e+05 3.96460218e+04\n",
      " 4.00900516e+04 1.96765161e+04 5.17128315e+05 3.14079806e+04\n",
      " 1.70884882e+05 4.73248699e+02 4.85566570e+02 5.72106811e+04\n",
      " 4.70807387e+05 9.60306947e+03 8.63075973e+03 6.09284003e+05\n",
      " 3.79166513e+02 6.22289206e+02 3.28008110e+05 2.32127863e+06\n",
      " 2.59655100e+05 5.23986657e+05 5.63854090e+04 2.63979888e+04\n",
      " 3.36798598e+04 5.95535072e+04 1.08653461e+05 4.72118607e+05\n",
      " 8.39041594e+04 1.08362366e+03 4.56091430e+03 2.20023884e+05]\n",
      "[0.8127945072148018]\n",
      "MAPE =  0.8127945072148018\n",
      "-\n",
      "mape score =  [[0.8070348016276856], [0.8138826203560301], [0.8107344069755698], [0.8041819547317111], [0.8127945072148018]]\n",
      ">>>>>>>>>>>>>>>>>>>>000753_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch [20/1000], Train Loss: 202876196087.0548, Val Loss: 11158555755.1529\n",
      "Epoch [40/1000], Train Loss: 218092936124.7767, Val Loss: 10588943915.0133\n",
      "Epoch [60/1000], Train Loss: 47034504887.6117, Val Loss: 26794747895.7180\n",
      "Epoch [80/1000], Train Loss: 343449827075.5744, Val Loss: 26828656122.3045\n",
      "Epoch [100/1000], Train Loss: 120580491206.3992, Val Loss: 29224949401.3344\n",
      "Epoch [120/1000], Train Loss: 108985663719.0629, Val Loss: 29530656521.0592\n",
      "Early stopping at epoch 138\n",
      "\n",
      "Fold number: 0\n",
      "[2.60730394e+05 6.86870996e+03 2.98156796e+04 1.04287096e+05\n",
      " 9.92097811e+03 8.22572472e+04 1.33405365e+04 9.32181127e+04\n",
      " 7.57975478e+04 1.71793510e+05 5.00574744e+04 2.18749547e+04\n",
      " 4.26476663e+04 1.30014743e+05 3.87149036e+05 2.16609409e+02\n",
      " 1.89524459e+05 1.31885183e+02 5.97643756e+02 5.29472734e+04\n",
      " 2.18433429e+03 1.96571236e+04 2.38952352e+02 1.81354639e+04\n",
      " 1.20548763e+04 7.14686800e+04 7.14127372e+03 1.13430433e+03\n",
      " 1.17512352e+02 9.42564709e+02 3.00227498e+05 1.46420701e+05\n",
      " 5.24056390e+02 7.07198779e+03 1.02541439e+04 2.00996365e+05\n",
      " 1.17019030e+05 3.05066250e+05 8.25290866e+04 8.45934065e+03\n",
      " 1.01732217e+05 5.04826757e+05 3.12607559e+03 8.56439416e+04\n",
      " 4.01901772e+04 1.13058242e+04 8.91574256e+02 1.76907858e+03\n",
      " 1.68788295e+04 2.15613346e+04 7.02755097e+04 1.88824989e+04\n",
      " 1.41709080e+02 1.58184065e+02 2.41793973e+04 3.75166459e+02\n",
      " 2.95864454e+04 1.77602618e+04 2.96510155e+04 1.05368463e+05\n",
      " 6.88629889e+04 4.78720560e+04 8.89242748e+04 4.38066491e+03\n",
      " 1.32825428e+04 4.06813015e+03 8.43761115e+03 5.28957148e+04\n",
      " 2.61419338e+04 1.04642041e+05 5.07380767e+04 9.24789352e+03\n",
      " 3.87264051e+04 6.49126257e+05 1.20272335e+05 1.54865894e+04\n",
      " 2.66705466e+04 1.36338189e+02 1.01797068e+03 1.55858396e+05\n",
      " 4.15644762e+04 7.35220174e+04 6.87108625e+03 3.22945191e+03\n",
      " 9.99167036e+03 5.60901867e+03 1.32660799e+05 4.45835066e+05\n",
      " 2.54653747e+03 1.15113826e+04 1.07306177e+02 7.30787208e+04\n",
      " 3.70668733e+04 2.19203933e+05 9.59808002e+03 1.04646213e+05\n",
      " 2.67365121e+02 1.22264380e+05 7.60940889e+04 1.62340799e+05\n",
      " 4.33886231e+05 1.99951783e+04 2.01370985e+04 2.15127917e+04\n",
      " 3.73431133e+04 8.51158225e+03 8.35698565e+02 1.02446800e+05\n",
      " 4.01612261e+03 2.10220994e+02 1.38346389e+04 9.20707152e+02\n",
      " 1.86013545e+04 5.23280211e+04 7.78555299e+04 2.73538402e+05\n",
      " 9.80027249e+01 7.90372648e+02 1.89225669e+04 2.75354963e+05\n",
      " 4.51182564e+02 1.38689556e+05 3.40886014e+04 1.25510518e+04\n",
      " 1.88224200e+05 1.01182282e+04 4.67099337e+05 1.24966369e+03\n",
      " 1.28990767e+05 8.65712009e+04 1.65645113e+04 1.46144799e+05\n",
      " 2.94487883e+03 2.53329759e+02 1.46041070e+04 2.46325314e+05\n",
      " 1.16943806e+05 3.09004227e+03 3.91985930e+04 1.12905273e+02\n",
      " 9.71643637e+02 1.24266018e+05 7.65815610e+04 7.18105188e+04\n",
      " 2.10585769e+04 6.87652447e+03 1.12124677e+02 1.32361638e+05\n",
      " 4.93417199e+04 1.53355133e+05 2.28928020e+04 1.61011191e+04\n",
      " 1.99258274e+03 1.56629965e+04 5.58117816e+04 7.74606721e+03\n",
      " 1.81162847e+02 1.06919136e+05 1.92911843e+03 1.41548201e+05\n",
      " 2.29825039e+02 7.57282728e+04 1.17974566e+04 2.56670897e+03\n",
      " 3.13302414e+05 1.89558584e+04 3.09482397e+03 4.58339427e+02\n",
      " 9.57290008e+03 5.29503230e+02 2.02316652e+04 2.58462245e+05\n",
      " 1.30128247e+02 1.35531517e+05 1.07873253e+02 2.85252734e+02\n",
      " 7.80490361e+02 2.05577205e+05 4.03521723e+03 4.44791111e+02\n",
      " 2.61818208e+02 6.97160947e+04 1.03296484e+06 9.48015590e+03\n",
      " 1.66166612e+02 2.86641547e+04 1.20353514e+05 1.00128675e+04\n",
      " 6.44048395e+03 2.47790675e+04 1.07165218e+02 5.50880153e+04\n",
      " 1.64798715e+04 2.43518498e+02 2.82820941e+04 2.58614200e+04\n",
      " 9.01803481e+04 1.25547251e+05 4.60336857e+04 5.69038041e+02\n",
      " 1.09862174e+02 1.85529853e+04 7.03086086e+04 9.19270307e+03\n",
      " 1.12314730e+05 1.01796453e+05 1.65672779e+02 3.20025217e+04\n",
      " 2.89894347e+02 1.02360938e+05 1.34367254e+05 1.28322908e+04\n",
      " 1.43596667e+02 4.30664308e+02 1.56321621e+04 2.44683356e+02\n",
      " 3.97993414e+02 3.20255170e+02 6.74763524e+02 5.59999264e+02\n",
      " 9.77706041e+03 6.49648974e+04 1.77034340e+02 7.44469712e+03\n",
      " 5.88897796e+04 9.47113737e+03 2.80458652e+04 8.60752953e+01\n",
      " 1.23309339e+03 1.77104770e+02 2.52904168e+04 7.83939885e+02\n",
      " 7.05563387e+05 6.80426069e+04 4.94687215e+04 5.15855412e+04\n",
      " 7.68904384e+03 3.81027725e+02 6.63878725e+03 3.35768943e+04\n",
      " 3.72423182e+03 2.24377629e+02 1.33408510e+04 1.28120682e+03\n",
      " 1.15832377e+05 3.17646153e+04 1.85855205e+05 1.12653652e+04\n",
      " 5.00454780e+05 1.71382425e+03 1.31892028e+02 1.17670811e+03\n",
      " 4.21481911e+04 4.27863425e+05 2.03010947e+02 1.50317422e+04\n",
      " 8.06106331e+04 1.42551942e+04 1.78171182e+02 1.42238438e+05\n",
      " 5.29420894e+02 4.01751065e+04 4.25667610e+02 1.48777413e+05\n",
      " 1.58702196e+05 8.39963887e+03 3.09941510e+04 1.53149075e+02\n",
      " 3.56933184e+05 1.88449598e+02 1.55700849e+05 5.02252280e+03\n",
      " 7.46363906e+03 4.10683490e+02 1.04008057e+04 1.95672175e+04\n",
      " 5.82668966e+03 9.08455744e+03 1.62099046e+04 1.46726934e+04\n",
      " 1.23110082e+02 2.55955008e+04 4.70614860e+05 3.23230453e+02\n",
      " 2.06408926e+04 5.58941943e+04 6.80178749e+04 8.76042048e+03\n",
      " 5.05511217e+03 7.54367767e+04 2.69860505e+02 2.69372769e+05\n",
      " 9.70446709e+03 4.16725043e+03 1.31290789e+05 8.47801755e+04\n",
      " 1.97173624e+05 1.16117824e+03 2.43540716e+02 4.62849014e+03\n",
      " 1.98194087e+04 4.71991606e+02 3.38854407e+02 2.01018625e+05\n",
      " 9.96549216e+03 1.39237651e+02 1.57844170e+04 1.55079055e+02\n",
      " 2.37120301e+04 1.00575019e+05 4.25872351e+04 1.38261868e+03\n",
      " 6.23612507e+03 2.76859767e+02 1.59870966e+04 8.37842025e+04\n",
      " 3.29020582e+04 3.18954508e+03 6.58963347e+03 1.11661844e+05\n",
      " 1.18001555e+05 3.13007493e+04 6.22757954e+04 8.72346992e+04\n",
      " 8.25357249e+04 4.64788484e+02 3.75367190e+04 1.54247489e+03\n",
      " 2.77189702e+05 2.66965862e+02 6.79320119e+01 1.41158043e+04\n",
      " 2.32627516e+02 4.58720461e+04 5.62848956e+04 1.75515226e+05\n",
      " 2.86126365e+04 1.27444241e+05 8.36393385e+02 3.73745627e+04\n",
      " 1.53066297e+04 9.63403151e+04 4.02421980e+04 2.62079162e+02\n",
      " 8.84184714e+04 4.26403196e+02 1.36776356e+04 5.06573557e+05\n",
      " 2.47287860e+04 2.18886624e+02 4.85908278e+04 2.12353748e+05\n",
      " 1.28188579e+04 3.87664984e+03 5.88173472e+04 4.94895277e+05\n",
      " 8.52657822e+04 1.56951264e+05 2.06261507e+05 2.24803019e+02]\n",
      "[0.8971454923333353]\n",
      "MAPE =  0.8971454923333353\n",
      "Epoch [20/1000], Train Loss: 203091382853.7020, Val Loss: 54364479975.6065\n",
      "Epoch [40/1000], Train Loss: 225937913697.6666, Val Loss: 53004984147.7099\n",
      "Epoch [60/1000], Train Loss: 46485342752.0618, Val Loss: 51669413560.7512\n",
      "Epoch [80/1000], Train Loss: 336978975866.6130, Val Loss: 50574856964.4155\n",
      "Epoch [100/1000], Train Loss: 122085942186.4540, Val Loss: 50065858021.5275\n",
      "Epoch [120/1000], Train Loss: 119546520139.2618, Val Loss: 49206630200.2379\n",
      "Epoch [140/1000], Train Loss: 77674536688.7951, Val Loss: 49004889153.8646\n",
      "Epoch [160/1000], Train Loss: 49440241325.1613, Val Loss: 49001453122.9625\n",
      "Epoch [180/1000], Train Loss: 128842490359.7807, Val Loss: 48989129743.0090\n",
      "Epoch [200/1000], Train Loss: 32645844921.6327, Val Loss: 48971593051.1678\n",
      "Epoch [220/1000], Train Loss: 222343086891.7872, Val Loss: 48945817331.3673\n",
      "Epoch [240/1000], Train Loss: 272804468135.0071, Val Loss: 49032947938.4933\n",
      "Early stopping at epoch 252\n",
      "\n",
      "Fold number: 1\n",
      "[2.56886231e+05 6.90796129e+03 2.93605675e+04 6.97496870e+04\n",
      " 5.17157719e+03 8.32290661e+04 1.35019208e+04 9.48627338e+04\n",
      " 7.38110127e+04 1.79426717e+05 4.85570512e+04 4.60314642e+03\n",
      " 3.81895208e+04 1.28571998e+05 4.02642249e+05 2.06966991e+02\n",
      " 1.92638656e+05 1.36344908e+02 5.53931524e+02 9.70873242e+02\n",
      " 2.23756881e+03 1.93047208e+04 2.27517448e+02 1.87962017e+04\n",
      " 1.22938976e+04 6.99083177e+04 7.33848287e+03 1.17481901e+03\n",
      " 1.17357576e+02 9.05897316e+02 3.05031485e+05 7.85986581e+04\n",
      " 5.27661987e+02 7.33797505e+03 1.04402700e+04 2.02056958e+05\n",
      " 1.07098127e+05 3.06904442e+05 8.26629687e+04 8.52394680e+03\n",
      " 1.04070718e+05 4.61493371e+05 3.19197598e+03 8.50515752e+04\n",
      " 4.12514591e+04 1.18059959e+04 9.25141216e+02 1.48183070e+03\n",
      " 1.69872913e+04 2.15440214e+04 6.61313160e+04 1.94022137e+04\n",
      " 1.43513071e+02 1.47441685e+02 2.45772884e+04 3.85540450e+02\n",
      " 3.02969855e+04 1.80042438e+04 3.08494471e+04 1.06044651e+05\n",
      " 7.17927496e+04 4.77676165e+04 9.26067750e+04 4.59032351e+03\n",
      " 9.91584098e+04 4.05271713e+03 1.95917593e+04 5.51306654e+04\n",
      " 5.39677953e+04 1.12388800e+05 5.19419537e+04 9.37046947e+03\n",
      " 1.63280698e+05 6.74753489e+05 1.24431395e+05 1.60683222e+04\n",
      " 2.64383334e+04 1.32824354e+02 1.00844396e+03 1.60686768e+05\n",
      " 4.19738436e+04 7.03207565e+04 9.48172149e+04 3.26757485e+03\n",
      " 1.94368123e+05 1.12116389e+05 1.33452311e+05 4.54216091e+05\n",
      " 2.57741586e+03 1.54232393e+05 3.12100124e+04 7.69864001e+04\n",
      " 1.02778738e+05 2.25539300e+05 9.06997536e+03 1.09384060e+05\n",
      " 2.75578609e+02 1.21690893e+05 7.21648347e+04 1.63892793e+05\n",
      " 4.43647723e+05 2.07773958e+04 9.38956633e+04 2.22190466e+04\n",
      " 3.77296601e+04 8.33720482e+03 8.20302544e+02 1.02908564e+05\n",
      " 4.05627353e+03 2.14765503e+02 1.43742829e+04 8.90216869e+02\n",
      " 1.83055410e+04 5.09202335e+04 1.21143530e+05 2.85176542e+05\n",
      " 9.32926056e+01 8.00605792e+02 1.96367388e+04 2.40552673e+05\n",
      " 4.34875330e+02 1.36935285e+05 3.42682504e+04 1.07390567e+04\n",
      " 1.93531899e+05 9.42573444e+03 4.08011302e+05 1.28950222e+03\n",
      " 1.34438277e+05 9.00721120e+04 1.72140255e+04 1.32779426e+05\n",
      " 2.90914745e+03 2.31266988e+02 1.48322887e+04 2.38045229e+05\n",
      " 1.12850990e+05 3.12486911e+03 2.18941855e+03 2.09681827e+04\n",
      " 1.00123119e+03 1.27642165e+05 7.04651066e+04 7.20507828e+04\n",
      " 1.99953633e+04 6.42364829e+03 1.15304484e+02 5.07372552e+04\n",
      " 4.66024508e+04 1.45608937e+05 2.20386971e+04 1.67798852e+04\n",
      " 1.96670484e+03 3.98525283e+03 2.21733621e+05 7.91556055e+03\n",
      " 1.87745860e+02 1.11058370e+05 1.98945881e+03 1.46988524e+05\n",
      " 2.37081653e+02 7.87144029e+04 1.21222931e+04 1.17762112e+05\n",
      " 1.05876511e+05 1.90937949e+04 2.94490951e+03 4.60330442e+02\n",
      " 2.12671911e+03 1.47121209e+05 2.08473746e+04 1.51137515e+05\n",
      " 1.32480735e+02 1.29859412e+05 1.10691626e+02 2.72273105e+02\n",
      " 8.01172595e+02 1.98137068e+05 3.83711059e+03 4.32727584e+02\n",
      " 2.75444715e+05 7.70170759e+04 1.05583896e+06 9.86101444e+03\n",
      " 1.65948463e+02 1.94186324e+03 1.23058374e+05 1.00210737e+04\n",
      " 6.50998055e+03 2.55588303e+04 1.06592517e+02 5.47934607e+04\n",
      " 1.68226033e+04 2.53263392e+02 2.66750703e+04 2.51341830e+04\n",
      " 8.74695467e+04 1.30637513e+05 4.36056360e+04 5.38136178e+02\n",
      " 1.11453753e+02 1.75192415e+04 6.28553462e+04 9.49767896e+03\n",
      " 7.11414054e+04 9.81823961e+04 1.70632742e+02 1.45424282e+04\n",
      " 2.93340094e+02 1.01011719e+05 1.34743795e+05 1.32994992e+04\n",
      " 1.44319529e+02 4.18879736e+02 5.22853622e+02 2.52166604e+02\n",
      " 4.06693209e+02 3.05805369e+02 6.95231920e+02 5.76865250e+02\n",
      " 1.00834239e+04 5.45095795e+05 1.76923323e+02 2.86144579e+05\n",
      " 6.10101892e+04 9.57299077e+03 2.89753629e+04 8.33991411e+01\n",
      " 2.71064580e+04 1.84590269e+02 2.08586980e+04 4.16030324e+02\n",
      " 1.96846052e+04 7.03147024e+04 4.91406573e+04 2.70304041e+05\n",
      " 1.23983246e+04 3.90898555e+02 6.83353340e+03 2.15002052e+04\n",
      " 3.88675683e+03 2.11292712e+02 4.53958303e+03 1.34159582e+03\n",
      " 1.15323276e+05 1.99510929e+04 1.73805426e+05 1.17808345e+04\n",
      " 5.14186085e+05 1.52850637e+03 1.26886549e+02 1.06276668e+03\n",
      " 6.54889850e+02 4.45016875e+05 2.09600122e+02 1.58221317e+04\n",
      " 8.12815948e+04 1.43247260e+04 1.85949044e+02 1.45198842e+05\n",
      " 5.40822672e+02 3.76554521e+04 4.29606305e+02 1.40200767e+05\n",
      " 1.62254764e+05 8.67613683e+03 2.17818447e+04 1.56285245e+02\n",
      " 3.63245999e+05 1.93920203e+02 1.07760250e+04 5.21591803e+03\n",
      " 6.98218453e+03 2.14217936e+02 1.08017760e+04 2.08676611e+04\n",
      " 5.82039121e+03 4.74888376e+04 1.63978705e+04 1.51475334e+04\n",
      " 1.28561927e+02 2.62641568e+04 4.77761476e+05 3.11735404e+02\n",
      " 2.04727488e+04 5.14696624e+04 6.56451928e+04 8.11065857e+04\n",
      " 5.22689518e+03 7.81956967e+04 2.79298563e+02 2.75643922e+05\n",
      " 9.11964333e+03 3.97597343e+03 1.30179997e+05 8.59799694e+04\n",
      " 2.05441656e+05 1.21215107e+03 2.47312893e+02 1.99795875e+02\n",
      " 1.96494374e+04 4.93200121e+02 3.47609773e+02 1.92996646e+05\n",
      " 1.02413459e+04 1.32195651e+02 1.58096237e+04 1.36714042e+05\n",
      " 2.43481190e+04 1.04920290e+05 4.41326002e+04 1.39455718e+03\n",
      " 7.68770639e+04 2.60761442e+02 2.57044346e+05 7.94073991e+04\n",
      " 3.43110755e+04 3.27831220e+03 6.17669521e+03 1.11427519e+05\n",
      " 1.18205195e+05 3.22512262e+04 4.86043602e+04 1.24410904e+05\n",
      " 7.73048838e+04 2.47835709e+02 3.85508485e+04 1.56725385e+03\n",
      " 2.87458591e+05 2.62337230e+02 6.78145574e+01 1.45485096e+04\n",
      " 2.24080203e+02 4.76519450e+04 5.77271699e+04 1.81896760e+05\n",
      " 2.88626969e+04 1.21350004e+05 8.26215688e+02 3.83843616e+04\n",
      " 1.51955874e+04 9.20357494e+04 4.10607515e+04 2.70604994e+02\n",
      " 9.10250000e+04 4.10050682e+02 1.11162249e+04 4.81154177e+05\n",
      " 2.55520957e+04 2.25730191e+02 4.98929734e+04 1.93235793e+05\n",
      " 1.22387319e+04 4.03995823e+03 5.79844175e+04 4.78303396e+05\n",
      " 8.81415337e+04 1.57276316e+05 2.13435658e+05 2.22620306e+02]\n",
      "[0.8563429122776298]\n",
      "MAPE =  0.8563429122776298\n",
      "Epoch [20/1000], Train Loss: 44863591913.1542, Val Loss: 128978570023.6405\n",
      "Epoch [40/1000], Train Loss: 151987353670.6449, Val Loss: 83810641819.4230\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # stock_info = sys.argv[0]\n",
    "    # lag_bin = int(sys.argv[1])\n",
    "    # lag_day = int(sys.argv[2])\n",
    "    # bin_num = int(sys.argv[3])\n",
    "    # random_state_here = int(sys.argv[4])\n",
    "    # test_set_size = float(sys.argv[5])\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = sorted(list(set(s.split('_25')[0] for s in files)))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:2]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        mape_list = []\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(testInputs), torch.tensor(testgraphInput), torch.tensor(testTargets))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "        K = 5  # Number of folds\n",
    "        for k in range(K):\n",
    "            torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "            trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "            trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "            trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "            valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "            valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "            trainInputs = normalize(trainInputsAll[:, :])\n",
    "            valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "            # Assuming trainInputs, trainGraphInput, trainGraphFeatureInput, trainTargets are PyTorch tensors\n",
    "            train_dataset = TensorDataset(torch.tensor(trainInputs), torch.tensor(trainGraphInput),torch.tensor(trainTargets))\n",
    "            val_dataset = TensorDataset(torch.tensor(valInputs), torch.tensor(valGraphInput),torch.tensor(valTargets))\n",
    "\n",
    "            # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "            # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "        \n",
    "            model = GATModel(7,8,1,0.3,0.2,3)\n",
    "            model.fit(train_loader, val_loader)\n",
    "            predictions=model.test(test_loader)\n",
    "            torch.save(model.state_dict(), f'models/gat_{stock_info}_{lag_bin}_{lag_day}_gcn_model_iteration_{k}.pt')\n",
    "    \n",
    "            print()\n",
    "            print('Fold number:', k)\n",
    "\n",
    "            new_predictions = np.array([item.detach().numpy() for item in predictions]).flatten()\n",
    "            MAPE = []\n",
    "\n",
    "            MAPE.append(mean_absolute_percentage_error(testTargets[:], new_predictions[:]))\n",
    "            print(new_predictions)\n",
    "            print(MAPE)\n",
    "            testTargets0 = list(testTargets)\n",
    "\n",
    "            res = {\n",
    "                'testTargets': testTargets0,\n",
    "                'new_predictions': new_predictions\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame(res)\n",
    "            res_df.to_csv(f'./result/gat_{stock_info}_{lag_bin}_{lag_day}_res_test_MAPE{k}.csv', index=False)\n",
    "\n",
    "            print('MAPE = ', np.array(MAPE).mean())\n",
    "            MAPE_mean = np.array(MAPE).mean()\n",
    "            mape_list.append(MAPE)\n",
    "\n",
    "        print('-')\n",
    "        print('mape score = ', mape_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8eb41-44fb-4c32-ab31-5e5eeef960db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe661f3-83b3-4f01-8f8b-d7bb51f9bd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b24ff-7880-4dbd-8a13-96d4e682ce53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
