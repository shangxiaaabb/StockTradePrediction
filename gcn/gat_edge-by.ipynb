{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e99867e-b47b-4bd1-b701-6860aee8f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160f4062-acf2-4d2d-bb5e-8a3d4865f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\")\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.A = nn.Parameter(torch.zeros(size=(2*out_features, 16)))\n",
    "        nn.init.xavier_uniform_(self.A.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "      \n",
    "        E = [torch.matmul(a_input.double(), self.A[:,i].unsqueeze(1).double()).squeeze(3)[:,:,i] for i in range(N)]\n",
    "\n",
    "        e = self.leakyrelu(torch.stack(E, dim=2))\n",
    "        # print(e.shape)\n",
    "\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        # print(attention.shape)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        #x = F.log_softmax(x, dim=2)[:, -1, :]\n",
    "        # print(x)\n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self,train_loader, val_loader,lr_rate,w_d,num_epochs=5000,patience=100):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr_rate,weight_decay=w_d)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0 \n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs.squeeze(dim=1), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "            #     train_loss += loss.item()\n",
    "            # avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            #         val_loss += val_loss.item()\n",
    "            # avg_val_loss=val_loss / len(val_loader)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return\n",
    "                \n",
    "                \n",
    "    def test(self,test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110decbd-cfe6-4c03-934f-eab8843b749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000046_XSHE', '000753_XSHE', '000951_XSHE', '000998_XSHE', '002282_XSHE', '002679_XSHE', '002841_XSHE', '002882_XSHE', '300133_XSHE', '300174_XSHE', '300263_XSHE', '300343_XSHE', '300433_XSHE', '300540_XSHE', '600622_XSHG', '603053_XSHG', '603095_XSHG', '603359_XSHG']\n",
      ">>>>>>>>>>>>>>>>>>>>000046_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "|   iter    |  target   |   alpha   |  dropout  |  hidden   |    lr     | nb_heads  | weight... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Epoch [20/5000], Train Loss: 261822451968.9021, Val Loss: 742201643523.1125\n",
      "Epoch [40/5000], Train Loss: 589252049946.1726, Val Loss: 649694628895.6106\n",
      "Epoch [60/5000], Train Loss: 227437208323.4013, Val Loss: 525825842874.0508\n",
      "Epoch [80/5000], Train Loss: 446482223479.6632, Val Loss: 558449053251.7234\n",
      "Epoch [100/5000], Train Loss: 202168880798.1959, Val Loss: 647346015844.0005\n",
      "Epoch [120/5000], Train Loss: 2328226144798.3433, Val Loss: 532074929559.4905\n",
      "Epoch [140/5000], Train Loss: 144439330057.5938, Val Loss: 612031408559.2478\n",
      "Epoch [160/5000], Train Loss: 460341294355.9467, Val Loss: 730142808353.8623\n",
      "Early stopping at epoch 164\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.8549  \u001b[0m | \u001b[0m0.4336   \u001b[0m | \u001b[0m0.3881   \u001b[0m | \u001b[0m6.0      \u001b[0m | \u001b[0m0.003093 \u001b[0m | \u001b[0m2.027    \u001b[0m | \u001b[0m0.001014 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 147595571854.6047, Val Loss: 823005506531.2955\n",
      "Epoch [40/5000], Train Loss: 1530682133707.2966, Val Loss: 404867392387.6420\n",
      "Epoch [60/5000], Train Loss: 342453369317.8413, Val Loss: 928151315209.6710\n",
      "Epoch [80/5000], Train Loss: 628155730414.7734, Val Loss: 436487271758.2561\n",
      "Epoch [100/5000], Train Loss: 798759496517.9747, Val Loss: 616774487893.6013\n",
      "Epoch [120/5000], Train Loss: 313774887011.7878, Val Loss: 828128736154.0375\n",
      "Epoch [140/5000], Train Loss: 320255846784.3310, Val Loss: 522524034271.0511\n",
      "Early stopping at epoch 149\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.7866  \u001b[0m | \u001b[95m0.249    \u001b[0m | \u001b[95m0.2382   \u001b[0m | \u001b[95m7.19     \u001b[0m | \u001b[95m0.005434 \u001b[0m | \u001b[95m3.934    \u001b[0m | \u001b[95m0.006884 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 122430984423.6219, Val Loss: 609956822645.4495\n",
      "Epoch [40/5000], Train Loss: 254505290057.3373, Val Loss: 684786723150.6317\n",
      "Epoch [60/5000], Train Loss: 644754216799.2672, Val Loss: 626635352399.8937\n",
      "Epoch [80/5000], Train Loss: 244046008187.6193, Val Loss: 892295418681.1110\n",
      "Epoch [100/5000], Train Loss: 3546106325906.5518, Val Loss: 830549023628.9700\n",
      "Epoch [120/5000], Train Loss: 485738619606.5257, Val Loss: 865253194507.3400\n",
      "Epoch [140/5000], Train Loss: 659026999110.3364, Val Loss: 839136396413.1329\n",
      "Epoch [160/5000], Train Loss: 207726022268.5176, Val Loss: 789819872505.1060\n",
      "Epoch [180/5000], Train Loss: 701154477686.5496, Val Loss: 617419709930.1196\n",
      "Epoch [200/5000], Train Loss: 566062808113.4016, Val Loss: 855138491191.1326\n",
      "Epoch [220/5000], Train Loss: 882485816041.5980, Val Loss: 720206416866.4576\n",
      "Epoch [240/5000], Train Loss: 583229415467.9393, Val Loss: 740596174867.9366\n",
      "Early stopping at epoch 255\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-0.6785  \u001b[0m | \u001b[95m0.2636   \u001b[0m | \u001b[95m0.4512   \u001b[0m | \u001b[95m6.082    \u001b[0m | \u001b[95m0.006738 \u001b[0m | \u001b[95m3.921    \u001b[0m | \u001b[95m0.005631 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 245452259496.7808, Val Loss: 884529386985.2295\n",
      "Epoch [40/5000], Train Loss: 305917954077.8688, Val Loss: 465665125162.1123\n",
      "Epoch [60/5000], Train Loss: 770012697635.9045, Val Loss: 495109902387.5701\n",
      "Epoch [80/5000], Train Loss: 509335435030.2423, Val Loss: 488134537845.7201\n",
      "Epoch [100/5000], Train Loss: 180208050892.1248, Val Loss: 361720621572.7809\n",
      "Epoch [120/5000], Train Loss: 493791516220.2291, Val Loss: 643097635328.3551\n",
      "Epoch [140/5000], Train Loss: 680713066976.0768, Val Loss: 645459470334.2482\n",
      "Epoch [160/5000], Train Loss: 884770734035.6942, Val Loss: 683080359150.3094\n",
      "Epoch [180/5000], Train Loss: 517904545394.2007, Val Loss: 645914818422.4529\n",
      "Early stopping at epoch 193\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.8949  \u001b[0m | \u001b[0m0.2123   \u001b[0m | \u001b[0m0.1792   \u001b[0m | \u001b[0m8.402    \u001b[0m | \u001b[0m0.009686 \u001b[0m | \u001b[0m3.194    \u001b[0m | \u001b[0m0.006954 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 775952817189.5249, Val Loss: 1811327532665.0564\n",
      "Epoch [40/5000], Train Loss: 192377213761.6789, Val Loss: 796270739658.7555\n",
      "Epoch [60/5000], Train Loss: 3412271411262.0811, Val Loss: 669816303016.9298\n",
      "Epoch [80/5000], Train Loss: 258073020131.8751, Val Loss: 572419962074.6610\n",
      "Epoch [100/5000], Train Loss: 472076600358.7039, Val Loss: 909326999186.7181\n",
      "Epoch [120/5000], Train Loss: 671696541451.0190, Val Loss: 774091191950.5391\n",
      "Epoch [140/5000], Train Loss: 221554252478.4470, Val Loss: 626039103994.3105\n",
      "Early stopping at epoch 150\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m-0.6234  \u001b[0m | \u001b[95m0.8011   \u001b[0m | \u001b[95m0.4578   \u001b[0m | \u001b[95m6.255    \u001b[0m | \u001b[95m0.0004866\u001b[0m | \u001b[95m2.189    \u001b[0m | \u001b[95m0.008794 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 1299927639799.5168, Val Loss: 326184098784.3365\n",
      "Epoch [40/5000], Train Loss: 569747972241.4481, Val Loss: 392909937102.1615\n",
      "Epoch [60/5000], Train Loss: 313151382318.1569, Val Loss: 556181850179.3884\n",
      "Epoch [80/5000], Train Loss: 569641249693.3378, Val Loss: 838152965825.2926\n",
      "Epoch [100/5000], Train Loss: 153243687260.7017, Val Loss: 447430757629.3539\n",
      "Epoch [120/5000], Train Loss: 494724813237.5007, Val Loss: 484329447929.9944\n",
      "Epoch [140/5000], Train Loss: 156302668605.7955, Val Loss: 219157299875.4692\n",
      "Epoch [160/5000], Train Loss: 1299356274343.3645, Val Loss: 590912292935.6385\n",
      "Epoch [180/5000], Train Loss: 311525805198.4160, Val Loss: 723669121989.5732\n",
      "Epoch [200/5000], Train Loss: 511720620838.2068, Val Loss: 416137832750.8506\n",
      "Epoch [220/5000], Train Loss: 694426993244.5276, Val Loss: 448813940872.9333\n",
      "Epoch [240/5000], Train Loss: 213659583206.5799, Val Loss: 578363719801.6505\n",
      "Early stopping at epoch 240\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m-0.6226  \u001b[0m | \u001b[95m0.1787   \u001b[0m | \u001b[95m0.2684   \u001b[0m | \u001b[95m8.874    \u001b[0m | \u001b[95m0.005378 \u001b[0m | \u001b[95m5.843    \u001b[0m | \u001b[95m0.003224 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 592682512289.6356, Val Loss: 776840965286.8986\n",
      "Epoch [40/5000], Train Loss: 653462384302.9119, Val Loss: 666125167180.0917\n",
      "Epoch [60/5000], Train Loss: 412644193354.4498, Val Loss: 836891377892.5980\n",
      "Epoch [80/5000], Train Loss: 327670879666.5992, Val Loss: 776865025019.2930\n",
      "Epoch [100/5000], Train Loss: 264706528998.6891, Val Loss: 706996497198.7832\n",
      "Epoch [120/5000], Train Loss: 215876845420.2119, Val Loss: 471087967943.8999\n",
      "Epoch [140/5000], Train Loss: 387945166182.7522, Val Loss: 933420387650.0483\n",
      "Early stopping at epoch 158\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m-0.5546  \u001b[0m | \u001b[95m0.6492   \u001b[0m | \u001b[95m0.4339   \u001b[0m | \u001b[95m6.055    \u001b[0m | \u001b[95m0.007526 \u001b[0m | \u001b[95m7.922    \u001b[0m | \u001b[95m0.007507 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 332300606534.5916, Val Loss: 720747927914.5773\n",
      "Epoch [40/5000], Train Loss: 426035315139.3520, Val Loss: 777053147926.8669\n",
      "Epoch [60/5000], Train Loss: 244717879428.4711, Val Loss: 759863893940.2036\n",
      "Epoch [80/5000], Train Loss: 306069436466.5960, Val Loss: 815065367703.3125\n",
      "Epoch [100/5000], Train Loss: 179568917447.8763, Val Loss: 667019357329.3016\n",
      "Early stopping at epoch 103\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.6066  \u001b[0m | \u001b[0m0.3244   \u001b[0m | \u001b[0m0.4157   \u001b[0m | \u001b[0m6.31     \u001b[0m | \u001b[0m0.004534 \u001b[0m | \u001b[0m7.36     \u001b[0m | \u001b[0m0.003007 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 497983902036.8342, Val Loss: 865830688626.3854\n",
      "Epoch [40/5000], Train Loss: 627070128015.0706, Val Loss: 741294264618.9576\n",
      "Epoch [60/5000], Train Loss: 327122089580.2178, Val Loss: 764729444085.7644\n",
      "Epoch [80/5000], Train Loss: 333899549972.8411, Val Loss: 500459534465.7602\n",
      "Epoch [100/5000], Train Loss: 256441966109.5581, Val Loss: 431581441993.0002\n",
      "Epoch [120/5000], Train Loss: 343592274328.9304, Val Loss: 609465617825.0250\n",
      "Epoch [140/5000], Train Loss: 135222843016.0660, Val Loss: 360700282460.4699\n",
      "Epoch [160/5000], Train Loss: 467333702855.2823, Val Loss: 771514316239.0035\n",
      "Epoch [180/5000], Train Loss: 579394615286.9087, Val Loss: 505209209518.9821\n",
      "Epoch [200/5000], Train Loss: 295823075626.7800, Val Loss: 413065644249.3227\n",
      "Early stopping at epoch 205\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.7975  \u001b[0m | \u001b[0m0.3302   \u001b[0m | \u001b[0m0.152    \u001b[0m | \u001b[0m6.058    \u001b[0m | \u001b[0m0.00682  \u001b[0m | \u001b[0m2.481    \u001b[0m | \u001b[0m0.002729 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 232284827345.4693, Val Loss: 404934702732.8154\n",
      "Epoch [40/5000], Train Loss: 285518212742.7316, Val Loss: 682450888064.2825\n",
      "Epoch [60/5000], Train Loss: 510849702996.0384, Val Loss: 474255307326.6211\n",
      "Epoch [80/5000], Train Loss: 169262045160.4637, Val Loss: 352102709076.3840\n",
      "Epoch [100/5000], Train Loss: 1760894078200.0525, Val Loss: 548964348636.9550\n",
      "Epoch [120/5000], Train Loss: 338201622215.6291, Val Loss: 724681675673.9401\n",
      "Epoch [140/5000], Train Loss: 762722648305.2008, Val Loss: 469276725863.7655\n",
      "Epoch [160/5000], Train Loss: 128585599596.6419, Val Loss: 188142826675.1270\n",
      "Epoch [180/5000], Train Loss: 216524700615.3589, Val Loss: 212953762104.6176\n",
      "Epoch [200/5000], Train Loss: 2662808812837.4150, Val Loss: 884828697758.5609\n",
      "Epoch [220/5000], Train Loss: 247064813137.7604, Val Loss: 194589994697.1136\n",
      "Epoch [240/5000], Train Loss: 324793423276.3453, Val Loss: 235987775855.0689\n",
      "Epoch [260/5000], Train Loss: 168828862648.0135, Val Loss: 292008240728.5189\n",
      "Epoch [280/5000], Train Loss: 279616538270.7121, Val Loss: 293988842828.8705\n",
      "Epoch [300/5000], Train Loss: 485507163565.0437, Val Loss: 678368028200.3810\n",
      "Epoch [320/5000], Train Loss: 697869663295.7867, Val Loss: 817961069860.4479\n",
      "Early stopping at epoch 331\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.7539  \u001b[0m | \u001b[0m0.4933   \u001b[0m | \u001b[0m0.1213   \u001b[0m | \u001b[0m7.722    \u001b[0m | \u001b[0m0.001553 \u001b[0m | \u001b[0m5.125    \u001b[0m | \u001b[0m0.007028 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 379514337261.5831, Val Loss: 822532900912.0974\n",
      "Epoch [40/5000], Train Loss: 306747636281.6459, Val Loss: 717913497590.4585\n",
      "Epoch [60/5000], Train Loss: 807663333900.6652, Val Loss: 575172328846.6372\n",
      "Epoch [80/5000], Train Loss: 116897821486.4379, Val Loss: 523137825231.2655\n",
      "Epoch [100/5000], Train Loss: 345472653497.0452, Val Loss: 692910414195.4281\n",
      "Epoch [120/5000], Train Loss: 425621064718.8351, Val Loss: 869870751437.8030\n",
      "Epoch [140/5000], Train Loss: 198341857450.2715, Val Loss: 839583274873.3193\n",
      "Epoch [160/5000], Train Loss: 546550156982.2220, Val Loss: 830345058136.2559\n",
      "Epoch [180/5000], Train Loss: 363872479060.4202, Val Loss: 544889566699.3004\n",
      "Epoch [200/5000], Train Loss: 480105687518.6072, Val Loss: 740339925549.5792\n",
      "Epoch [220/5000], Train Loss: 534382621888.2642, Val Loss: 637502915800.6499\n",
      "Epoch [240/5000], Train Loss: 319539124116.3383, Val Loss: 869516456128.5286\n",
      "Epoch [260/5000], Train Loss: 475986232294.9511, Val Loss: 473799604891.2917\n",
      "Epoch [280/5000], Train Loss: 579640991207.4387, Val Loss: 763024217698.0533\n",
      "Epoch [300/5000], Train Loss: 111301124011.5650, Val Loss: 648283998884.3597\n",
      "Early stopping at epoch 303\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.6561  \u001b[0m | \u001b[0m0.5491   \u001b[0m | \u001b[0m0.2501   \u001b[0m | \u001b[0m7.437    \u001b[0m | \u001b[0m0.005656 \u001b[0m | \u001b[0m1.964    \u001b[0m | \u001b[0m0.006481 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 350219803933.9490, Val Loss: 760634522987.2662\n",
      "Epoch [40/5000], Train Loss: 172711988472.0469, Val Loss: 875180465111.8197\n",
      "Epoch [60/5000], Train Loss: 2056455273826.6919, Val Loss: 880571754109.3186\n",
      "Epoch [80/5000], Train Loss: 1270126686460.9919, Val Loss: 684721183680.0396\n",
      "Epoch [100/5000], Train Loss: 69535894749.6457, Val Loss: 429281737883.2533\n",
      "Epoch [120/5000], Train Loss: 377244129772.6017, Val Loss: 292408075180.8157\n",
      "Epoch [140/5000], Train Loss: 565894484486.4565, Val Loss: 731050086360.4524\n",
      "Epoch [160/5000], Train Loss: 166735192287.0634, Val Loss: 406062453704.2928\n",
      "Epoch [180/5000], Train Loss: 386521322507.9348, Val Loss: 759397785554.9844\n",
      "Epoch [200/5000], Train Loss: 671698748927.8372, Val Loss: 745804332079.3239\n",
      "Epoch [220/5000], Train Loss: 717198321114.9502, Val Loss: 356684312448.0343\n",
      "Early stopping at epoch 224\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.6098  \u001b[0m | \u001b[0m0.5782   \u001b[0m | \u001b[0m0.2274   \u001b[0m | \u001b[0m7.989    \u001b[0m | \u001b[0m0.004539 \u001b[0m | \u001b[0m2.205    \u001b[0m | \u001b[0m0.003099 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 209521764102.3449, Val Loss: 584945138050.7191\n",
      "Epoch [40/5000], Train Loss: 225025318962.5462, Val Loss: 660049056971.6775\n",
      "Epoch [60/5000], Train Loss: 289009183480.3358, Val Loss: 363138299779.8463\n",
      "Epoch [80/5000], Train Loss: 295037983150.6684, Val Loss: 673186848477.6996\n",
      "Epoch [100/5000], Train Loss: 457330388526.0156, Val Loss: 675924432771.1021\n",
      "Epoch [120/5000], Train Loss: 482296415319.0955, Val Loss: 762957411784.1508\n",
      "Epoch [140/5000], Train Loss: 251828210101.3041, Val Loss: 702443659466.8267\n",
      "Epoch [160/5000], Train Loss: 198798034828.3096, Val Loss: 674704958043.5880\n",
      "Epoch [180/5000], Train Loss: 485882139910.2537, Val Loss: 747820448977.4857\n",
      "Epoch [200/5000], Train Loss: 286659570294.5319, Val Loss: 521987950553.3469\n",
      "Epoch [220/5000], Train Loss: 286859513564.0406, Val Loss: 770925409271.1857\n",
      "Early stopping at epoch 225\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-0.857   \u001b[0m | \u001b[0m0.8177   \u001b[0m | \u001b[0m0.3119   \u001b[0m | \u001b[0m6.177    \u001b[0m | \u001b[0m0.008389 \u001b[0m | \u001b[0m2.588    \u001b[0m | \u001b[0m0.001933 \u001b[0m |\n",
      "Epoch [20/5000], Train Loss: 142129165470.3559, Val Loss: 366347041611.1310\n",
      "Epoch [40/5000], Train Loss: 87584486581.6058, Val Loss: 382189728326.7551\n",
      "Epoch [60/5000], Train Loss: 112178438586.3534, Val Loss: 438460919335.0702\n",
      "Epoch [80/5000], Train Loss: 599533420372.7324, Val Loss: 698245903555.0306\n",
      "Epoch [100/5000], Train Loss: 143077514148.5374, Val Loss: 283979203909.1303\n",
      "Epoch [120/5000], Train Loss: 224400164186.1302, Val Loss: 413272475335.5302\n",
      "Epoch [140/5000], Train Loss: 156735654352.5542, Val Loss: 292335611067.3483\n",
      "Epoch [160/5000], Train Loss: 379650116642.1825, Val Loss: 782236797764.4059\n",
      "Epoch [180/5000], Train Loss: 601519310165.8356, Val Loss: 607285210349.5634\n",
      "Epoch [200/5000], Train Loss: 358021152310.1789, Val Loss: 859326744239.1301\n",
      "Epoch [220/5000], Train Loss: 211457553501.1489, Val Loss: 503715069863.9258\n",
      "Epoch [240/5000], Train Loss: 408468286223.7953, Val Loss: 527383396315.1376\n",
      "Early stopping at epoch 245\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.7263  \u001b[0m | \u001b[0m0.4276   \u001b[0m | \u001b[0m0.1489   \u001b[0m | \u001b[0m8.19     \u001b[0m | \u001b[0m0.008694 \u001b[0m | \u001b[0m5.64     \u001b[0m | \u001b[0m0.00401  \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "def target_function(lr, hidden, nb_heads, dropout, alpha, weight_decay):\n",
    "    # 在这里构建你的模型，并返回一个评估指标（例如，MAPE）\n",
    "    model = GAT(7, int(hidden), 1, dropout, alpha, int(nb_heads)).to(device)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(train_inputs).to(device), torch.tensor(train_graph_inputs).to(device),torch.tensor(train_targets).to(device)), batch_size=50, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(val_inputs).to(device), torch.tensor(val_graph_inputs).to(device), torch.tensor(val_targets).to(device)), batch_size=50, shuffle=False)\n",
    "    model.fit(train_loader, val_loader, lr, weight_decay)\n",
    "    predictions = model.test(val_loader)\n",
    "    val_predictions = np.array([item.detach().cpu().numpy() for item in predictions]).flatten()\n",
    "    mape = mean_absolute_percentage_error(val_targets, val_predictions)   \n",
    "    return -np.mean(mape)  # 贝叶斯优化最小化目标，因此我们取负值\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin) + 1) * (int(lag_day) + 1)\n",
    "    forecast_days = 15\n",
    "    bin_num = 24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files = file_name('./data/')\n",
    "    stocks_info = sorted(list(set(s.split('_25')[0] for s in files)))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:2]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num * forecast_days\n",
    "        num_iterations = 10  # Define the number of random search iterations\n",
    "        \n",
    "        # 加载数据\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes, -1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        # 划分数据集，将数据集中的 15*24 天数据作为测试集\n",
    "        test_inputs = inputs[-test_set_size:]\n",
    "        test_targets = targets[-test_set_size:]\n",
    "        test_graph_inputs = graph_input[-test_set_size:]\n",
    "        test_graph_features = graph_features[:, -test_set_size:]\n",
    "\n",
    "        # 去除测试集后剩下的数据\n",
    "        train_val_inputs = inputs[:-test_set_size]\n",
    "        train_val_targets = targets[:-test_set_size]\n",
    "        train_val_graph_inputs = graph_input[:-test_set_size]\n",
    "        train_val_graph_features = graph_features[:-test_set_size]\n",
    "\n",
    "        # 将剩下的数据按照 80% 和 20% 划分为训练集和验证集\n",
    "        val_size = int(train_val_inputs.shape[0] * 0.2)\n",
    "        train_inputs, val_inputs, train_targets, val_targets, train_graph_inputs, val_graph_inputs = train_test_split(train_val_inputs, train_val_targets, train_val_graph_inputs, test_size=val_size, random_state=random_state_here)\n",
    "        test_inputs = normalize(test_inputs)\n",
    "        \n",
    "        # 定义超参数搜索空间\n",
    "        pbounds = {'lr': (0.0001, 0.01),\n",
    "                   'hidden': (6, 9),\n",
    "                   'nb_heads': (1, 8),\n",
    "                   'dropout': (0.1, 0.5),\n",
    "                   'alpha': (0.1, 0.9),\n",
    "                   'weight_decay': (0.0001, 0.01)}\n",
    "        \n",
    "        # 创建贝叶斯优化对象\n",
    "        optimizer = BayesianOptimization(f=target_function, pbounds=pbounds, random_state=1)\n",
    "        \n",
    "        # 运行贝叶斯优化搜索\n",
    "        optimizer.maximize(init_points=10, n_iter=10)\n",
    "        \n",
    "        # 输出最佳超参数组合和对应的目标函数值\n",
    "        print(optimizer.max)\n",
    "        # 使用贝叶斯优化得到的最佳超参数来重新训练最佳模型\n",
    "        best_lr = optimizer.max['params']['lr']\n",
    "        best_hidden = optimizer.max['params']['hidden']\n",
    "        best_nb_heads = optimizer.max['params']['nb_heads']\n",
    "        best_dropout = optimizer.max['params']['dropout']\n",
    "        best_alpha = optimizer.max['params']['alpha']\n",
    "        best_weight_decay = optimizer.max['params']['weight_decay']\n",
    "\n",
    "        # 重新构建模型并训练\n",
    "        best_model = GAT(7, int(best_hidden), 1, best_dropout, best_alpha, int(best_nb_heads)).to(device)\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(train_inputs).to(device), torch.tensor(train_graph_inputs).to(device), torch.tensor(train_targets).to(device)), batch_size=50, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.tensor(val_inputs).to(device), torch.tensor(val_graph_inputs).to(device), torch.tensor(val_targets).to(device)), batch_size=50, shuffle=False)\n",
    "        best_model.fit(train_loader, val_loader, best_lr, best_weight_decay)\n",
    "\n",
    "        # 使用最佳模型进行预测\n",
    "        test_loader = DataLoader(TensorDataset(torch.tensor(test_inputs).to(device), torch.tensor(test_graph_inputs).to(device), torch.tensor(test_targets).to(device)), batch_size=50, shuffle=False)\n",
    "        predictions = best_model.test(test_loader)\n",
    "        new_predictions = np.array([item.detach().cpu().numpy() for item in predictions]).flatten()\n",
    "\n",
    "        # 计算最佳模型的MAPE\n",
    "        best_MAPE = mean_absolute_percentage_error(test_targets, new_predictions)\n",
    "        print('Best MAPE:', np.mean(best_MAPE))\n",
    "\n",
    "        # 保存最佳模型的参数\n",
    "        torch.save(best_model.state_dict(), f'models/gat_edge{stock_info}_{lag_bin}_{lag_day}_best_model.pt')\n",
    "\n",
    "        # 保存最佳结果到 CSV 文件\n",
    "        best_results = {\n",
    "            'testTargets': test_targets.tolist(),\n",
    "            'new_predictions': new_predictions.tolist(),\n",
    "            'MAPE': best_MAPE.tolist()\n",
    "        }\n",
    "        res_df = pd.DataFrame(best_results)\n",
    "        res_df.to_csv(f'./result/gat_edge{stock_info}_{lag_bin}_{lag_day}_best_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24562a-d621-42c5-aba8-0b5ed439c76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676ef83-75a9-44d2-8c98-f5c6b20e5939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
