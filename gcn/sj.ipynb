{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb45470-a70d-4d33-8ea7-bf9ff3c0cb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc81597-3239-412a-8684-91a63b881d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b55c6c29-af44-4cdc-af13-294e0da81ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.A = nn.Parameter(torch.zeros(size=(2*out_features, 16)))\n",
    "        nn.init.xavier_uniform_(self.A.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "      \n",
    "        E = []\n",
    "        for i in range(16):\n",
    "            a = self.A[:,i].unsqueeze(1)\n",
    "            e_col = torch.matmul(a_input.double(), a.double()).squeeze(3)[:,:,i]\n",
    "            E.append(e_col)\n",
    "\n",
    "        e = self.leakyrelu(torch.stack(E, dim=2))\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        #x = F.log_softmax(x, dim=2)[:, -1, :]\n",
    "        # print(x)\n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "\n",
    "\n",
    "    def fit(self,train_loader, val_loader,num_epochs=5000, patience=100):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0015,weight_decay=5e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        # 使用平滑的 L1 损失，也称为 Huber loss\n",
    "        # criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0 \n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs.squeeze(dim=1), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "            #     train_loss += loss.item()\n",
    "            # avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            #         val_loss += val_loss.item()\n",
    "            # avg_val_loss=val_loss / len(val_loader)\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "                \n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe95c98-b9ed-44b0-862d-a6a99b9215e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['300540_XSHE', '000046_XSHE', '300174_XSHE', '300133_XSHE', '603053_XSHG', '300263_XSHE', '603095_XSHG', '300343_XSHE', '002679_XSHE', '002282_XSHE', '000951_XSHE', '000998_XSHE', '000753_XSHE', '002841_XSHE', '300433_XSHE', '603359_XSHG', '600622_XSHG', '002882_XSHE']\n",
      ">>>>>>>>>>>>>>>>>>>>300540_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch [20/5000], Train Loss: 127437829441.1225, Val Loss: 29878993345.2005\n",
      "Epoch [40/5000], Train Loss: 49641341230.7724, Val Loss: 22733513988.8577\n",
      "Epoch [60/5000], Train Loss: 47546555643.5168, Val Loss: 30620584253.5149\n",
      "Epoch [80/5000], Train Loss: 69164987459.2881, Val Loss: 32354738431.1619\n",
      "Epoch [100/5000], Train Loss: 19970733912.3916, Val Loss: 31732936820.7914\n",
      "Early stopping at epoch 111\n",
      "\n",
      "Fold number: 0\n",
      "[7.80539094e+02 8.15184914e+02 7.59182276e+03 2.91767242e+04\n",
      " 6.24601393e+04 1.66352434e+04 5.85265285e+04 1.95418765e+03\n",
      " 5.61889757e+02 2.79617019e+04 3.53624656e+04 4.42072233e+04\n",
      " 2.64723111e+03 2.48655294e+02 8.97986506e+02 4.60130191e+03\n",
      " 6.83610274e+04 1.54506802e+03 8.21382526e+03 1.12691512e+03\n",
      " 1.26851261e+03 1.06731703e+05 1.43769271e+03 6.20392270e+03\n",
      " 5.34539951e+02 1.69218976e+05 3.44480850e+03 4.15638397e+03\n",
      " 9.31810996e+04 3.04299113e+03 2.87286304e+04 2.34678269e+03\n",
      " 2.03130103e+03 4.75801813e+02 1.36813866e+05 6.87326444e+04\n",
      " 3.81160840e+03 2.98133073e+04 5.36567963e+03 1.24286411e+04\n",
      " 1.23317345e+03 2.57890717e+04 1.62664163e+03 4.77887246e+04\n",
      " 9.15068959e+02 4.12768468e+03 2.22228410e+03 2.06453528e+03\n",
      " 1.64098796e+03 5.66495233e+03 2.17657715e+03 7.22490876e+03\n",
      " 3.29059226e+03 6.01849030e+04 6.21534849e+02 3.25481677e+04\n",
      " 8.76419904e+02 6.81747876e+02 1.84291368e+03 1.30098487e+03\n",
      " 4.76915411e+03 2.17916431e+02 3.72588039e+04 1.00912195e+05\n",
      " 9.81077628e+03 5.81043226e+02 3.03715949e+04 2.49880651e+04\n",
      " 2.32478851e+03 4.82986241e+04 4.09263252e+04 3.85806455e+03\n",
      " 1.26401935e+03 5.25760945e+02 1.02831877e+02 9.57663042e+02\n",
      " 1.41717180e+03 2.08163149e+04 1.85658119e+04 2.33714238e+03\n",
      " 1.54004913e+03 1.08533094e+03 4.59012345e+03 2.65171050e+04\n",
      " 5.75120941e+02 6.01432030e+02 8.20750251e+03 6.40405762e+02\n",
      " 2.55515480e+04 1.09084420e+04 1.11223735e+03 1.65238829e+04\n",
      " 7.40932954e+02 1.12205761e+03 1.99431079e+04 1.73928655e+03\n",
      " 3.97844703e+04 1.67888692e+03 7.41285095e+04 3.27811598e+04\n",
      " 4.59487101e+04 1.43831692e+03 9.56612702e+04 4.48579579e+03\n",
      " 8.20741228e+02 4.78025376e+04 1.72625756e+03 1.23376049e+03\n",
      " 5.06904192e+03 4.14335028e+04 1.52586501e+03 1.81734802e+03\n",
      " 1.59590492e+03 5.54830790e+04 1.22382621e+03 8.04143168e+02\n",
      " 2.31836342e+04 2.58725304e+03 3.54978443e+03 1.56373679e+03\n",
      " 1.64467766e+04 9.68281656e+02 5.63413269e+02 5.98777612e+02\n",
      " 1.03304627e+03 1.23698055e+04 8.76595644e+02 4.02905716e+03\n",
      " 1.93181821e+05 5.98623355e+02 3.50245510e+03 6.68353167e+04\n",
      " 7.71643230e+02 4.20334160e+05 8.97392825e+03 8.01509076e+02\n",
      " 1.21624732e+03 1.17200836e+03 6.43233280e+04 6.39029110e+02\n",
      " 5.76263106e+02 1.14296253e+04 1.72965601e+04 2.70306718e+04\n",
      " 1.03580336e+05 7.03516979e+03 1.71876213e+03 5.25348517e+04\n",
      " 7.84290523e+04 9.60956153e+02 3.36222744e+04 3.18627179e+03\n",
      " 5.60553845e+02 2.57247294e+04 3.34519120e+04 8.16890946e+04\n",
      " 2.35673237e+04 7.92521074e+02 7.61984219e+02 2.27107578e+05\n",
      " 9.68338243e+04 7.18508006e+02 2.73265892e+04 7.47869053e+03\n",
      " 2.86691110e+04 7.84862535e+02 4.79031156e+03 8.17149013e+02\n",
      " 1.73849798e+03 3.53450547e+04 1.36830847e+03 1.14395167e+03\n",
      " 1.24817522e+03 2.15145710e+03 6.95393149e+02 1.51447895e+03\n",
      " 3.81226016e+04 6.78961759e+02 3.63404320e+03 1.79847772e+03\n",
      " 8.14389336e+03 3.13724500e+03 2.09842519e+03 1.66207330e+04\n",
      " 4.54351442e+04 8.56764950e+02 7.62036160e+03 8.99875820e+02\n",
      " 2.42618076e+04 5.15993043e+03 3.13725164e+05 2.16101827e+02\n",
      " 9.14399149e+04 2.75564353e+03 1.23160118e+05 8.21867178e+04\n",
      " 2.86954108e+03 9.77303956e+02 3.63063859e+04 3.58858921e+03\n",
      " 1.77502711e+03 1.73510235e+05 6.77607523e+02 8.17564932e+03\n",
      " 8.34327057e+02 5.52646537e+04 5.69726292e+04 1.47816815e+03\n",
      " 4.11790330e+04 8.66478854e+03 1.82787815e+03 4.93504049e+02\n",
      " 8.27627501e+02 5.58052589e+03 5.87875016e+02 4.03417442e+05\n",
      " 1.26558711e+03 8.18808174e+03 1.84413174e+03 1.05808269e+05\n",
      " 3.67754516e+04 2.73902576e+04 3.07360575e+03 1.07380197e+03\n",
      " 5.27958636e+02 2.67786895e+03 8.75549587e+04 8.09571600e+02\n",
      " 1.96320089e+03 1.01064189e+04 2.97327244e+03 1.30815290e+03\n",
      " 2.15576033e+04 7.77872477e+03 7.69319302e+04 5.64644244e+03\n",
      " 5.62263428e+03 1.71046752e+03 5.17404728e+04 1.37529215e+03\n",
      " 2.11441716e+04 1.43528599e+04 1.14764768e+03 3.01922552e+05\n",
      " 1.26450592e+03 7.10637048e+02 5.41135640e+03 2.50804386e+03\n",
      " 2.48053407e+02 3.28758873e+04 2.10392246e+03 1.95211041e+04\n",
      " 7.95281143e+02 3.22998436e+03 1.02852060e+03 2.43861718e+03\n",
      " 4.36538093e+03 3.45530129e+03 9.02910162e+02 1.67045237e+03\n",
      " 6.08992125e+02 1.02990454e+03 1.41905470e+04 2.21087358e+05\n",
      " 9.11359992e+04 3.84245537e+04 3.17002800e+02 8.01396574e+02\n",
      " 7.08977048e+03 4.09103496e+03 1.77641368e+03 6.11066565e+04\n",
      " 9.91947684e+02 1.42384638e+05 1.60294848e+03 9.25895605e+02\n",
      " 2.46508460e+03 4.43212035e+04 2.01378883e+04 1.61972600e+03\n",
      " 9.33431736e+02 3.15070825e+04 1.87709866e+03 2.62790774e+03\n",
      " 1.78108211e+03 1.19987764e+05 1.24349722e+03 5.65144993e+03\n",
      " 3.23645664e+02 4.05623108e+04 1.74301341e+04 6.36430127e+03\n",
      " 2.60684460e+04 4.47631422e+04 2.00160076e+05 2.79641832e+03\n",
      " 4.30608892e+03 2.37123276e+05 5.41671220e+03 3.48362882e+02\n",
      " 1.24870861e+03 3.69958868e+02 1.12642964e+05 8.19184728e+03\n",
      " 9.27783029e+03 1.15943375e+03 8.03730557e+02 2.57330678e+03\n",
      " 4.32834858e+04 1.54114401e+05 5.64215124e+04 1.12363587e+03\n",
      " 3.36925796e+03 9.19967244e+04 6.77543759e+04 2.54220676e+03\n",
      " 1.39723694e+03 1.95836129e+03 9.56570877e+02 2.53970294e+04\n",
      " 5.91346411e+02 4.36633784e+03 9.68645002e+04 1.11599166e+05\n",
      " 3.16223749e+05 2.12494298e+04 3.79181152e+03 8.58930433e+02\n",
      " 4.31959730e+04 4.66082080e+03 3.23619365e+05 5.48255657e+04\n",
      " 2.46740725e+04 1.41288704e+03 3.81529477e+04 1.71345245e+05\n",
      " 2.77386324e+03 6.94748971e+03 1.10698029e+03 1.41028393e+03\n",
      " 2.33405570e+03 6.53230649e+02 1.66670525e+05 9.23014852e+02\n",
      " 9.81568544e+02 1.07200866e+05 2.86759717e+04 8.14360495e+03\n",
      " 2.18231159e+03 1.98530386e+03 1.41221631e+05 6.63078718e+03\n",
      " 3.65045883e+02 9.78793403e+02 5.21532016e+03 3.48056208e+03\n",
      " 7.99744121e+03 7.54604263e+02 7.24623083e+04 5.89999279e+03]\n",
      "[0.812822661198388]\n",
      "MAPE =  0.812822661198388\n",
      "Epoch [20/5000], Train Loss: 788587246207.8972, Val Loss: 8694141268.1886\n",
      "Epoch [40/5000], Train Loss: 68840269680.5347, Val Loss: 4852066966.1126\n",
      "Epoch [60/5000], Train Loss: 39022387484.5474, Val Loss: 9027063487.2127\n",
      "Epoch [80/5000], Train Loss: 58190748024.6543, Val Loss: 8735574961.9476\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 80\u001b[0m\n\u001b[1;32m     76\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m GAT(\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.6\u001b[39m,\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m predictions\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[1;32m     82\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/gat_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag_bin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag_day\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gcn_model_iteration_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 129\u001b[0m, in \u001b[0;36mGAT.fit\u001b[0;34m(self, train_loader, val_loader, num_epochs, patience)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, graph_input, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    128\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 129\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), targets)\n\u001b[1;32m    131\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 105\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)   \u001b[38;5;66;03m# dropout，防止过拟合\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([att(x, adj) \u001b[38;5;28;01mfor\u001b[39;00m att \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 将每个head得到的表示进行拼接\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)   \u001b[38;5;66;03m# dropout，防止过拟合\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_att(x, adj)   \u001b[38;5;66;03m# 输出并激活\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 105\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)   \u001b[38;5;66;03m# dropout，防止过拟合\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m att \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 将每个head得到的表示进行拼接\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)   \u001b[38;5;66;03m# dropout，防止过拟合\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_att(x, adj)   \u001b[38;5;66;03m# 输出并激活\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 50\u001b[0m, in \u001b[0;36mGraphAttentionLayer.forward\u001b[0;34m(self, inp, adj)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m):\n\u001b[1;32m     49\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA[:,i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     e_col \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)[:,:,i]\n\u001b[1;32m     51\u001b[0m     E\u001b[38;5;241m.\u001b[39mappend(e_col)\n\u001b[1;32m     53\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu(torch\u001b[38;5;241m.\u001b[39mstack(E, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/fx/traceback.py:51\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_stack\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_overridden:\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m current_stack\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # stock_info = sys.argv[0]\n",
    "    # lag_bin = int(sys.argv[1])\n",
    "    # lag_day = int(sys.argv[2])\n",
    "    # bin_num = int(sys.argv[3])\n",
    "    # random_state_here = int(sys.argv[4])\n",
    "    # test_set_size = float(sys.argv[5])\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = list(set(s.split('_25')[0] for s in files))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:2]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        mape_list = []\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(testInputs), torch.tensor(testgraphInput), torch.tensor(testTargets))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "        K = 5  # Number of folds\n",
    "        for k in range(K):\n",
    "            torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "            trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "            trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "            trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "            valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "            valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "            trainInputs = normalize(trainInputsAll[:, :])\n",
    "            valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "            # Assuming trainInputs, trainGraphInput, trainGraphFeatureInput, trainTargets are PyTorch tensors\n",
    "            train_dataset = TensorDataset(torch.tensor(trainInputs), torch.tensor(trainGraphInput),torch.tensor(trainTargets))\n",
    "            val_dataset = TensorDataset(torch.tensor(valInputs), torch.tensor(valGraphInput),torch.tensor(valTargets))\n",
    "\n",
    "            # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "            # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "        \n",
    "            model = GAT(7,8,1,0.6,0.3,8)\n",
    "            model.fit(train_loader, val_loader)\n",
    "            predictions=model.test(test_loader)\n",
    "            torch.save(model.state_dict(), f'models/gat_{stock_info}_{lag_bin}_{lag_day}_gcn_model_iteration_{k}.pt')\n",
    "    \n",
    "            print()\n",
    "            print('Fold number:', k)\n",
    "\n",
    "            new_predictions = np.array([item.detach().numpy() for item in predictions]).flatten()\n",
    "            MAPE = []\n",
    "\n",
    "            MAPE.append(mean_absolute_percentage_error(testTargets[:], new_predictions[:]))\n",
    "            print(new_predictions)\n",
    "            print(MAPE)\n",
    "            testTargets0 = list(testTargets)\n",
    "\n",
    "            res = {\n",
    "                'testTargets': testTargets0,\n",
    "                'new_predictions': new_predictions\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame(res)\n",
    "            res_df.to_csv(f'./result/gat_{stock_info}_{lag_bin}_{lag_day}_res_test_MAPE{k}.csv', index=False)\n",
    "\n",
    "            print('MAPE = ', np.array(MAPE).mean())\n",
    "            MAPE_mean = np.array(MAPE).mean()\n",
    "            mape_list.append(MAPE)\n",
    "\n",
    "        print('-')\n",
    "        print('mape score = ', mape_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d04ed0-901f-425c-9bb1-8bfc05d73cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
