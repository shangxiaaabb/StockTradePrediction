{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346d7663-efeb-468e-baad-ba380c3e5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71bae64-c20e-4b93-8e9c-20e6703af300",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cccd0ca3-033b-4444-b1b1-963d07f8f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"Graph Convolutional Network\"\"\"\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        reg_const=0.0001\n",
    "        self.gcn1 = GCNConv(dim_in, dim_h,weight_decay=reg_const,add_self_loops=True)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_h,weight_decay=reg_const,add_self_loops=True)\n",
    "        self.gcn3 = GCNConv(dim_h, dim_h,bias=True,weight_decay=reg_const,add_self_loops=True)\n",
    "        self.linear2 = torch.nn.Linear(dim_h, dim_out).double()        \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for layer in [self.gcn1, self.gcn2, self.gcn3, self.linear2]:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    init.zeros_(layer.bias)\n",
    "            elif isinstance(layer, GCNConv):\n",
    "                # For GCNConv, handle weight and bias separately\n",
    "                init.xavier_uniform_(layer.lin.weight)\n",
    "                if layer.lin.bias is not None:\n",
    "                    init.zeros_(layer.lin.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x, adj):\n",
    "#         x=x.double()\n",
    "#         edge_index = torch.stack([np.nonzero(t)for t in torch.unbind(adj,dim=0)],dim=0).permute(0,2,1)\n",
    "#         self.gcn1 = self.gcn1.to(torch.double)\n",
    "#         self.gcn2 = self.gcn2.to(torch.double)\n",
    "#         self.gcn3 = self.gcn3.to(torch.double)\n",
    "#         results = []\n",
    "#         for i in range(x.size(0)):\n",
    "#             h = self.gcn1(x[i], edge_index[i])\n",
    "#             h = torch.nn.LeakyReLU()(h)\n",
    "#             h = self.gcn2(h, edge_index[i])\n",
    "#             h = torch.nn.LeakyReLU()(h)\n",
    "#             h = self.gcn3(h, edge_index[i])\n",
    "#             h = torch.nn.LeakyReLU()(h)\n",
    "#             last_column = h[:, -1]\n",
    "#             conv1_new = last_column.unsqueeze(-1)\n",
    "#             #conv1_new = torch.nn.Flatten(conv1_new)\n",
    "#             #conv1_new = F.dropout(conv1_new,p=0.3)\n",
    "#             conv1_new = F.dropout(conv1_new.view(conv1_new.size(0), -1), p=0.3, training=self.training)        \n",
    "#             y_hat = self.linear2(conv1_new.T)\n",
    "#             results.append(y_hat)\n",
    "#         return torch.stack(results, dim=0)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = x.double()\n",
    "        edge_indices = []\n",
    "        for i in range(x.size(0)):\n",
    "            # 获取当前样本的 edge_index\n",
    "            edge_index = torch.nonzero(adj[i]).t().contiguous()\n",
    "            edge_indices.append(edge_index)\n",
    "\n",
    "        # 将 edge_index 列表堆叠为一个张量\n",
    "        edge_indices = torch.stack(edge_indices, dim=0)\n",
    "        print(edge_indices.shape)\n",
    "\n",
    "        # 在整个 batch 上应用 GCN\n",
    "        h = self.gcn1(x, edge_indices)\n",
    "        h = torch.nn.LeakyReLU()(h)\n",
    "        h = self.gcn2(h, edge_indices)\n",
    "        h = torch.nn.LeakyReLU()(h)\n",
    "        h = self.gcn3(h, edge_indices)\n",
    "        h = torch.nn.LeakyReLU()(h)\n",
    "\n",
    "        # 在最后一个维度上应用 dropout\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "\n",
    "        # 展平最后两个维度\n",
    "        conv1_new = h.view(h.size(0), -1)\n",
    "\n",
    "        # 应用线性层\n",
    "        y_hat = self.linear2(conv1_new.T)\n",
    "\n",
    "\n",
    "        #conv1_new = F.dropout(conv1_new.view(conv1_new.size(0), -1), p=0.3, training=self.training)\n",
    "\n",
    "        y_hat = self.linear2(conv1_new.T)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,train_loader, val_loader,num_epochs=5000, patience=50):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.RMSprop(self.parameters(), lr=0.00015,weight_decay=0.)\n",
    "        #criterion = nn.MSELoss()\n",
    "        # 使用平滑的 L1 损失，也称为 Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss += criterion(val_outputs, val_targets).item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}')\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return\n",
    "                \n",
    "                \n",
    "    def test(self,test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13076a9f-90ee-411e-83b2-0454921d17ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['603095_XSHG', '000046_XSHE', '600622_XSHG', '002282_XSHE', '002841_XSHE', '300263_XSHE', '300174_XSHE', '603359_XSHG', '000753_XSHE', '300343_XSHE', '000998_XSHE', '300540_XSHE', '300133_XSHE', '002882_XSHE', '000951_XSHE', '603053_XSHG', '300433_XSHE', '002679_XSHE']\n",
      ">>>>>>>>>>>>>>>>>>>>603095_XSHG>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2182843/740401852.py:41: RuntimeWarning: divide by zero encountered in log\n",
      "  eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([50, 2, 42])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m  model \u001b[38;5;241m=\u001b[39m GCN(\u001b[38;5;241m7\u001b[39m,valInputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     84\u001b[0m  \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 85\u001b[0m  \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m  predictions\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[1;32m     87\u001b[0m  \u001b[38;5;66;03m# Define optimizer and loss function\u001b[39;00m\n\u001b[1;32m     88\u001b[0m  \u001b[38;5;66;03m#optimizer = optim.RMSprop(model.parameters(), lr=0.00015)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# criterion = nn.MSELoss()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m  \u001b[38;5;66;03m# Save the PyTorch model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 101\u001b[0m, in \u001b[0;36mGCN.fit\u001b[0;34m(self, train_loader, val_loader, num_epochs, patience)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, graph_input, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    100\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 101\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m    103\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 63\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(edge_indices\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# 在整个 batch 上应用 GCN\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLeakyReLU()(h)\n\u001b[1;32m     65\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn2(h, edge_indices)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:176\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:61\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m     58\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 61\u001b[0m     edge_index, tmp_edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m tmp_edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m tmp_edge_weight\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch_geometric/utils/loop.py:298\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill_value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m     inv_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mmask\n\u001b[0;32m--> 298\u001b[0m     loop_attr[edge_index[\u001b[38;5;241m0\u001b[39m][inv_mask]] \u001b[38;5;241m=\u001b[39m \u001b[43medge_attr\u001b[49m\u001b[43m[\u001b[49m\u001b[43minv_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    300\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    302\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # stock_info = sys.argv[0]\n",
    "    # lag_bin = int(sys.argv[1])\n",
    "    # lag_day = int(sys.argv[2])\n",
    "    # bin_num = int(sys.argv[3])\n",
    "    # random_state_here = int(sys.argv[4])\n",
    "    # test_set_size = float(sys.argv[5])\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = list(set(s.split('_25')[0] for s in files))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:2]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)#.astype(np.float64)\n",
    "        #inputs_data = [[[float(x) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        #graph_input = np.nonzero(graph_input)\n",
    "        #graph_input = torch.tensor(graph_input,dtype=torch.float64)\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        mape_list = []\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(testInputs), torch.tensor(testgraphInput), torch.tensor(testTargets))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "        K = 5  # Number of folds\n",
    "        for k in range(K):\n",
    "            torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "            trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "            trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "            trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "            valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "            valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "            trainInputs = normalize(trainInputsAll[:, :])\n",
    "            valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "            # Assuming trainInputs, trainGraphInput, trainGraphFeatureInput, trainTargets are PyTorch tensors\n",
    "            train_dataset = TensorDataset(torch.tensor(trainInputs), torch.tensor(trainGraphInput),torch.tensor(trainTargets))\n",
    "            val_dataset = TensorDataset(torch.tensor(valInputs), torch.tensor(valGraphInput),torch.tensor(valTargets))\n",
    "\n",
    "            # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "            # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "            # Instantiate the PyTorch model\n",
    "        \n",
    "            model = GCN(7,valInputs.shape[1],1)\n",
    "            print()\n",
    "            model.fit(train_loader, val_loader)\n",
    "            predictions=model.test(test_loader)\n",
    "            # Define optimizer and loss function\n",
    "            #optimizer = optim.RMSprop(model.parameters(), lr=0.00015)\n",
    "           # criterion = nn.MSELoss()\n",
    "\n",
    "            # Train the model\n",
    "            # train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "            # Save the PyTorch model\n",
    "            torch.save(model.state_dict(), f'models/{stock_info}_{lag_bin}_{lag_day}_gcn_model_iteration_{k}.pt')\n",
    "            print(model.state_dict())\n",
    "            # Evaluate on the test set\n",
    "            # model.eval()\n",
    "            # with torch.no_grad():\n",
    "            #     predictions = []\n",
    "            #     for test_inputs, test_graph_input, test_graph_feature, _ in test_loader:\n",
    "            #         batch_predictions = model(test_inputs, test_graph_input, test_graph_feature)\n",
    "            #         predictions.append(batch_predictions)\n",
    "            #     predictions = torch.cat(predictions)\n",
    "\n",
    "            # Perform any necessary operations with the predictions\n",
    "            # ...\n",
    "            print()\n",
    "            print('Fold number:', k)\n",
    "            print('Total number of epochs ran =', len(history))  # This is a PyTorch equivalent\n",
    "\n",
    "            new_predictions = np.array(predictions)\n",
    "            print(type(new_predictions))\n",
    "            print(new_predictions.shape)\n",
    "            # new_predictions = new_predictions.squeeze().tolist()\n",
    "            new_predictions = [item for sublist in new_predictions for item in sublist]\n",
    "            MAPE = []\n",
    "\n",
    "            # MAPE.append(mean_absolute_percentage_error(trainTargets[:], new_predictions[:]))\n",
    "            # testTargets0 = [item for sublist in trainTargets for item in sublist]\n",
    "            print(testTargets)\n",
    "            print(new_predictions)\n",
    "            MAPE.append(mean_absolute_percentage_error(testTargets[:], new_predictions[:]))\n",
    "            print(MAPE)\n",
    "            # testTargets0 = [item for sublist in testTargets for item in sublist]\n",
    "            testTargets0 = list(testTargets)\n",
    "\n",
    "            res = {\n",
    "                'testTargets': testTargets0,\n",
    "                'new_predictions': new_predictions\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame(res)\n",
    "            res_df.to_csv(f'./result/{stock_info}_{lag_bin}_{lag_day}_res_test_MAPE{k}.csv', index=False)\n",
    "\n",
    "            print('MAPE = ', np.array(MAPE).mean())\n",
    "            MAPE_mean = np.array(MAPE).mean()\n",
    "            mape_list.append(MAPE)\n",
    "\n",
    "        print('-')\n",
    "        print('mape score = ', mape_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a4eb2-4c6f-4b71-b27f-bf65e3674129",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.randint(0,10,size=50*16*16).reshape([50,16,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1503d15a-4f3b-4ad1-bfee-fdaff6b32678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 600, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(torch.stack([np.nonzero(t)for t in torch.unbind( torch.randn(10,20,30),dim=0)],dim=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
