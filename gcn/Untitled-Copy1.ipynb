{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e99867e-b47b-4bd1-b701-6860aee8f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa7e249-b0e8-4daf-a41a-1a0302565d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "hyperparams = {\n",
    "    'lr': np.linspace(0.005, 0.03, 6),\n",
    "    'hidden':[6,7,8,9],\n",
    "    'nb_heads':[2,4,6,8],\n",
    "    'dropout': np.linspace(0.4, 0.8, 5),\n",
    "    'alpha':[0.01,0.05,0.1,0.15,0.5],\n",
    "    'weight_decay':[3e-4,4e-4,5e-4,6e-4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160f4062-acf2-4d2d-bb5e-8a3d4865f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.A = nn.Parameter(torch.zeros(size=(2*out_features, 16)))\n",
    "        nn.init.xavier_uniform_(self.A.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "      \n",
    "        E = [torch.matmul(a_input.double(), self.A[:,i].unsqueeze(1).double()).squeeze(3)[:,:,i] for i in range(N)]\n",
    "\n",
    "        e = self.leakyrelu(torch.stack(E, dim=2))\n",
    "        # print(e.shape)\n",
    "\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        # print(attention.shape)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        #x = F.log_softmax(x, dim=2)[:, -1, :]\n",
    "        # print(x)\n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self,train_loader, val_loader,lr_rate,w_d,num_epochs=10000,patience=100):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr_rate,weight_decay=w_d)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0 \n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs.squeeze(dim=1), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "            #     train_loss += loss.item()\n",
    "            # avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            #         val_loss += val_loss.item()\n",
    "            # avg_val_loss=val_loss / len(val_loader)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return\n",
    "                \n",
    "                \n",
    "    def test(self,test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110decbd-cfe6-4c03-934f-eab8843b749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000046_XSHE', '000753_XSHE', '000951_XSHE', '000998_XSHE', '002282_XSHE', '002679_XSHE', '002841_XSHE', '002882_XSHE', '300133_XSHE', '300174_XSHE', '300263_XSHE', '300343_XSHE', '300433_XSHE', '300540_XSHE', '600622_XSHG', '603053_XSHG', '603095_XSHG', '603359_XSHG']\n",
      ">>>>>>>>>>>>>>>>>>>>000046_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch [20/10000], Train Loss: 291539180195.4180, Val Loss: 528176185830.3896\n",
      "Epoch [40/10000], Train Loss: 744455438821.3120, Val Loss: 527599190913.1263\n",
      "Epoch [60/10000], Train Loss: 719233550810.8129, Val Loss: 527599190913.1263\n",
      "Epoch [80/10000], Train Loss: 194204479180.9997, Val Loss: 527599190913.1263\n",
      "Epoch [100/10000], Train Loss: 494757651862.6298, Val Loss: 527508501678.1952\n",
      "Epoch [120/10000], Train Loss: 488244485424.2845, Val Loss: 527599190913.1263\n",
      "Epoch [140/10000], Train Loss: 499079729530.7350, Val Loss: 527599190913.1263\n",
      "Early stopping at epoch 153\n",
      "MAPE= 1.0\n",
      "Epoch [20/10000], Train Loss: 756601716733.3746, Val Loss: 527599190913.1263\n",
      "Epoch [40/10000], Train Loss: 662165756733.8802, Val Loss: 527599190913.1263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# 训练和评估模型\u001b[39;00m\n\u001b[1;32m    107\u001b[0m model \u001b[38;5;241m=\u001b[39m GAT(\u001b[38;5;241m7\u001b[39m, hyperparams_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m, hyperparams_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m], hyperparams_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], hyperparams_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_heads\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 108\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhyperparams_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[1;32m    110\u001b[0m new_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m predictions])\u001b[38;5;241m.\u001b[39mflatten()\n",
      "Cell \u001b[0;32mIn[3], line 117\u001b[0m, in \u001b[0;36mGAT.fit\u001b[0;34m(self, train_loader, val_loader, lr_rate, w_d, num_epochs, patience)\u001b[0m\n\u001b[1;32m    115\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(inputs, graph_input)\n\u001b[1;32m    116\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), targets)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), max_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/bioinfo/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = sorted(list(set(s.split('_25')[0] for s in files)))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:1]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        \n",
    "        test_inputs_tensor = torch.tensor(testInputs).to(device)\n",
    "        test_graph_input_tensor = torch.tensor(testgraphInput).to(device)\n",
    "        test_targets_tensor = torch.tensor(testTargets).to(device)\n",
    "        \n",
    "        \n",
    "        test_dataset = TensorDataset(test_inputs_tensor, test_graph_input_tensor, test_targets_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "  \n",
    "        \n",
    "\n",
    "        # 随机搜索\n",
    "        best_hyperparams_per_fold = []  # 存储每个折的最佳参数\n",
    "        best_mape_per_fold = []  # 存储每个折的最佳MAPE\n",
    "\n",
    "        for k in range(K):\n",
    "            best_mape = float('inf')\n",
    "            best_hyperparams = {}\n",
    "            best_model=None\n",
    "\n",
    "            # 随机搜索\n",
    "            num_iterations = 10 \n",
    "            for i in range(num_iterations):\n",
    "                # 随机采样超参数组合\n",
    "                hyperparams_sample = {}\n",
    "                for key, value in hyperparams.items():\n",
    "                    hyperparams_sample[key] = np.random.choice(value)\n",
    "\n",
    "                torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "                trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "                trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "                trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "                valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "                valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "                trainInputs = normalize(trainInputsAll[:, :])\n",
    "                valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "                # Convert numpy arrays to PyTorch tensors and move them to GPU\n",
    "                train_inputs_tensor = torch.tensor(trainInputs).to(device)\n",
    "                train_graph_input_tensor = torch.tensor(trainGraphInput).to(device)\n",
    "                train_graph_feature_input_tensor = torch.tensor(trainGraphFeatureInput).to(device)\n",
    "                train_targets_tensor = torch.tensor(trainTargets).to(device)\n",
    "\n",
    "                val_inputs_tensor = torch.tensor(valInputs).to(device)\n",
    "                val_graph_input_tensor = torch.tensor(valGraphInput).to(device)\n",
    "                val_graph_feature_input_tensor = torch.tensor(valGraphFeatureInput).to(device)\n",
    "                val_targets_tensor = torch.tensor(valTargets).to(device)\n",
    "\n",
    "                # Create datasets and dataloaders\n",
    "                train_dataset = TensorDataset(train_inputs_tensor, train_graph_input_tensor, train_targets_tensor)\n",
    "                val_dataset = TensorDataset(val_inputs_tensor, val_graph_input_tensor, val_targets_tensor)\n",
    "              \n",
    "                # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "                # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "                train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "                # 训练和评估模型\n",
    "                model = GAT(7, hyperparams_sample['hidden'], 1, hyperparams_sample['dropout'], hyperparams_sample['alpha'], hyperparams_sample['nb_heads']).to(device)\n",
    "                model.fit(train_loader, val_loader, hyperparams_sample['lr'],hyperparams_sample['weight_decay'])\n",
    "                predictions = model.test(test_loader)\n",
    "                new_predictions = np.array([item.detach().cpu().numpy() for item in predictions]).flatten()\n",
    "                mape = mean_absolute_percentage_error(test_targets_tensor.cpu().numpy(), new_predictions)\n",
    "                print('MAPE=',mape)\n",
    "\n",
    "                # 更新每个折的最佳参数和MAPE\n",
    "                if mape < best_mape:\n",
    "                    best_model = model\n",
    "                    best_mape = mape\n",
    "                    best_hyperparams = hyperparams_sample\n",
    "                    best_results = {\n",
    "                        'testTargets': testTargets.tolist(),\n",
    "                        'new_predictions': new_predictions.tolist(),\n",
    "                        'MAPE': mape.tolist()\n",
    "                    }\n",
    "\n",
    "            # 保存最佳模型\n",
    "            torch.save(best_model.state_dict(), f'models/gat_{stock_info}_{lag_bin}_{lag_day}_gcn_model_{k}.pt')\n",
    "            res_df = pd.DataFrame(best_results)\n",
    "            res_df.to_csv(f'./result/gat_{stock_info}_{lag_bin}_{lag_day}_MAPE{k}.csv', index=False)\n",
    "            # 记录每个折的最佳参数和MAPE\n",
    "            best_hyperparams_per_fold.append(best_hyperparams)\n",
    "            best_mape_per_fold.append(best_mape)\n",
    "            print(f\"Fold {k + 1}: Best hyperparameters - {hyperparams_fold}, Best MAPE - {mape_fold}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abfc43-aa4f-4e14-b82e-f2a6965eb3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机搜索\n",
    "best_accuracy = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # 随机采样超参数组合\n",
    "    hyperparams_sample = {}\n",
    "    for key, value in hyperparams.items():\n",
    "        hyperparams_sample[key] = np.random.choice(value)\n",
    "\n",
    "    # 模型训练和评估\n",
    "    accuracy = train_and_evaluate(hyperparams_sample)\n",
    "\n",
    "    # 记录最优超参数组合\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hyperparams = hyperparams_sample\n",
    "\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298198cb-90b8-453f-88ff-e0bd65739b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # 释放不再需要的GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733e58e-6ae6-4e11-809b-0e8b21c14c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
