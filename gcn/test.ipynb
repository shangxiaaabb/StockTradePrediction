{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from data_loader import StockDataset\n",
    "from model.GHATModel import GAT\n",
    "from config import Config\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def build_adj():\n",
    "    # connection = [\n",
    "    # (1, 0),\n",
    "    # (9, 0), (12, 0), \n",
    "    # (8, 9), (8, 12), (5, 9), (11, 12), \n",
    "    # (4, 5), (4, 8), (7, 8), (7, 11), (10, 11),\n",
    "    # (3, 4), (3, 7), (6, 7), (6, 10), (2, 3), (2, 6)]\n",
    "    \n",
    "    # 无向图\n",
    "    connection = [\n",
    "        (1, 0), (0, 1),\n",
    "        (9, 0), (12, 0), (0, 9), (0, 12),\n",
    "        (8, 9), (8, 12), (5, 9), (11, 12), (9, 8), (12, 8), (9, 5), (12, 11),\n",
    "        (4, 5), (4, 8), (7, 8), (7, 11), (10, 11), (5, 4), (8, 4), (8, 7), (11, 7), (11, 10),\n",
    "        (3, 4), (3, 7), (6, 7), (6, 10), (2, 3), (2, 6), (4, 3), (7, 3), (7, 6), (10, 6), (3, 2), (6, 2)\n",
    "        ]\n",
    "    adj_matrix = torch.zeros(13, 13).float()\n",
    "    for source, target in connection:\n",
    "        adj_matrix[source][target] = 1\n",
    "    return adj_matrix\n",
    "\n",
    "large_market_cap_stocks = [\n",
    "    \"000951\", \"002841\", \"300133\", \"300343\", \"000998\", \"300433\",\n",
    "    \"601021\", \"603197\", \"300166\", \"600026\", \"000998\", \"600171\",\n",
    "    \"300917\", \"603087\", \"002309\", \"300451\", \"002549\", \"603466\"\n",
    "]\n",
    "\n",
    "medium_market_cap_stocks = [\n",
    "    \"300540\", \"603359\", \"000046\", \"300263\", \"002679\", \"603053\",\n",
    "    \"000403\", \"603306\", \"600970\", \"002703\", \"000931\", \"002186\",\n",
    "    \"300633\", \"603195\", \"300133\", \"600360\", \"600729\", \"603777\"\n",
    "]\n",
    "\n",
    "small_market_cap_stocks = [\n",
    "    \"300174\", \"603095\", \"000753\", \"600622\", \"002282\", \"002882\",\n",
    "    \"300912\", \"603926\", \"002451\", \"002672\", \"000551\", \"300758\",\n",
    "    \"001207\", \"300865\", \"002247\", \"002379\", \"300389\", \"300491\"\n",
    "]\n",
    "market_cap = {'large': large_market_cap_stocks, 'medium': medium_market_cap_stocks, 'small': small_market_cap_stocks}\n",
    "\n",
    "# 高流动股票代码列表（前六位）\n",
    "high_flow_stocks = [\n",
    "    \"300133\", \"300343\", \"000046\", \"300263\", \"000753\", \"600622\", \n",
    "    \"300166\", \"600026\", \"600970\", \"002703\", \"002451\", \"002672\", \n",
    "    \"002309\", \"300451\", \"300133\", \"600360\", \"002247\", \"002379\"\n",
    "]\n",
    "\n",
    "# 中流动股票代码列表（前六位）\n",
    "medium_flow_stocks = [\n",
    "    \"000998\", \"300433\", \"002679\", \"603053\", \"002282\", \"002882\", \n",
    "    \"000998\", \"600171\", \"000931\", \"002186\", \"000551\", \"300758\", \n",
    "    \"002549\", \"603466\", \"600729\", \"603777\", \"300389\", \"300491\"\n",
    "]\n",
    "\n",
    "# 低流动股票代码列表（前六位）\n",
    "low_flow_stocks = [\n",
    "    \"000951\", \"002841\", \"300540\", \"603359\", \"300174\", \"603095\", \n",
    "    \"601021\", \"603197\", \"000403\", \"603306\", \"300912\", \"603926\", \n",
    "    \"300917\", \"603087\", \"300633\", \"603195\", \"001207\", \"300865\"\n",
    "]\n",
    "flow_dict = {'high': high_flow_stocks, 'medium': medium_flow_stocks, 'low': low_flow_stocks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_pred_300263.csv com is 0.935416413980514\n",
      "model_pred_002882.csv com is 1.550863929888626\n",
      "model_pred_002841.csv com is -1.0267474556341747\n",
      "model_pred_002282.csv com is 3.756918435396213\n",
      "model_pred_300174.csv com is -0.4021384301654137\n",
      "model_pred_000998.csv com is -0.12983024485678749\n",
      "model_pred_000951.csv com is -1.3434334328673543\n",
      "model_pred_000046.csv com is -0.46476788085150367\n",
      "model_pred_300133.csv com is 0.8758025108458504\n",
      "model_pred_000753.csv com is 1.9249466546347918\n"
     ]
    }
   ],
   "source": [
    "pred_dir = './pred/'\n",
    "scaler_dir = './data/volume/0308/Scaler/'\n",
    "\n",
    "for path in os.listdir(pred_dir):\n",
    "    if path.endswith('.csv'):\n",
    "        date_suffix = path[-10:-4]\n",
    "        scaler_path = os.path.join(scaler_dir, f'{date_suffix}.m')\n",
    "        \n",
    "        stand = joblib.load(scaler_path)\n",
    "        data = pd.read_csv(os.path.join(pred_dir, path))\n",
    "        \n",
    "        data.iloc[:, 0] = stand.transform(data.iloc[:, 0].values.reshape(-1, 1)).flatten().astype(float)\n",
    "        data.iloc[:, 1] = stand.transform(data.iloc[:, 1].values.reshape(-1, 1)).flatten().astype(float)\n",
    "        \n",
    "        aps_value = np.abs(data.iloc[:, 0] - data.iloc[:, 1])\n",
    "        mape = np.mean(aps_value/data.iloc[:, 1])\n",
    "        # mean_difference = np.mean(np.abs(data.iloc[:, 0] - data.iloc[:, 1])/data.iloc[:, 1])\n",
    "        print(f\"{path} com is {mape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Tensorboard\n",
    "\n",
    "save_models/saved_models-64-0.2-MSELoss/000951/scalar 558.3517 0.0579\n",
    "\n",
    "save_models/saved_models-128-0.1-L1Loss/002882/scalar 0.6444 0.111  \n",
    "\n",
    "save_models/saved_models-32-0.1-L1Loss/000951/scalar  0.3645  0.1776\n",
    "\n",
    "save_models/saved_models-128-0.3-L1Loss/300263/scalar 5.4386  0.2057\n",
    "\n",
    "save_models/saved_models-32-0.1-L1Loss/300174/scalar  0.6048  0.2104  \n",
    "\n",
    "save_models/saved_models-128-0.1-L1Loss/000046/scalar  0.3891  0.215\n",
    "\n",
    "save_models/saved_models-32-0.1-L1Loss/300133/scalar 0.4245 0.2317\n",
    "\n",
    "save_models/saved_models-128-0.1-L1Loss/000753/scalar  1.1202  0.2362\n",
    "\n",
    "save_models/saved_models-32-0.1-L1Loss/000753/scalar  0.6045 0.2512\n",
    "\n",
    "save_models/saved_models-128-0.1-L1Loss/002841/scalar  0.4857  0.461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000951 com MAE is 0.17173053386621098, the MAPE is 2.518633466308861, the MSE is 0.5221717812411385\n",
      "002882 com MAE is 0.12423068589359212, the MAPE is 2.1046718006878993, the MSE is 0.26982633313342996\n",
      "300263 com MAE is 0.3505307760632711, the MAPE is 3.1935994183582874, the MSE is 0.6466603122536327\n",
      "300174 com MAE is 0.2546607789525313, the MAPE is 2.1168768878390187, the MSE is 0.7743779097974093\n",
      "000046 com MAE is 0.14571692178444937, the MAPE is 2.8024512486183615, the MSE is 0.46706784226470577\n",
      "300133 com MAE is 0.4677060045788819, the MAPE is 6.006919753169601, the MSE is 0.8068875438268603\n",
      "000753 com MAE is 0.3408899679655601, the MAPE is 4.5000158535369925, the MSE is 0.997200295958236\n",
      "002841 com MAE is 0.3678779471036437, the MAPE is 4.670743773679899, the MSE is 0.7112576372651055\n",
      "000998 com MAE is 0.11683616764812571, the MAPE is 2.3214763711409097, the MSE is 0.39026816087632676\n"
     ]
    }
   ],
   "source": [
    "model_path = pd.read_excel('./model.xlsx')\n",
    "path_dict = {}\n",
    "for path in model_path['path']:\n",
    "    for _ in os.listdir(path.split('/scalar')[0]):\n",
    "        if 'train' in _ and '.tar' not in _:\n",
    "            pt_path = os.path.join(path, _).replace('/scalar', '')\n",
    "            pt_path = f\"./{pt_path}\"\n",
    "            pred_path = f'./pred/model_pred_{_[:6]}.csv'\n",
    "            data_path = f'./data/volume/0308/Input/{_[:6]}_3_3_inputs.npy'\n",
    "            path_dict[_[:6]] = [pt_path, data_path, pred_path]\n",
    "\n",
    "for key, value in path_dict.items():\n",
    "    pt_path, data_path, pred_path = value[0], value[1], value[2]\n",
    "    \n",
    "    # load data\n",
    "    data = np.load(data_path, allow_pickle= True)\n",
    "    data = np.array([value[_] for item in data for value in item for _ in [0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype= np.float32).reshape(data.shape[0], 13, 9)\n",
    "    data = torch.from_numpy(data).to(device)\n",
    "\n",
    "    # load model\n",
    "    model = GAT(n_feat= len([0, 1, 2, 3, 4, 5, 6, 7, 8]), n_hid= 16, out_features= len([1]), \n",
    "                pred_length= 1, n_heads= 4)\n",
    "    model = model.to(device= device)\n",
    "\n",
    "    state_dict = torch.load(pt_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # model pred\n",
    "    model_pred = model(data, build_adj())\n",
    "    model_pred = model_pred.cpu().detach().numpy()\n",
    "\n",
    "    # to csv\n",
    "    pred_data = pd.read_csv(pred_path)\n",
    "    pred_data[f'{key}-pred'] = model_pred\n",
    "    pred_data.to_csv(f'./result/{key}.csv', index= None)\n",
    "\n",
    "    # com model performance\n",
    "    non_zero_mask = pred_data[f'{key}-true'] != 0\n",
    "    mape = np.mean(np.abs((pred_data[f'{key}-true'][non_zero_mask]- pred_data[f'{key}-pred'][non_zero_mask])/ pred_data[f'{key}-true'][non_zero_mask]))\n",
    "    # mape = np.mean(np.abs((pred_data[f'{key}-true']- pred_data[f'{key}-pred'])/ (pred_data[f'{key}-true']+ 1e-8)))\n",
    "    mae = np.mean(pred_data[f'{key}-true']- pred_data[f'{key}-pred'])\n",
    "    mse = np.mean((pred_data[f'{key}-true']- pred_data[f'{key}-pred'])** 2)\n",
    "\n",
    "    print(f\"{key} com MAE is {mae}, the MAPE is {mape}, the MSE is {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| stock |  MAE   | MAPE | MSE  |\n",
    "| :--:  | :--:   | :--: | :--: |\n",
    "| 000046 | 0.067 | 2.784| 0.444|\n",
    "| 002882 | 0.094 | 4.669| 0.710|\n",
    "| 000951 | 0.194 | 2.633| 0.568|\n",
    "| 300263 | 0.330 | 2.967| 0.609|\n",
    "| 300174 | 0.229 | 2.031| 0.746|\n",
    "| 300133 | 0.430 | 5.966| 0.785|\n",
    "| 000753 | 0.357 | 4.565| 1.047|\n",
    "| 002841 | 0.369 | 4.688| 0.713|\n",
    "| 000998 | 0.091 | 2.411| 0.367|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000753 com MAE is 0.3568296744179487, the MAPE is 4.565257975045892, the MSE is 1.0468893311222307\n",
      "300263 com MAE is 0.3301482182794872, the MAPE is 2.966666300469162, the MSE is 0.6086557043995929\n",
      "000998 com MAE is 0.0915205719877487, the MAPE is 2.4114674667919633, the MSE is 0.36674533100505274\n",
      "000951 com MAE is 0.19498004116868423, the MAPE is 2.6327771803610047, the MSE is 0.5684222432400793\n",
      "300133 com MAE is 0.4397617898415026, the MAPE is 5.966244515032575, the MSE is 0.7851925197273011\n",
      "000046 com MAE is 0.06712500368747397, the MAPE is 2.7842108061709787, the MSE is 0.4440601307134777\n",
      "002882 com MAE is 0.09368558017979427, the MAPE is 2.097611695986554, the MSE is 0.3116063670139982\n",
      "002841 com MAE is 0.3670204628341969, the MAPE is 4.669078104983375, the MSE is 0.7103900892096832\n",
      "300174 com MAE is 0.22933013198451282, the MAPE is 2.030887703447141, the MSE is 0.7463646253501821\n"
     ]
    }
   ],
   "source": [
    "result = './end_result/'\n",
    "for data_path in os.listdir(result):\n",
    "    pred_data = pd.read_csv(f'{result}{data_path}')\n",
    "    key = data_path.split('.csv')[0]\n",
    "    # com model performance\n",
    "    non_zero_mask = pred_data[f'{key}-true'] != 0\n",
    "    mape = np.mean(np.abs((pred_data[f'{key}-true'][non_zero_mask]- pred_data[f'{key}-pred'][non_zero_mask])/ (pred_data[f'{key}-true'][non_zero_mask])))\n",
    "    mae = np.mean(pred_data[f'{key}-true']- pred_data[f'{key}-pred'])\n",
    "    mse = np.mean((pred_data[f'{key}-true']- pred_data[f'{key}-pred'])** 2)\n",
    "\n",
    "    print(f\"{key} com MAE is {mae}, the MAPE is {mape}, the MSE is {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300433_XSHE_25_daily.csv the corr is -0.05018412780538093\n",
      "000951_XSHE_25_daily.csv the corr is 0.006499070456356722\n",
      "000046_XSHE_25_daily.csv the corr is -0.025292199108096332\n",
      "000998_XSHE_25_daily.csv the corr is -0.06417301369701807\n",
      "002679_XSHE_25_daily.csv the corr is -0.04395794009055617\n",
      "300263_XSHE_25_daily.csv the corr is -0.044277313292397036\n",
      "300540_XSHE_25_daily.csv the corr is 0.01619292344657734\n",
      "603053_XSHG_25_daily.csv the corr is -0.03826706361246143\n",
      "600622_XSHG_25_daily.csv the corr is -0.02819947489846059\n",
      "300174_XSHE_25_daily.csv the corr is -0.03089981921673612\n",
      "002882_XSHE_25_daily.csv the corr is -0.024562945968328902\n",
      "000753_XSHE_25_daily.csv the corr is -0.012347011070974035\n",
      "603359_XSHG_25_daily.csv the corr is -0.007381870227684722\n",
      "300133_XSHE_25_daily.csv the corr is -0.023778421675317782\n",
      "002282_XSHE_25_daily.csv the corr is -0.015833455705401456\n",
      "002841_XSHE_25_daily.csv the corr is -0.02189742569235223\n",
      "300343_XSHE_25_daily.csv the corr is -0.026467016324222114\n",
      "603095_XSHG_25_daily.csv the corr is -0.01751676568552221\n"
     ]
    }
   ],
   "source": [
    "path_dir = './data/0308/0308-data/'\n",
    "path_list = os.listdir(path_dir)\n",
    "for path in path_list:\n",
    "    test = pd.read_csv(f'{path_dir}{path}')\n",
    "    corr_value = test.iloc[:, 1:].corr()\n",
    "    print(f'{path} the corr is {corr_value.iloc[0, 4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_low = pd.read_csv('./data/0308/0308-data/000046_XSHE_25_daily.csv')\n",
    "data_medium = pd.read_csv('./data/0308/0308-data/002882_XSHE_25_daily.csv')\n",
    "data_heigh = pd.read_csv('./data/0308/0308-data/000998_XSHE_25_daily.csv')\n",
    "data_analysis = {'low': data_low, 'medium': data_medium, 'heigh': data_heigh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox(data_dict: dict):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(data_dict), figsize=(12, 5))\n",
    "    for i, (key, value) in enumerate(data_dict.items()):\n",
    "        sns.boxplot(value['daily_volume'], ax= axes[i])\n",
    "        axes[i].set_xlabel(f'{key}')\n",
    "        axes[i].set_ylabel('daily_volume')\n",
    "    # 调整子图之间的间距\n",
    "    plt.tight_layout()\n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "\n",
    "def com_return(path_dir: str):\n",
    "    path_list = os.listdir(path_dir)\n",
    "    with tqdm(total=len(path_list)) as pbar:\n",
    "        for path in path_list:\n",
    "            file_path = os.path.join(path_dir, path)\n",
    "            data = pd.read_excel(file_path)\n",
    "            # data['收益率'] = data['收盘点位'].pct_change()\n",
    "            data['日收益率'] = (data['收盘价'] - data['昨收价']) / data['昨收价'] * 100\n",
    "            data = data.dropna()\n",
    "\n",
    "            data.to_excel(f\"./StockData/{path.split('.')[0]}.xlsx\")\n",
    "            pbar.update(1)\n",
    "\n",
    "def com_stock_return(file_path_target: str, file_path_all: str):\n",
    "    market_dict = {}\n",
    "    for root, dirs, files in os.walk(file_path_target):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                target_path = os.path.join(root, file)\n",
    "                test1 = pd.read_csv(target_path)\n",
    "                end, start = str(test1['time'].max())[:8], str(test1['time'].min())[:8]\n",
    "                end, start = int(end), int(start)\n",
    "\n",
    "                stock_info = target_path.split('_')[1][-6:]\n",
    "\n",
    "                # get com return data path\n",
    "                if os.path.exists(os.path.join(file_path_all, f\"{stock_info}.SZ.xls\")):\n",
    "                    path_used = os.path.join(file_path_all, f\"{stock_info}.SZ.xls\")\n",
    "                else:\n",
    "                    path_used = os.path.join(file_path_all, f\"{stock_info}.SH.xls\")\n",
    "\n",
    "                # open file\n",
    "                try:\n",
    "                    df = pd.read_excel(path_used)\n",
    "\n",
    "                    # com daily_return and yead_return\n",
    "                    daily_return = (df['收盘价'] - df['昨收价']) / df['昨收价'] * 100\n",
    "                    year_return = daily_return.std()* np.sqrt(252)\n",
    "                    # print(f\"The {stock_info} Annualized Volatility {year_return}\")\n",
    "                    market_dict[stock_info] = year_return\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return market_dict\n",
    "\n",
    "# plot_bbox(data_dict= data_analysis)\n",
    "# com_return(path_dir= './stockData/allstock/')\n",
    "\n",
    "\n",
    "market_return = com_stock_return(file_path_target= './data/raw_data/', file_path_all= './stockData/allstock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000403': 83.94171399855212,\n",
       " '600171': 49.431082769848935,\n",
       " '000551': 46.50375391336568,\n",
       " '002703': 246.06839649718341,\n",
       " '000998': 68.47825078391368,\n",
       " '002672': 41.42633479227145,\n",
       " '600026': 49.93037604196431,\n",
       " '000931': 711.7914954246801,\n",
       " '600970': 59.77701983835026,\n",
       " '603306': 49.6107296401739,\n",
       " '002186': 88.21341202517382,\n",
       " '300758': 66.66853128226039,\n",
       " '601021': 46.600327475973614,\n",
       " '300166': 58.26820108949775,\n",
       " '603197': 58.51819068581007,\n",
       " '603926': 48.08719028720553,\n",
       " '002451': 59.7939809524463,\n",
       " '300633': 58.063514324622176,\n",
       " '300389': 57.66784187527512,\n",
       " '600360': 64.38524516755488,\n",
       " '600729': 40.57379679498191,\n",
       " '603195': 87.85985838233813,\n",
       " '002247': 50.5248307861731,\n",
       " '002379': 50.15308049963824,\n",
       " '603466': 59.71602967430237,\n",
       " '300451': 68.05646239340707,\n",
       " '002309': 44.45894984085891,\n",
       " '300491': 66.12479336938773,\n",
       " '300133': 53.877745104515355,\n",
       " '603777': 59.42841792615807,\n",
       " '603087': 181.59597335111425,\n",
       " '002549': 50.55504593970418,\n",
       " '300433': 61.93785048792762,\n",
       " '000951': 51.66226411810109,\n",
       " '000046': 47.05557654752438,\n",
       " '002679': 57.31379553203301,\n",
       " '300263': 52.404450133453096,\n",
       " '300540': 60.176878569756205,\n",
       " '603053': 77.63344371829432,\n",
       " '600622': 48.38823727448727,\n",
       " '300174': 50.51133548968239,\n",
       " '002882': 50.581170107072545,\n",
       " '000753': 44.25366753338578,\n",
       " '603359': 48.63193625287926,\n",
       " '002282': 51.8801265961952,\n",
       " '002841': 49.23137771789938,\n",
       " '300343': 59.716901180533974,\n",
       " '603095': 97.87555783578242}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "import numpy as np\n",
    "import ast\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_shape(matrix, target_shape=(24, 7)):\n",
    "    # 获取当前矩阵的形状\n",
    "    current_shape = matrix.shape\n",
    "    \n",
    "    # 计算在每个维度上需要添加多少个0\n",
    "    padding = [(max(0, ts - cs), 0) for cs, ts in zip(current_shape, target_shape)]\n",
    "    \n",
    "    # 使用np.pad进行填充，'constant'表示用常数进行填充，默认为0\n",
    "    padded_matrix = np.pad(matrix, padding, mode='constant')\n",
    "    \n",
    "    return padded_matrix\n",
    "\n",
    "def convert_string(s):\n",
    "    s = s.strip(\"[]' \")\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "def convert_string_to_list(data: pd.DataFrame, time_length: int=6, bin_length: int=4, padding: str=True):\n",
    "    tmp_list = []\n",
    "    for item in data.values:\n",
    "        for value in item:\n",
    "            tmp_list.append(list(convert_string(value)))\n",
    "    tmp_list = np.array(tmp_list, dtype=np.float32)\n",
    "    \n",
    "    # if tmp_list.shape[0] < time_length * bin_length and padding:\n",
    "    #     tmp_list = pad_to_shape(tmp_list, target_shape=(time_length * bin_length, 7))\n",
    "    return tmp_list\n",
    "\n",
    "def build_training_data(input_path: str, time_length: int=6, bin_length: int=4, pred_length: int=1, way: str='None'):\n",
    "    data = pd.read_csv(input_path).iloc[:, 1:]\n",
    "    \n",
    "    train_features = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    gnn_data, lstm_data, target_data = [], [], []\n",
    "    \n",
    "    # 构建输入数据\n",
    "    for i in range(time_length, data.shape[0], 1):  # 行\n",
    "        for j in range(bin_length, data.shape[1]-pred_length):  # 列\n",
    "            if j < bin_length:\n",
    "                tmp_matrix = convert_string_to_list(data.iloc[i-time_length:i, :j+1], padding= True)\n",
    "            else:\n",
    "                tmp_matrix = convert_string_to_list(data.iloc[i-time_length:i, j-bin_length:j], padding= True)\n",
    "            \n",
    "            gnn_data.append(tmp_matrix)\n",
    "\n",
    "            lstm_data_tmp = pd.concat([data.iloc[i-2, j:data.shape[1]], data.iloc[i-1, :j]], axis=0)\n",
    "            for value in lstm_data_tmp:\n",
    "                lstm_data.append(convert_string(value))\n",
    "\n",
    "            # lstm_data.append(pd.concat([data.iloc[i-2, j:data.shape[1]], data.iloc[i-1, :j]], axis=0).values)\n",
    "            target_data.append(convert_string(data.iloc[i-1, j]))\n",
    "\n",
    "    # 将 lstm_data 转换为 NumPy 数组\n",
    "    gnn_data = np.array(gnn_data, dtype= np.float32)\n",
    "    lstm_data = np.array(lstm_data, dtype=np.float32)\n",
    "    lstm_data.resize((gnn_data.shape))\n",
    "    target_data = np.array(target_data, dtype= np.float32)\n",
    "\n",
    "    return gnn_data, lstm_data, target_data\n",
    "\n",
    "class StockDataDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                gnn_data: np.array,\n",
    "                lstm_data: np.array,\n",
    "                target_data: np.array,\n",
    "                train_features: list= [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "                pred_features: list= [0, 1, 2, 3, 4, 5, 6, 7, 8]):\n",
    "        super().__init__()\n",
    "        # data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.lstm_data = lstm_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "        # data features\n",
    "        self.train_features = train_features\n",
    "        self.pred_features = pred_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.gnn_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        先去把数据按照顺序切分好,然后根据index去找到切片\n",
    "        \"\"\"\n",
    "        return self.gnn_data[index], self.lstm_data[index], self.target_data[index]\n",
    "\n",
    "def gen_adjmatrix(time_length: int = 6, bin_length: int = 4, direct: bool = False) -> np.ndarray:\n",
    "    num_nodes = time_length * bin_length\n",
    "    adjacency_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n",
    "    for i in range(num_nodes):\n",
    "        row, col = divmod(i, bin_length)\n",
    "        # 右侧邻居\n",
    "        if col < bin_length - 1:\n",
    "            right_neighbor = i + 1\n",
    "            adjacency_matrix[i, right_neighbor] = 1\n",
    "            if not direct:\n",
    "                adjacency_matrix[right_neighbor, i] = 1\n",
    "        # 下方邻居\n",
    "        if row < time_length - 1:\n",
    "            bottom_neighbor = i + bin_length\n",
    "            adjacency_matrix[i, bottom_neighbor] = 1\n",
    "            if not direct:\n",
    "                adjacency_matrix[bottom_neighbor, i] = 1\n",
    "    return adjacency_matrix\n",
    "\n",
    "def draw_grid_with_features(adjacency_matrix, time_length, bin_length, features, feature_index=0):\n",
    "    G = nx.DiGraph() if np.any(adjacency_matrix != adjacency_matrix.T) else nx.Graph()\n",
    "    num_nodes = time_length * bin_length\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "    # 添加边\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adjacency_matrix[i, j] == 1:\n",
    "                G.add_edge(i, j)\n",
    "\n",
    "    # 设置布局\n",
    "    pos = {i: (i % bin_length, time_length - 1 - i // bin_length) for i in range(num_nodes)}\n",
    "    \n",
    "    # 绘制图，并在节点上显示选定的特征值\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    labels = {i: f'{i}\\n{features[i, feature_index]:.2f}' for i in range(num_nodes)}  # 创建带有选定特征值的标签\n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=500, font_size=10, edge_color='gray', arrowsize=20, arrowstyle='-|>', connectionstyle='arc3,rad=0.1', labels=labels)\n",
    "    plt.title('6x4 Grid Graph with Selected Feature')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成邻接矩阵\n",
    "time_length = 6\n",
    "bin_length = 4\n",
    "direct = True  # 设置为 True 表示有向图，设置为 False 表示无向图\n",
    "adj_matrix = gen_adjmatrix(time_length, bin_length, direct)\n",
    "\n",
    "# np.random.seed(0)  # 为了可重复性\n",
    "# features = gnn_data[0]\n",
    "\n",
    "# 绘制图并加上特征值（例如，选择第一个特征值）\n",
    "# draw_grid_with_features(adj_matrix, time_length, bin_length, features, feature_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000046\n",
      "Mean Squared Error (MSE): 0.34640854597091675\n",
      "Root Mean Squared Error (RMSE): 0.5885648131370544\n",
      "Mean Absolute Error (MAE): 0.3114861249923706\n",
      "R-squared (R²): 0.996669590473175\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from new_model.GHAT import PredModel\n",
    "from config import Config\n",
    "\n",
    "config = Config()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_path_dict = {'000046': ('./data/volume/0308/Features/000046_25_daily_f_all.csv', '/stock-trade-pred/gcn/saved_models/000046/model_best.pt'),\n",
    "                  '000753': ('./data/volume/0308/Features/000753_25_daily_f_all.csv', '/stock-trade-pred/gcn/saved_models/000753/model_best.pt'),\n",
    "                  '000951': ('./data/volume/0308/Features/000951_25_daily_f_all.csv', '/stock-trade-pred/gcn/saved_models/000951/model_best.pt'),\n",
    "                  '002882': ('./data/volume/0308/Features/002882_25_daily_f_all.csv', '/stock-trade-pred/gcn/saved_models/002882/model_best.pt'),\n",
    "                  '300174': ('./data/volume/0308/Features/300174_25_daily_f_all.csv', '/stock-trade-pred/gcn/saved_models/300174/model_best.pt'),\n",
    "                  '300263': ('./data/volume/0308/Features/300263_25_daily_f_all.csv', '/stock-trade-pred/gcn/saved_models/300263/model_best.pt'),}\n",
    "\n",
    "for i, (stock_num, (data_path, model_pt)) in enumerate(data_path_dict.items()):\n",
    "    if stock_num == '000046':\n",
    "        print(f'{stock_num}')\n",
    "        gnn_data, lstm_data, target_data = build_training_data(input_path= data_path)\n",
    "        data = pd.read_csv(data_path)\n",
    "        model = PredModel(config.in_features, config.out_features, config.embed_dim,\n",
    "                            config.ff_dim, config.n_heads, config.n_nodes, config.n_layers).to(device)\n",
    "        model.load_state_dict(torch.load(model_pt))\n",
    "        model.eval()\n",
    "\n",
    "        # 生成邻接矩阵\n",
    "        time_length = 6\n",
    "        bin_length = 4\n",
    "        direct = True  # 设置为 True 表示有向图，设置为 False 表示无向图\n",
    "        adj_matrix = gen_adjmatrix(time_length, bin_length, direct)\n",
    "\n",
    "        test_gnn = torch.from_numpy(gnn_data).to(device)\n",
    "        test_lstm = torch.from_numpy(lstm_data).to(device)\n",
    "        adj_matrix = torch.from_numpy(adj_matrix).to(device)\n",
    "        pred = model(test_lstm, test_gnn, adj_matrix)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "\n",
    "        mse = mean_squared_error(target_data[:, 1], pred)\n",
    "        print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(target_data[:, 1], pred))\n",
    "        print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "        mae = mean_absolute_error(target_data[:, 1], pred)\n",
    "        print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "        r2 = r2_score(target_data[:, 1], pred)\n",
    "        print(f'R-squared (R²): {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.57215   ],\n",
       "       [10.664997  ],\n",
       "       [11.716411  ],\n",
       "       ...,\n",
       "       [-0.17429066],\n",
       "       [-0.57298136],\n",
       "       [-0.9215288 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.1711],\n",
       "        [4.1734],\n",
       "        [4.1695],\n",
       "        ...,\n",
       "        [4.0745],\n",
       "        [4.0793],\n",
       "        [4.0753]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4010],\n",
       "        [ 6.4916],\n",
       "        [ 7.5469],\n",
       "        ...,\n",
       "        [-4.2488],\n",
       "        [-4.6523],\n",
       "        [-4.9969]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选中的实验条件组合是: {'A': 'A1', 'B': 'B2', 'C': 'C1', 'D': 'D1', 'E': 'E1'}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "conditions = {\n",
    "    'A': ['A1', 'A2'],\n",
    "    'B': ['B1', 'B2', 'B3'],\n",
    "    'C': ['C1', 'C2'],\n",
    "    'D': ['D1'],\n",
    "    'E': ['E1', 'E2']\n",
    "}\n",
    "all_combinations = list(itertools.product(*conditions.values()))\n",
    "random_combination = random.choice(all_combinations)\n",
    "formatted_combination = dict(zip(conditions.keys(), random_combination))\n",
    "print(\"随机选中的实验条件组合是:\", formatted_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
