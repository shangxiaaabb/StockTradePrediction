{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e99867e-b47b-4bd1-b701-6860aee8f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160f4062-acf2-4d2d-bb5e-8a3d4865f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\")\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.A = nn.Parameter(torch.zeros(size=(2*out_features, 16)))\n",
    "        nn.init.xavier_uniform_(self.A.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "      \n",
    "        E = [torch.matmul(a_input.double(), self.A[:,i].unsqueeze(1).double()).squeeze(3)[:,:,i] for i in range(N)]\n",
    "\n",
    "        e = self.leakyrelu(torch.stack(E, dim=2))\n",
    "        # print(e.shape)\n",
    "\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        # print(attention.shape)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        #x = F.log_softmax(x, dim=2)[:, -1, :]\n",
    "        # print(x)\n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self,train_loader, val_loader,num_epochs=5000,patience=100):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0015,weight_decay=5e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0 \n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs.squeeze(dim=1), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "            #     train_loss += loss.item()\n",
    "            # avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            #         val_loss += val_loss.item()\n",
    "            # avg_val_loss=val_loss / len(val_loader)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return\n",
    "                \n",
    "                \n",
    "    def test(self,test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110decbd-cfe6-4c03-934f-eab8843b749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000046_XSHE', '000753_XSHE', '000951_XSHE', '000998_XSHE', '002282_XSHE', '002679_XSHE', '002841_XSHE', '002882_XSHE', '300133_XSHE', '300174_XSHE', '300263_XSHE', '300343_XSHE', '300433_XSHE', '300540_XSHE', '600622_XSHG', '603053_XSHG', '603095_XSHG', '603359_XSHG']\n",
      ">>>>>>>>>>>>>>>>>>>>000046_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch [20/5000], Train Loss: 255475853519.6974, Val Loss: 224267717496.7388\n",
      "Epoch [40/5000], Train Loss: 585450297842.5344, Val Loss: 225218907260.2716\n",
      "Epoch [60/5000], Train Loss: 115711627820.4839, Val Loss: 203297116670.4182\n",
      "Epoch [80/5000], Train Loss: 148984065810.5049, Val Loss: 444513662999.6833\n",
      "Epoch [100/5000], Train Loss: 228027186137.0814, Val Loss: 433486229471.5841\n",
      "Epoch [120/5000], Train Loss: 391125016288.5026, Val Loss: 272652472054.1885\n",
      "Early stopping at epoch 121\n",
      "\n",
      "Fold number: 0\n",
      "[ 372191.17536087  502880.05518598   32137.55582965  278140.73998322\n",
      "   16924.08737874   30076.05823944  549994.7587975   263834.31570079\n",
      "   27058.20769687   81948.73537752  329898.03170041  366929.88205225\n",
      "  408630.95510781  306289.15013003  478192.39575882 1752698.11692621\n",
      "  321984.53116852  205433.73152683   25755.45826196  427324.15120525\n",
      "   53359.80508517  188959.81975255  256008.2189135   180453.03207402\n",
      "   22500.89738743  173745.10239623  124149.51437896  226817.61624292\n",
      "  438245.14521011    9449.06294123  202077.51533152   13930.89832808\n",
      "  197558.69334495   22261.43218373  227122.94271218  665613.24552736\n",
      "  131203.0137724    20461.40260087  233299.73310284  324401.00261265\n",
      "  156192.96995824  280459.14414893   19783.43384275  267462.84633707\n",
      "  128883.90701292  186618.38811569  139189.48039426    6817.9671021\n",
      "   23170.9427196   368138.93385057    5322.38921895  213039.70394665\n",
      "  355091.54657918  250308.70774818  318609.1245489   164816.01812791\n",
      "   23678.51307601  303329.39933014  182409.61549382  198439.2701286\n",
      "   15050.8974695    13279.17691604   19578.26307574  369285.42911764\n",
      "  198787.59585994  219814.63340421   65481.62617832  325910.65998064\n",
      "  438569.88895875  221511.42139232    7427.86372686  458702.90454064\n",
      "   23871.03463922  174133.05851882   15326.36192358   39187.33103922\n",
      "   99447.32061741  375732.6195161   219540.51876582   10439.49720864\n",
      "   19750.99617081  126439.80797475   16720.85037213   15325.19980367\n",
      "  137119.59988607   12501.73220319 1926902.3129189     6511.94523454\n",
      "  288136.25843002 1177895.00393144  166033.33287994  174972.07564587\n",
      "  107693.97292358   16248.22245419   10297.71434048  263657.26007312\n",
      "  230653.88983381   14709.66635735  250698.19533703  370452.57709829\n",
      "  519533.62890676  561680.28423434   24884.89405306   12406.8782587\n",
      "   73775.06789      13790.6478968    12743.34240182  331065.55293365\n",
      "   26338.1532038   248182.53636482    5589.62031164  473593.18670203\n",
      "    9484.6397516    24621.58778444   14081.40973048   15140.64302443\n",
      "   29668.78084303  485790.41402162  191433.68607388   59376.32272773\n",
      "   11170.06679307  132788.57793125  118278.01209777   19003.34863543\n",
      "   15600.5775276    32977.57768985  556571.96751094   10400.01251906\n",
      "  236039.75265682  276944.84907322  251008.52701848  131204.91292768\n",
      "  381699.83712789   13097.25847396  532417.21037855  162332.70119307\n",
      "   16055.01602535  612978.8439397   524082.73867363   14008.81151439\n",
      "  143178.46490371  764374.49066088   15274.17959786  295361.94095489\n",
      "   10509.83344091  266799.83260117  228905.50931639  586797.3725551\n",
      "  232072.14621859  217872.78103591  111981.78163407  224816.11424324\n",
      "  379170.58341053   11103.61955627  285684.83796963   85858.49037169\n",
      "  220668.03425538   30911.46275782   10523.35910492   20148.75881804\n",
      "  260764.18582986   12330.74580483  285013.95874965   27360.07671965\n",
      "   27455.40924569   54413.25872578   10724.14491456  147808.29252782\n",
      "    8919.03892535   59776.12811859  198255.31318149   12621.1626183\n",
      "   15915.47966912  191164.74515708  656988.11462677   16368.72739751\n",
      "   10829.08926093  349221.49171202  137671.14554533 1047530.16149626\n",
      "   30106.65816742   50485.35092003  190544.29354509  158444.63385791\n",
      "  200522.8748206    87781.69966796  292287.75632048   10072.37149325\n",
      "  135155.02362862  280957.38342527  109491.13994848  204795.02170737\n",
      "  539852.23575154    3783.60788641  229355.79461427  228355.87296427\n",
      "    8333.07976099  154099.33051297   78841.30693725    5170.37710823\n",
      "   25227.83209312  119489.35631564  108413.22295025    9888.17532452\n",
      "   16498.17461906   12922.47266436  393457.98865299   20749.79839403\n",
      "   23390.34064821  562197.66695031   18044.74211918  470500.07642214\n",
      "   15077.28713056  606194.80627892  260264.94373177  105279.56281579\n",
      "  836144.07894158  248881.18369959  150482.42446799  271774.45370484\n",
      "  113192.53990066  125031.85701658   37123.16256872   19211.31589538\n",
      "   17095.81046956   24483.29516204  369887.52581526    8340.92556569\n",
      "  365580.82114684  255857.51504848   13635.20671599   14809.73795776\n",
      "  334859.54589608  172990.11446439  275989.46514685   46235.42290818\n",
      "  241518.00829511  497065.0077486   313922.85120223  326765.39031097\n",
      "   28516.36396847   91745.19935493   13258.29854324  915938.52500218\n",
      "  166316.80226615   15027.79406138   20092.68311694    7996.22568224\n",
      "  450144.66313119   14134.98314178   24928.58309492  516002.01815619\n",
      "   30697.23347882    7056.80067982   12711.05334264   16178.67702821\n",
      "  735873.67822598   13301.53471832  304198.60569791    9224.07011849\n",
      "  238280.01412708    7664.10645854   10583.73584017  226232.36201715\n",
      "   83015.59609624   28250.96837848  372864.17087364    7106.93204004\n",
      "  133892.74184214   19140.87582378  353363.30153214   23732.93204642\n",
      "   25450.68445032   10298.70324393  297962.87897924   15137.43018437\n",
      " 1571357.39077211  135723.72962458  596484.45410511  166084.04108336\n",
      "   22374.32342947   16308.9303297   625444.3255524   255092.38497032\n",
      "   24629.85507105  111035.69155069   87589.86430882  150793.99844012\n",
      "  642278.72776557   14337.51869649   19050.58854729  149475.63907024\n",
      " 1118975.67470021  108542.22912971  111661.76270509   15563.43041512\n",
      "  153507.92344782  143751.84258582   34009.60061559  162990.85390075\n",
      "   16047.96412353  638551.01071506  883278.78493001   92569.34417235\n",
      "  472398.42924519  470032.13051494  654959.46380587   19177.10927373\n",
      "   17327.09203567  148930.75677608   17138.9912681    21906.2180143\n",
      "  119692.18105649   49502.09984487  432369.00757982   30504.85708393\n",
      "   11994.77320641   17928.14548487  146465.48978745   42201.82842713\n",
      "  320699.4082254   156881.72574065   17890.20168306  273020.7661261\n",
      "  136241.9622569    14552.99328308   74726.51857696   85575.33460949\n",
      "   11500.78260258   10194.50840804  300985.79863784  321010.60162274\n",
      "  176655.1315892    35729.96180945  363097.34601399   15817.22547603\n",
      "  101044.73045674  210967.5761854    15629.35852535   22021.114494\n",
      "  138737.53686649    8405.2573583   183152.22887745  249766.22522527\n",
      "  244756.20719446   62615.05439174   23375.10327852 1003923.97951681\n",
      "  838232.84139089   17860.0269642   628207.66754516  185734.92001747\n",
      "   16059.800185     83460.08848229  252493.79649899  266029.41159474\n",
      "  340175.32517238  339241.16712267  126380.41707049   18291.25188465]\n",
      "[0.6764209351170724]\n",
      "MAPE =  0.6764209351170724\n",
      "Epoch [20/5000], Train Loss: 352699264389.8179, Val Loss: 1324122629951.2004\n",
      "Epoch [40/5000], Train Loss: 462861756446.2502, Val Loss: 916110714215.9116\n",
      "Epoch [60/5000], Train Loss: 140287280323.8139, Val Loss: 1050572579310.8735\n",
      "Epoch [80/5000], Train Loss: 167393825409.4353, Val Loss: 1327577615416.8906\n",
      "Epoch [100/5000], Train Loss: 230627792085.2390, Val Loss: 869262336464.8174\n",
      "Early stopping at epoch 104\n",
      "\n",
      "Fold number: 1\n",
      "[ 497889.55688289   41579.78866797   30939.0120641   206699.23834865\n",
      "  164642.94210357  386062.71626674  642502.46660681  268943.20725918\n",
      "   24800.55550021   60689.33409145   23117.80929466  215539.23543579\n",
      "  272364.83116627   13520.35772198  436859.75945855   33816.28828787\n",
      "  174724.63956707  198624.20708429  259733.47361429   22299.19658577\n",
      "  725092.84495447   30567.3740167    14789.42619811  252823.07155406\n",
      "  314582.06282785  190785.80485194  180672.71000315  213991.31072782\n",
      "  382319.0987309   135656.41241051  274511.30536679  228248.3738306\n",
      "  112469.10858553  324070.79354272  333954.56548984  754346.2459811\n",
      "    7868.5482264   325383.05000009  598970.05920313   16982.80911056\n",
      "   10301.84381509   24050.86569182   17829.99858826  230376.25408728\n",
      "  132203.85028413  304266.66652033  132038.89887211  129387.00835776\n",
      "  323374.56157366   20068.99846635   92895.06308354  240340.5578669\n",
      "  623135.50026206   14186.72333007  231653.5790831   291860.01758972\n",
      "  378134.93685887  403024.49416689  150488.59391903   16998.55849824\n",
      "  281767.62060829  232215.55798344  281888.86674071  583210.99746211\n",
      "  211652.72455742    9939.26174439  962675.1204328   345660.91291066\n",
      "   17147.47491704  240429.88146344   95608.16624561   13735.06566951\n",
      "   21482.90291375  253430.01153021  216699.33514741  212810.2349429\n",
      "  128730.27978756  224555.99042149  211849.8545788   102979.08968822\n",
      "   18086.34899201  122092.52684284   15573.33969478  162011.16237172\n",
      "  143007.17275296  225574.28334485   40368.77885315    5893.64810776\n",
      "   12896.88817988   39887.74552508   73100.91473889  222443.90398604\n",
      "  109608.73032607  246754.42963449  155005.63412465  230042.96974136\n",
      "   17845.38321365  212485.85741505   94611.73173935  645677.80147432\n",
      "  466486.49698942  642313.16105299   21162.32287578  184340.98107512\n",
      "    5404.81210262  161396.23889227  126170.57256145  404677.20777463\n",
      "  191062.64538723  341971.62627392   89973.88316796   47769.14631311\n",
      "  147655.64723809  733041.57866059  155612.34963723  319785.92219661\n",
      "  391733.69174437  337565.35524636   11490.89567035   56625.87243128\n",
      "   11429.56255934   84228.2464542    12107.91596054  313833.96020797\n",
      "  282525.59759279  448287.75355565  248692.71328762  265823.49423305\n",
      "  190457.41536745  324440.47363883  246825.4472095     6040.31058848\n",
      "   25294.35854427   13782.06317239  566435.90560956  173321.97533749\n",
      "  294332.22852887   61091.18705562  483874.56201155  449369.52454117\n",
      "  164633.3601289    22828.30581339   14133.51603409  302152.65818824\n",
      "  436506.73667893   13196.41793902  220733.74834019   26391.76996884\n",
      "  158028.82533983  253640.37352296  221716.60932493    9755.71068847\n",
      "    9043.02350653  320522.60216813  136248.27430962    7632.45562912\n",
      "   10528.86434286  383648.99463368  157921.39880769  546141.38749001\n",
      "  276347.39250465  172055.27145685  226897.6860662   549188.0183081\n",
      "  935314.68098356   53568.53334029  329764.06269918   92362.64972087\n",
      "    7262.98287007   58058.22280856  134348.84098954  235257.34873966\n",
      "  249426.06651458   11594.02116677  291810.59746646  334107.32120645\n",
      "    8752.49889721    8756.13322866    7175.87506081  396685.99722944\n",
      "  925329.36324502 1522211.77425166   12506.57248904  211735.27820599\n",
      "   10582.55487325   84390.60179326  274857.6838663   124491.12618507\n",
      "    9208.56704719   15083.24729244  134520.65334067  188657.30970199\n",
      "  631045.9032998    86377.13820608  198673.92827602  165098.81090386\n",
      "  179918.23218403   18809.37705545   67060.35282711   86124.4976711\n",
      "   21999.48095663  330110.29624767  159704.72841971  141956.47753777\n",
      "  269832.70818254   13232.1420905   616575.43827611  317879.42001634\n",
      "   22683.12926925   20215.59629906  222987.7410904   285206.67545143\n",
      "  266927.65284451   20542.74294602  222083.71930914  128440.73132035\n",
      "  651064.35132322  330731.95865042   16732.98051052    9487.27604148\n",
      "   12627.32433118  138449.00280828  626725.54372227  258687.30978539\n",
      "   17208.76186359  236973.51643683    9760.26728969    6833.3180495\n",
      "   31519.73795874  205492.9162841   261552.3105549   285830.03668006\n",
      "   20632.02120373  115567.11521772  217908.66042746  223088.23314948\n",
      "  313740.52221013   34291.07026334  422630.18814182  247769.74967013\n",
      "   18069.31375617   95574.70159733  173119.43304163  828457.47804511\n",
      "  254405.03018966   16319.26563449  287814.71710377  166938.90296162\n",
      "   11003.05843319  172653.01965269    8119.23660185  251574.08746523\n",
      "  652520.8439853   125120.06681695  166192.16698845  250099.6250367\n",
      "  696349.08753357   11703.11695594  248833.98864814   92879.5770153\n",
      "  376005.5596295   142952.746386    370099.39493008  581117.74175305\n",
      "  105990.27871415   26193.93615814   15651.64556668  126414.0999699\n",
      "  185917.54257734  318451.91636481   11721.45982764  373217.55273998\n",
      "  399135.73506864  181900.9456412    16005.38973252  265548.15289103\n",
      "  694341.86405696  144870.60082773   16120.3515182    71891.52274306\n",
      "  332327.68154343  302930.57856885  874004.2620131    65181.33332756\n",
      "   24289.28233137  151233.92262721  223810.84700827  136574.79490093\n",
      "  715912.37702096   15877.4956618   626588.22903755  165239.50987682\n",
      "  655576.50911457   85256.11984151  110541.67896926   13471.97258044\n",
      "  105819.36298683  126823.74898185   29632.40463319   11530.93101674\n",
      "  164293.52330666  780205.34453778  458386.84329213  117554.25017663\n",
      "   46377.18014642    9601.07020429  367732.78340833  600346.29782898\n",
      "  270972.59082014   37012.3340497   229048.37686361  313486.829271\n",
      "    8285.56251282   39353.31179702   41981.99619236  333448.63878355\n",
      "  402122.86588432  331022.41820403  232093.17405712  228031.29902597\n",
      "  226980.16716674  208107.08004989  704291.61087733  173579.74259183\n",
      "    9538.55113411  249681.46132947    4202.1141188  1271421.38113499\n",
      "  479834.40076479   18863.07771165  272277.10399226  487637.35127355\n",
      "  246115.98463066  244119.87378305  215926.18167835   14427.72895998\n",
      "    6660.68222213   13443.08672414   13324.23226164  412325.63811152\n",
      "   18839.23444932  170606.44453238  193260.86650381  364731.2541582\n",
      "  178817.834193     13461.28555078  714317.06347888 1339584.91698993\n",
      "  724524.75053823   18453.6051931   504101.67809725  185790.37550268\n",
      "  250561.85263112  104817.96197228  417952.70294763  261308.03891017\n",
      "  255982.85197155   13554.69576028  180342.02954462  202127.01109685]\n",
      "[0.6530116881003102]\n",
      "MAPE =  0.6530116881003102\n",
      "Epoch [20/5000], Train Loss: 465742665912.6149, Val Loss: 288571632692.1918\n",
      "Epoch [40/5000], Train Loss: 377183149475.3845, Val Loss: 281814624474.6179\n",
      "Epoch [60/5000], Train Loss: 266361265700.3093, Val Loss: 208394842605.9583\n",
      "Epoch [80/5000], Train Loss: 223117843676.2871, Val Loss: 126378168260.5093\n",
      "Epoch [100/5000], Train Loss: 387511508172.2861, Val Loss: 172872043751.8204\n",
      "Epoch [120/5000], Train Loss: 558101443508.7153, Val Loss: 164756098787.1599\n",
      "Epoch [140/5000], Train Loss: 529633808862.7610, Val Loss: 147780081911.0251\n",
      "Epoch [160/5000], Train Loss: 328278170323.8079, Val Loss: 206618737661.8972\n",
      "Epoch [180/5000], Train Loss: 154470186217.6684, Val Loss: 144830085254.2651\n",
      "Epoch [200/5000], Train Loss: 314848759163.2189, Val Loss: 163161006945.9910\n",
      "Early stopping at epoch 201\n",
      "\n",
      "Fold number: 2\n",
      "[2.74665026e+05 4.44424149e+05 4.95073373e+05 2.02189801e+05\n",
      " 2.31131372e+05 2.02946018e+05 7.04809146e+05 1.48669309e+05\n",
      " 2.50851813e+05 1.41994442e+05 6.46130916e+05 1.09516902e+05\n",
      " 3.60621530e+05 1.15918862e+04 2.07862376e+05 3.37079538e+05\n",
      " 8.06879666e+03 1.51215177e+05 1.73008490e+05 2.11523950e+05\n",
      " 4.58191264e+05 3.74546562e+05 1.37563094e+04 5.28614641e+03\n",
      " 2.74795922e+05 5.62697964e+03 3.96977255e+03 3.56769972e+05\n",
      " 4.11283648e+05 8.53218721e+03 2.15587721e+05 1.76208549e+05\n",
      " 3.89058154e+05 1.47949558e+05 9.97842383e+03 3.65394106e+05\n",
      " 1.92836278e+05 1.63544095e+05 3.06040010e+05 1.74092309e+04\n",
      " 2.58090465e+05 2.46494163e+05 2.23207058e+05 9.74672506e+03\n",
      " 1.63194144e+05 5.91589369e+03 6.12331338e+03 4.70410087e+03\n",
      " 1.62699780e+04 5.92282184e+05 3.33021629e+03 8.97405101e+03\n",
      " 1.81397614e+04 1.86963535e+05 1.19627500e+05 3.27614574e+05\n",
      " 1.58478297e+05 2.45665021e+05 9.54931613e+03 3.49945005e+05\n",
      " 3.27698061e+05 1.15718668e+05 3.78909514e+05 1.94589182e+04\n",
      " 7.38804065e+04 1.24720311e+05 7.87568161e+05 1.92146050e+05\n",
      " 2.42413560e+05 6.71147456e+03 2.15642859e+05 3.46429294e+05\n",
      " 2.92202516e+05 8.13178101e+04 2.21749852e+05 7.10914670e+03\n",
      " 2.10444460e+05 1.02546925e+04 6.07506660e+03 8.40115886e+04\n",
      " 1.36254690e+05 4.08241818e+03 3.05502281e+05 9.39033027e+04\n",
      " 3.21947842e+03 1.49971534e+05 3.67018866e+05 1.53390568e+05\n",
      " 1.70737386e+05 5.58802556e+04 1.41323521e+05 2.35887087e+05\n",
      " 9.30848946e+04 2.44493322e+05 6.29806867e+04 8.44880322e+03\n",
      " 3.45929739e+05 1.67191453e+04 1.89527029e+05 6.89500702e+05\n",
      " 2.82437526e+04 3.14156871e+05 2.06635715e+04 1.98069157e+05\n",
      " 2.01814302e+05 2.38062895e+05 1.24009028e+05 1.65990565e+05\n",
      " 3.99005129e+05 3.93041854e+05 2.94657148e+04 9.93281165e+05\n",
      " 2.86777511e+05 4.92311315e+05 8.18632607e+04 1.84348527e+05\n",
      " 4.40262837e+05 2.19734048e+04 1.95958239e+05 5.33469968e+05\n",
      " 9.35459599e+04 5.13999589e+03 1.98574388e+05 2.90312176e+05\n",
      " 1.36508702e+05 2.30935829e+05 9.39927686e+04 1.42109191e+05\n",
      " 8.10902587e+03 2.29463394e+05 1.59892683e+04 5.78858536e+03\n",
      " 5.42154553e+05 1.45489224e+05 3.54081563e+05 5.17872851e+03\n",
      " 1.43403595e+05 7.22668574e+05 6.61751118e+05 1.16786460e+05\n",
      " 9.01891949e+04 3.12709692e+05 1.23628501e+05 3.95707513e+05\n",
      " 1.16434852e+05 1.20495735e+04 6.76204302e+03 2.72359686e+04\n",
      " 8.87821132e+03 1.07514818e+04 1.07751989e+05 2.74753983e+05\n",
      " 9.52486528e+03 1.56174458e+05 3.49994372e+05 1.04008031e+05\n",
      " 1.19409442e+05 4.86408561e+05 1.31309063e+05 2.28353283e+05\n",
      " 9.19220788e+03 1.43990789e+05 2.50332514e+05 3.47103280e+05\n",
      " 3.39331359e+05 9.67323214e+05 8.37247765e+04 6.61871206e+03\n",
      " 1.86696196e+05 6.71071740e+05 4.59572985e+03 1.59717127e+05\n",
      " 2.80327959e+05 3.32206022e+05 2.61377032e+05 1.04916258e+05\n",
      " 1.45069282e+05 1.43625588e+05 9.74892753e+04 2.89948516e+04\n",
      " 2.87073365e+04 4.40311637e+05 4.04028216e+05 2.28282990e+05\n",
      " 1.83620646e+05 8.03479866e+04 1.20059353e+04 5.16988472e+04\n",
      " 6.77190418e+04 1.45900895e+05 1.18359916e+05 5.88841981e+03\n",
      " 2.31339752e+05 4.60415630e+04 1.06658440e+05 7.49312987e+03\n",
      " 7.97176821e+03 3.74101407e+05 9.52836392e+04 3.87928805e+04\n",
      " 3.33784998e+05 3.33859163e+05 1.01534998e+05 7.70797752e+03\n",
      " 1.67827193e+05 1.92594680e+05 8.48387314e+05 2.15042647e+05\n",
      " 3.21695762e+05 1.80286889e+04 1.70175289e+04 3.11641901e+05\n",
      " 2.37780968e+05 5.86918126e+05 7.85480572e+03 2.34160000e+03\n",
      " 3.06653903e+05 6.81959028e+05 6.79592804e+03 1.16955232e+05\n",
      " 2.09463335e+05 3.56145014e+03 3.01042075e+05 1.31853845e+05\n",
      " 1.66527643e+05 1.50847831e+05 1.01316740e+04 4.72419359e+03\n",
      " 3.81638553e+05 1.42464927e+05 1.90484980e+05 1.79171071e+05\n",
      " 5.19976796e+05 3.89946699e+03 1.66583587e+05 4.30054596e+05\n",
      " 4.10996218e+05 1.09428079e+06 5.83787167e+05 1.44167912e+05\n",
      " 1.86051662e+05 3.27418298e+03 9.09656285e+03 2.48252920e+04\n",
      " 1.39724245e+05 1.23198694e+05 1.48399653e+05 1.05675311e+05\n",
      " 8.09094845e+04 1.73447622e+05 1.51419504e+05 6.69590627e+05\n",
      " 4.65936975e+05 1.02995892e+05 1.02246571e+05 1.43976738e+05\n",
      " 3.93289993e+05 9.10382496e+04 4.36138116e+05 8.76060928e+04\n",
      " 1.22452623e+05 9.42360654e+03 9.46546813e+03 5.32859982e+05\n",
      " 4.38635893e+03 3.03929055e+05 1.54412230e+05 1.19911046e+05\n",
      " 1.55138413e+05 3.60781487e+05 3.04901404e+05 1.98282689e+05\n",
      " 2.26843702e+05 1.01090401e+05 5.26394839e+05 3.00842482e+05\n",
      " 1.03807531e+06 7.47723772e+04 4.53550139e+05 4.05644511e+03\n",
      " 1.82853151e+05 4.47558475e+05 4.27533286e+05 7.47681214e+04\n",
      " 3.70490521e+05 9.21610959e+04 1.02377478e+05 1.35342094e+05\n",
      " 2.48583066e+04 1.67435985e+05 1.64120230e+05 5.61360033e+03\n",
      " 4.94435307e+04 2.86586568e+03 8.77375794e+04 1.61836546e+05\n",
      " 1.49664158e+05 5.11180143e+05 2.53767295e+05 3.19757767e+05\n",
      " 3.54595433e+05 3.47510289e+04 2.77596307e+04 8.93025124e+04\n",
      " 1.03809239e+06 8.64856303e+03 1.17586771e+06 2.17024708e+05\n",
      " 3.86255800e+05 4.69233309e+05 1.44076413e+05 2.48087389e+05\n",
      " 8.28114424e+03 5.35710779e+05 7.22834858e+05 2.12322416e+05\n",
      " 1.47536450e+05 1.18523627e+05 5.64996206e+05 1.12973550e+05\n",
      " 9.53723545e+03 3.49646505e+05 2.72354241e+05 7.16174723e+03\n",
      " 1.12361556e+05 2.91882272e+05 6.35987834e+04 7.96177982e+05\n",
      " 1.46557197e+05 2.69579119e+05 2.06326965e+04 1.17488439e+04\n",
      " 7.36882569e+03 1.14480615e+04 1.55622343e+04 3.33273268e+05\n",
      " 8.19843584e+04 2.55526070e+05 1.20591307e+05 1.69725592e+05\n",
      " 1.39855815e+05 1.10910330e+05 6.89461885e+03 5.80069290e+05\n",
      " 9.21564040e+03 1.23140328e+05 1.46331151e+05 2.65427601e+06\n",
      " 3.19165390e+04 2.07303049e+05 4.38290285e+05 3.37207908e+05\n",
      " 1.63538596e+05 3.39330395e+03 7.44387344e+05 1.55992330e+05\n",
      " 1.64801656e+04 5.10187136e+05 1.30804513e+05 1.51125269e+05]\n",
      "[0.6289009428867358]\n",
      "MAPE =  0.6289009428867358\n",
      "Epoch [20/5000], Train Loss: 567065441539.8782, Val Loss: 191881152090.2662\n",
      "Epoch [40/5000], Train Loss: 137240694595.1455, Val Loss: 249119958117.1896\n",
      "Epoch [60/5000], Train Loss: 517288036971.3079, Val Loss: 142710369064.0472\n",
      "Epoch [80/5000], Train Loss: 237698980474.3892, Val Loss: 135999455045.2734\n",
      "Epoch [100/5000], Train Loss: 334352296194.8571, Val Loss: 205573411875.7088\n",
      "Epoch [120/5000], Train Loss: 595404401987.1541, Val Loss: 179856140546.8746\n",
      "Epoch [140/5000], Train Loss: 298899885540.6913, Val Loss: 180971737140.4690\n",
      "Early stopping at epoch 148\n",
      "\n",
      "Fold number: 3\n",
      "[ 432258.10617214  741597.0003816   626103.58674266  398731.77467161\n",
      "  257561.26380957  540973.07621786  727235.1147036    17714.37554256\n",
      "  476088.02974779  102186.09051828  809183.01569758   16891.45033298\n",
      " 1190183.56414118  650436.83814753  492107.52928756 1576680.38656244\n",
      "  407792.61981588  232421.84231506  351591.91770224  478777.26985566\n",
      "  914613.55566351  706275.99786454  840849.01063375   12579.6379181\n",
      "  418028.16062652  191836.08637144    3959.09456355  365735.09893498\n",
      "  581475.03078393   14250.53965172  117763.10948265  201841.95077949\n",
      "  223227.18892969  306947.50680118   24969.57196822  622138.8263827\n",
      "  364076.81706812  415587.13975835  849721.18686667  412018.45847297\n",
      "  373391.66354098  750180.99620218  434096.48170494  392465.8710432\n",
      "  229252.90500901  289173.24219792  269945.1679827    11659.69246836\n",
      "  776783.5381542    30027.35881147  108623.65508116   18255.35801861\n",
      "  374177.77671872   18676.26541779   21106.16721901  333916.62148621\n",
      "  416600.25866846   25385.20234328   12964.0848466   601854.44437817\n",
      "  603724.17392889   13458.37331687   24022.6356292    37108.28764509\n",
      "  190186.32382649  198839.10471895 1261715.97441116  201783.71721191\n",
      "  322719.6655495   286867.56781852  133416.85463318  337495.25125794\n",
      "   29541.74105397  157730.75518246   15787.64850259  159166.88902426\n",
      "   10403.18434804  204303.05682692  263715.92482703  111595.82730736\n",
      "  291001.57870017    5651.66254155   21796.71998612  215537.72010253\n",
      "    8673.94446464   13420.89601234 1885606.98672591    8957.93003598\n",
      "  257209.49356838 2009520.2329661   521067.90236563   17511.77382419\n",
      "  313158.66417664  548760.96495608  125718.87189857   16224.63397552\n",
      "  717082.43142536   14246.6077688   175759.73709158  858855.05781668\n",
      "   44357.6891058   825178.59924914   25231.32182365  218538.72315359\n",
      "  192008.2133795   240778.79409334   12981.40036613  270868.42089333\n",
      "   29690.74815366  443910.73318393  118540.09802982 1463106.94376365\n",
      "  368976.19858181  727891.24001708  204786.42077024  329464.37908811\n",
      "  467583.77750162  414510.14864577  219961.07716798   69204.40225169\n",
      "  218423.2638237   191469.50422923  444886.00118916  253656.82810996\n",
      "  342916.24078288   35373.21549433  246906.704991    164430.08111978\n",
      "  576164.2068552   397212.06357059  493653.25920916  317539.31784464\n",
      "  687427.91824178  498265.58213129  594311.53349938   10996.26223286\n",
      "   21268.56845376 1284157.79796369 1260472.37097794  285629.4413226\n",
      "  153863.4731761  1397399.97329058  302937.34567164   34539.54417734\n",
      "  269240.99188702  348079.72635854   14515.6895433   876757.16632791\n",
      "   14570.75119917  365603.27025216  153404.8433364   185899.02468481\n",
      "  228144.59916268   14517.12484183  587149.31748477  335995.62383998\n",
      "  632139.39122302  489803.41445295    8836.92375287  312552.58845854\n",
      "   17212.08381276   19606.45880287   20429.08735836  701123.87568371\n",
      "  493893.55031979 1240289.73695798   11494.57410518    7577.24502762\n",
      "    9467.84844397   76696.49108446  170690.61268326  286113.50543193\n",
      "   20615.70096137  243039.5081599   558154.58568832  233878.070582\n",
      "   11935.10885937  229282.44453284  118976.34914668   54709.57346388\n",
      "   55295.11953996  862301.45836959  409828.24776153  258010.26392927\n",
      "  290346.99355424   91601.45326397  859168.70844706  100735.16914807\n",
      "  170177.65288578   21446.41729746  151206.68861861   10936.44494003\n",
      "  617200.52736452   77791.24499046  207692.13621815   11184.8694183\n",
      "   13462.10433025  429952.40470907  111228.78405246   94690.01459361\n",
      "  421662.3995062   545669.54611336   10764.50127278   12978.11724662\n",
      "   22656.97593732  553066.06615586 1128788.76153861   24439.75581514\n",
      "  682603.61052402  820273.57496638  422174.89343206   24418.47060368\n",
      "  688469.64781106   21986.53136464  113356.46345079    6442.70291376\n",
      "  585665.42127799   43474.36852853  409061.91439334   11911.94333735\n",
      "  377726.90350716    8301.06791236  675180.38705827  325135.81110336\n",
      "  347608.62501866  370125.05514484   12978.33852567    8453.70352235\n",
      "  522383.75446988   16057.8681007    15544.48850584  395277.25633657\n",
      "  306629.88612802  164949.87116795   15144.67666132  582769.23093482\n",
      "  701099.34754832  820109.34025014 1013592.87616348  177643.27132805\n",
      "  333457.73948354    4221.19637305   13413.17311574 1214835.93500656\n",
      "  135682.62102651  321161.46558411   20891.19633255   10360.30123243\n",
      "  479788.59574464  182885.20870875  249659.49245879  676179.22377464\n",
      "  874227.89911797  158652.32104718  233533.19433978   17833.00861736\n",
      "  765724.6487952   215004.67220376  689759.37453367  195890.82097701\n",
      "  351216.58652413   17369.66262508  370561.38449923 1029128.86042099\n",
      "  104238.66521026   36237.13770762  296251.68125756  194963.25950491\n",
      "   13007.04086977  344284.94649972  548766.21597705  400761.92994231\n",
      "  440686.23485133   15254.76311446  774582.3706945   306544.89203749\n",
      " 1302735.16859964  119984.08445544  679751.99816886  122529.62899141\n",
      "  299131.50204267   24167.87582646 1793923.23910818  185441.85056066\n",
      "  542520.77375718   11400.5933835   164342.66388261   10938.41117306\n",
      "  937117.17965977  659230.89273385  355164.91436511   10728.60245035\n",
      " 2525910.97192183  198136.01491164  148973.77493145   18628.10561848\n",
      "  146004.61726021  363704.75461471  713775.19199667  308669.16206578\n",
      "  298541.09345312  614107.08324733 1308069.55865972    8682.20001864\n",
      "  780414.71978937  428681.33726866  796345.98524111  377156.92233956\n",
      "   21512.64627976  781640.38727669  305217.39295994  440741.80200581\n",
      "  170445.11382348  914858.50799083  587101.67996053  539253.42482783\n",
      "  227969.18058307  322253.49479404  299288.25564665   15477.32117313\n",
      "   20723.83662293  503844.63453388  378010.9793274   476564.53354936\n",
      "  371198.45945909  496414.98239385    6632.25371534 1339246.91266019\n",
      "  251107.95220873  204623.13892326  283828.07156479  336261.96248953\n",
      "  193961.75032696   17751.12813656  321969.57647832   17877.54252805\n",
      "    8454.6228299   242994.0896879   284741.60137271  539304.82541118\n",
      "  366726.80340489  181760.38879364   11054.35255498  514517.05909023\n",
      "  258420.77987764  468878.22925368  404674.35449925 2845314.88320171\n",
      "   49475.2419017    23579.3761162   576708.44247774  458783.9877229\n",
      "   16116.71277196  116721.65895326  760275.80425886  295443.2765491\n",
      "  451530.21022099  360205.51422058  206491.26780107  328050.67798934]\n",
      "[0.6929344786827316]\n",
      "MAPE =  0.6929344786827316\n",
      "Epoch [20/5000], Train Loss: 662844953870.4617, Val Loss: 229299438759.8450\n",
      "Epoch [40/5000], Train Loss: 394642851399.5668, Val Loss: 309529508493.0237\n",
      "Epoch [60/5000], Train Loss: 274361268339.1119, Val Loss: 271538753130.5491\n",
      "Epoch [80/5000], Train Loss: 458371783959.2566, Val Loss: 298445820455.4206\n",
      "Epoch [100/5000], Train Loss: 124132047463.9275, Val Loss: 218252245458.1045\n",
      "Epoch [120/5000], Train Loss: 112870735122.1863, Val Loss: 251910626229.8518\n",
      "Epoch [140/5000], Train Loss: 462194077164.0189, Val Loss: 215632476070.8319\n",
      "Epoch [160/5000], Train Loss: 530442935035.5780, Val Loss: 241063475516.6546\n",
      "Epoch [180/5000], Train Loss: 1096889267842.8655, Val Loss: 210524301503.0047\n",
      "Epoch [200/5000], Train Loss: 846567809354.9520, Val Loss: 218069450740.4155\n",
      "Epoch [240/5000], Train Loss: 457180134234.2522, Val Loss: 266456937695.0455\n",
      "Epoch [260/5000], Train Loss: 766945370389.8726, Val Loss: 280970885564.5009\n",
      "Epoch [280/5000], Train Loss: 1008882097023.2776, Val Loss: 282854448472.9514\n",
      "Epoch [300/5000], Train Loss: 341561028735.4247, Val Loss: 279317939024.5782\n",
      "Early stopping at epoch 317\n",
      "\n",
      "Fold number: 4\n",
      "[ 383850.52078358 1685361.81985845  357632.51484447  308538.00263337\n",
      "  112218.90205326  219867.94125516 1325586.18802341  217927.66294332\n",
      "  329433.52059588    5399.05522663  369885.0601679   240460.52566944\n",
      "   25343.7595721   534443.25242846  343265.00367375  646528.21061745\n",
      "    8988.08613161  247758.55124778  623555.16746134  321594.2591887\n",
      "  595630.08645365  431408.07905093  265619.06384098    9835.45119219\n",
      "  203916.36220882    7923.89863573    8227.87113974  507133.62527553\n",
      "  329545.68594127  165952.29570222  504355.41504801  189467.52382442\n",
      "   13090.93291937  211870.72715952   14228.01274785  551435.78111889\n",
      "  272509.03734216  223778.40088981  519758.33706956  701360.89541069\n",
      "  276215.0879367   599463.90809676  315912.92402391   16875.73202195\n",
      "  135121.40094881   18571.30826857  172566.86469401    5313.9389198\n",
      "  252476.67488959  573831.51676436   75684.22388501   12932.57055783\n",
      "   23289.21433106   13498.05433533  239822.35051232  253603.47460077\n",
      "  204077.61221798   20308.97846104   11251.56453586  439290.91768283\n",
      "  211510.83844323  157657.36867904   17542.54601664   24231.61813829\n",
      "  135627.57442624  239852.43927895  743003.27465788  315306.82685843\n",
      "  123597.58967904   10873.19606684   80006.56015739   18261.67842758\n",
      "  351555.53104092  124369.7937969    16119.22399812    8025.19956246\n",
      "  206340.30298469   12029.18476723   11429.07126368  130644.3395363\n",
      "  151612.10190883    5189.67355853  259842.03438739  139158.19829879\n",
      "    5479.11805599  138455.05890556 1461952.66032045   93112.48316162\n",
      "   12158.92322173 2319547.21594995  156427.82011509  287043.66118563\n",
      "  145239.24901368   19436.32298104   99984.07950852   15444.33580229\n",
      "  483511.56377375   11094.34677239   14438.04757267  382007.80564987\n",
      "  541468.63758328 1159147.50212165   18229.85128435  125129.70295205\n",
      "  269815.80328908  175243.95120007    9459.8755621   182967.99961438\n",
      "   23197.35199294  254424.1225239    72037.54191473 1184994.8580735\n",
      "  320581.37079709  552242.18767333  116220.69164283   10945.36586928\n",
      "  275886.67641145  266687.32082181   13189.23486534  941822.79600496\n",
      "  105673.13923428    8119.96919749  427788.50555881  152709.75930441\n",
      "  188874.62193809  276760.43984592  194316.89031663  144185.04832147\n",
      "   11463.10578637   19613.78213317  328226.90782757  125571.14139013\n",
      "  578981.36837016  256213.00783288  332442.63159893  111915.11823205\n",
      "   12234.58546029  963096.33904764  636072.84467809  160667.10603294\n",
      "  106763.54558279   30892.08745929  198308.11167177  716560.32079258\n",
      "  168661.4112517   269856.7551448    11297.36536107   32655.36843525\n",
      "   13299.21774942   15422.10753622    7438.90229455  135071.90149788\n",
      "  198883.07425682    8161.71935556   15771.47184462  133121.06076942\n",
      "  186410.17563966  197541.69315328  134224.00689035  233716.34186429\n",
      "   17139.30450457   14927.22382871  600491.83529415  361069.34593624\n",
      "  537327.97873794  736099.0924813   140657.24126631    9059.24324725\n",
      "  141660.62790813 1240280.92767185    6458.58551908   11265.81841826\n",
      "  286936.96448955   11819.06410397  510566.80049933  139170.83588105\n",
      "  159650.55785612  481710.85696845   91101.73593376   48563.91592371\n",
      "   26365.97940747  517136.22071546  488499.18809714  457280.55499499\n",
      "  240854.43641892   56830.20787523   20881.97176575   92806.86664295\n",
      "  133165.40757184  225565.97036924  156274.44763132   10817.216873\n",
      "  213381.69325776   42738.58644393  321835.61442537   14869.00373706\n",
      "   11039.45673973  384209.78393188  104309.04546167   49384.95583102\n",
      "  240480.2080234   405524.93946116  136677.80176683  129640.58402824\n",
      "  212255.77871333  284855.58145451 1217712.08083152  231552.42501896\n",
      "  630456.56619063  769782.0675772   296022.62819105  311157.59862781\n",
      "   15573.47386173   18747.67539617    9321.1457329     3501.83393625\n",
      "  606550.52906351  580735.27633959  339784.95712714   83526.32268362\n",
      "  267240.21872506  115704.41000224  483170.44443976  189420.51256599\n",
      "  207821.14735519  142284.58974918  353815.69923787    6516.47982681\n",
      "  361092.98762463    8240.66029978  161948.04326803  246728.63579495\n",
      "  467046.38811053    6950.45906779  156116.21346264   37914.15783123\n",
      "   16993.74083391  924634.98773716   28195.66961732  231870.26901543\n",
      "  283393.18316815    3934.14565797   10214.66780986 1581510.09801417\n",
      "    8344.28526681   15293.54776009  237384.60882905   11413.26343027\n",
      "  353871.4373419    85716.39311929  156950.94827421  569567.56926269\n",
      "   31769.33088476    6182.65344847    9229.18150676  388768.79881247\n",
      "  440067.68827045  148943.25756705  282879.12649119  113530.79752824\n",
      "  234345.41048825   10941.75552981   15636.09793838  661722.32370861\n",
      "    5412.18530399  350276.50418071  245277.27194656  172364.43262083\n",
      "  200188.75708981  375189.62550556  328416.32532102  161430.31196232\n",
      "  256811.02945959  138940.47524578   20720.31065639  231696.39895909\n",
      "  988459.28752772    6982.40275895  144019.12094924    6018.7761016\n",
      "  237291.36143966  304472.05561819  939099.01039911  206458.22235905\n",
      "  760376.27573669    8043.90076402   74663.66802011  165914.59050035\n",
      "  554355.17029943   14862.52919839  185807.15209833    7745.59453855\n",
      " 1159119.57240323  186850.76046403    7732.17307979  219908.64322097\n",
      "  108902.56751404   18433.07312321  463658.34189709  199772.13373295\n",
      "  123433.1285182   541925.27589099  390552.55861456  200683.71368994\n",
      " 1043729.96946622  423220.95566663   24506.31041306   14074.57885366\n",
      "  212546.45301912  466804.39037631  215088.59404862  248365.36793981\n",
      "  165686.49355723  569718.68118034 1212079.97322707  228845.00507099\n",
      "  104133.91060727  222996.74312663  396722.75629039  170281.32529522\n",
      "   10095.85294087  361076.46500724  289861.7621579     9580.53090841\n",
      "  257746.73687286  495685.67551943    4250.99018427 1021411.44089694\n",
      "  381310.22487263   16793.98382253   16091.03819794   12811.90208602\n",
      "    9697.99205026  170931.38554545  267167.25391321    9194.81453034\n",
      "  107821.27762947  187127.63490029  105368.08885308  278641.63252235\n",
      "  220109.77416755    8373.64900665    9191.47611814  678631.60984879\n",
      "  428802.32330721  193364.80744081  260410.88898988 2271395.59003723\n",
      "   39319.50393847  233305.20409143  845012.40481679  335658.12992038\n",
      "  207004.10721861   75313.08371972 1024092.08643648  209776.78218649\n",
      "  791598.60090984  563300.7081767   137617.08298991  185510.7526924 ]\n",
      "[0.6243405285123704]\n",
      "MAPE =  0.6243405285123704\n",
      "-\n",
      "mape score =  [[0.6764209351170724], [0.6530116881003102], [0.6289009428867358], [0.6929344786827316], [0.6243405285123704]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # stock_info = sys.argv[0]\n",
    "    # lag_bin = int(sys.argv[1])\n",
    "    # lag_day = int(sys.argv[2])\n",
    "    # bin_num = int(sys.argv[3])\n",
    "    # random_state_here = int(sys.argv[4])\n",
    "    # test_set_size = float(sys.argv[5])\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = sorted(list(set(s.split('_25')[0] for s in files)))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:1]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        mape_list = []\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(testInputs), torch.tensor(testgraphInput), torch.tensor(testTargets))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "        K = 5  # Number of folds\n",
    "        for k in range(K):\n",
    "            torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "            trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "            trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "            trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "            valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "            valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "            trainInputs = normalize(trainInputsAll[:, :])\n",
    "            valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "            # Assuming trainInputs, trainGraphInput, trainGraphFeatureInput, trainTargets are PyTorch tensors\n",
    "            train_dataset = TensorDataset(torch.tensor(trainInputs), torch.tensor(trainGraphInput),torch.tensor(trainTargets))\n",
    "            val_dataset = TensorDataset(torch.tensor(valInputs), torch.tensor(valGraphInput),torch.tensor(valTargets))\n",
    "\n",
    "            # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "            # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "        \n",
    "            model = GAT(7,8,1,0.3,0.05,8)\n",
    "            model.fit(train_loader, val_loader)\n",
    "            predictions=model.test(test_loader)\n",
    "            torch.save(model.state_dict(), f'models/gat_{stock_info}_{lag_bin}_{lag_day}_gcn_model_iteration_{k}.pt')\n",
    "    \n",
    "            print()\n",
    "            print('Fold number:', k)\n",
    "\n",
    "            new_predictions = np.array([item.detach().numpy() for item in predictions]).flatten()\n",
    "            MAPE = []\n",
    "\n",
    "            MAPE.append(mean_absolute_percentage_error(testTargets[:], new_predictions[:]))\n",
    "            print(new_predictions)\n",
    "            print(MAPE)\n",
    "            testTargets0 = list(testTargets)\n",
    "\n",
    "            res = {\n",
    "                'testTargets': testTargets0,\n",
    "                'new_predictions': new_predictions\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame(res)\n",
    "            res_df.to_csv(f'./result/gat_{stock_info}_{lag_bin}_{lag_day}_res_test_MAPE{k}.csv', index=False)\n",
    "\n",
    "            print('MAPE = ', np.array(MAPE).mean())\n",
    "            MAPE_mean = np.array(MAPE).mean()\n",
    "            mape_list.append(MAPE)\n",
    "\n",
    "        print('-')\n",
    "        print('mape score = ', mape_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24562a-d621-42c5-aba8-0b5ed439c76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676ef83-75a9-44d2-8c98-f5c6b20e5939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
