{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e99867e-b47b-4bd1-b701-6860aee8f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pdb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed = 42\n",
    "\n",
    "def file_name(file_dir,file_type='.csv'):#默认为文件夹下的所有文件\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(file_type == ''):\n",
    "                lst.append(file)\n",
    "            else:\n",
    "                if os.path.splitext(file)[1] == str(file_type):#获取指定类型的文件名\n",
    "                    lst.append(file)\n",
    "    return lst\n",
    "\n",
    "def normalize0(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        maks = np.max(np.abs(eq))\n",
    "        if maks != 0:\n",
    "            normalized.append(eq / maks)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize1(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        mean = np.mean(eq)\n",
    "        std = np.std(eq)\n",
    "        if std != 0:\n",
    "            normalized.append((eq - mean) / std)\n",
    "        else:\n",
    "            normalized.append(eq)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def normalize(inputs):\n",
    "    normalized = []\n",
    "    for eq in inputs:\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            eps = 1e-10  # 可以根据需要调整epsilon的值\n",
    "\n",
    "            eq_log = [np.log(x + eps) if i < 5 else x for i, x in enumerate(eq)]\n",
    "\n",
    "            #eq_log = [np.log(x) if i < 5 else x for i, x in enumerate(eq)]\n",
    "            eq_log1 = np.nan_to_num(eq_log).tolist()\n",
    "            normalized.append(eq_log1)\n",
    "    return np.array(normalized)\n",
    "\n",
    "\n",
    "def k_fold_split(inputs, targets, K, seed=None):\n",
    "    # 确保所有随机操作都使用相同的种子\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    ind = int(len(inputs) / K)\n",
    "    inputsK = []\n",
    "    targetsK = []\n",
    "\n",
    "    for i in range(0, K - 1):\n",
    "        inputsK.append(inputs[i * ind:(i + 1) * ind])\n",
    "        targetsK.append(targets[i * ind:(i + 1) * ind])\n",
    "\n",
    "    inputsK.append(inputs[(i + 1) * ind:])\n",
    "    targetsK.append(targets[(i + 1) * ind:])\n",
    "\n",
    "    return inputsK, targetsK\n",
    "\n",
    "\n",
    "def merge_splits(inputs, targets, k, K):\n",
    "    if k != 0:\n",
    "        z = 0\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "    else:\n",
    "        z = 1\n",
    "        inputsTrain = inputs[z]\n",
    "        targetsTrain = targets[z]\n",
    "\n",
    "    for i in range(z + 1, K):\n",
    "        if i != k:\n",
    "            inputsTrain = np.concatenate((inputsTrain, inputs[i]))\n",
    "            targetsTrain = np.concatenate((targetsTrain, targets[i]))\n",
    "\n",
    "    return inputsTrain, targetsTrain, inputs[k], targets[k]\n",
    "\n",
    "\n",
    "def targets_to_list(targets):\n",
    "    targetList = np.array(targets)\n",
    "\n",
    "    return targetList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160f4062-acf2-4d2d-bb5e-8a3d4865f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\")\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903 \n",
    "    图注意力层\n",
    "    input: (B,N,C_in)\n",
    "    output: (B,N,C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features   # 节点表示向量的输入特征数\n",
    "        self.out_features = out_features   # 节点表示向量的输出特征数\n",
    "        self.dropout = dropout    # dropout参数\n",
    "        self.alpha = alpha     # leakyrelu激活的参数\n",
    "        self.concat = concat   # 如果为true, 再进行elu激活\n",
    "        \n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))  \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # 初始化\n",
    "        self.A = nn.Parameter(torch.zeros(size=(2*out_features, 16)))\n",
    "        nn.init.xavier_uniform_(self.A.data, gain=1.414)   # 初始化\n",
    "        \n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    \n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [B,N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵  [N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.matmul(inp.double(), self.W.double())   # [B, N, out_features]\n",
    "        N = h.size()[1]    # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1,1,N).view(-1, N*N, self.out_features), h.repeat(1, N, 1)], dim=-1).view(-1, N, N, 2*self.out_features)\n",
    "        # [B, N, N, 2*out_features]\n",
    "      \n",
    "        E = [torch.matmul(a_input.double(), self.A[:,i].unsqueeze(1).double()).squeeze(3)[:,:,i] for i in range(N)]\n",
    "\n",
    "        e = self.leakyrelu(torch.stack(E, dim=2))\n",
    "        # print(e.shape)\n",
    "\n",
    "        # [B, N, N, 1] => [B, N, N] 图注意力的相关系数（未归一化）\n",
    "        \n",
    "        zero_vec = -1e12 * torch.ones_like(e)    # 将没有连接的边置为负无穷\n",
    "\n",
    "\n",
    "        attention = torch.where(adj>0, e, zero_vec)   # [B, N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)    # softmax形状保持不变 [B, N, N]，得到归一化的注意力权重！\n",
    "        # print(attention.shape)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [B, N, N].[B, N, out_features] => [B, N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.relu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout \n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)   # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout,alpha=alpha, concat=False)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = torch.mean([att(x, adj) for att in self.attentions], dim=0)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)   # dropout，防止过拟合\n",
    "        x = self.out_att(x, adj)   # 输出并激活\n",
    "        return x[:, -1, :] # log_softmax速度变快，保持数值稳定\n",
    "        \n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        super(GATModel, self).__init__()   \n",
    "        self.gat1 = GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True)\n",
    "        self.gat2 = GraphAttentionLayer(n_hid, n_hid, dropout=dropout, alpha=alpha, concat=True)\n",
    "        self.gat3 = GraphAttentionLayer(n_hid, n_hid, dropout=dropout, alpha=alpha, concat=False)\n",
    "        self.linear2 = torch.nn.Linear(n_hid, n_hid).double() \n",
    "        self.linear3 = torch.nn.Linear(n_hid, n_class).double() \n",
    "    def forward(self, x, adj):\n",
    "        h = self.gat1(x, adj)\n",
    "        h = F.leaky_relu(h, negative_slope=0.2)\n",
    "        h = self.gat2(h, adj)\n",
    "        h = F.leaky_relu(h, negative_slope=0.2)\n",
    "        h = self.gat3(h, adj)\n",
    "        last_column = h[:, -1]\n",
    "        conv1_new = last_column.unsqueeze(-1)\n",
    "        #conv1_new = torch.nn.Flatten(conv1_new)\n",
    "        #conv1_new = F.dropout(conv1_new,p=0.3)\n",
    "        conv1_new = F.dropout(conv1_new.view(conv1_new.size(0), -1), p=0.3, training=self.training) \n",
    "        conv1_new = self.linear2(conv1_new)\n",
    "        y_hat = self.linear3(conv1_new)\n",
    "        return y_hat\n",
    "   \n",
    "    def fit(self,train_loader, val_loader,num_epochs=1000,patience=100):\n",
    "        #best_val_loss = float('inf')\n",
    "        best_val_loss = torch.tensor(float('inf'), dtype=torch.double)\n",
    "        patience_counter = 0\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.000015,weight_decay=5e-4)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0 \n",
    "            for inputs, graph_input, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs, graph_input)\n",
    "                loss = criterion(outputs.squeeze(dim=1), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm = 1)\n",
    "                optimizer.step()\n",
    "            #     train_loss += loss.item()\n",
    "            # avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_graph_input, val_targets in val_loader:\n",
    "                    val_outputs = self(val_inputs, val_graph_input)\n",
    "                    val_loss = criterion(val_outputs.squeeze(dim=1), val_targets)\n",
    "            #         val_loss += val_loss.item()\n",
    "            # avg_val_loss=val_loss / len(val_loader)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    return\n",
    "                \n",
    "                \n",
    "    def test(self,test_loader):\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for test_inputs, test_graph_input, _ in test_loader:\n",
    "            batch_predictions = self(test_inputs, test_graph_input)\n",
    "            predictions.append(batch_predictions)\n",
    "        predictions = torch.cat(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110decbd-cfe6-4c03-934f-eab8843b749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000046_XSHE', '000753_XSHE', '000951_XSHE', '000998_XSHE', '002282_XSHE', '002679_XSHE', '002841_XSHE', '002882_XSHE', '300133_XSHE', '300174_XSHE', '300263_XSHE', '300343_XSHE', '300433_XSHE', '300540_XSHE', '600622_XSHG', '603053_XSHG', '603095_XSHG', '603359_XSHG']\n",
      ">>>>>>>>>>>>>>>>>>>>000046_XSHE>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch [20/1000], Train Loss: 266736673042.0192, Val Loss: 444070147611.1388\n",
      "Epoch [40/1000], Train Loss: 507949334700.8182, Val Loss: 419730256341.8380\n",
      "Epoch [60/1000], Train Loss: 403920526874.5596, Val Loss: 433364484474.1827\n",
      "Epoch [80/1000], Train Loss: 669508210577.6982, Val Loss: 426991969533.9072\n",
      "Epoch [100/1000], Train Loss: 858474536012.1770, Val Loss: 423602734768.7367\n",
      "Epoch [120/1000], Train Loss: 506952470481.8826, Val Loss: 416958384335.3993\n",
      "Early stopping at epoch 136\n",
      "\n",
      "Fold number: 0\n",
      "[2.10051153e+03 2.07559709e+04 6.97741204e+04 1.11204497e+04\n",
      " 8.47714852e+02 5.91435415e+05 7.46615626e+04 3.40180801e+02\n",
      " 6.60140085e+05 4.59155146e+04 5.01985733e+04 1.50394907e+05\n",
      " 1.32984741e+04 1.48503798e+04 2.20402577e+05 1.80479821e+05\n",
      " 1.30676632e+04 4.05621331e+02 5.81137878e+05 8.04064354e+04\n",
      " 9.98616950e+04 5.40027897e+02 5.98754952e+04 2.66034723e+05\n",
      " 1.12547714e+03 5.89844473e+03 5.11233660e+03 4.77003669e+05\n",
      " 3.18766354e+03 7.50638497e+02 1.60735863e+04 8.85237031e+03\n",
      " 3.54571229e+05 3.45938415e+05 4.13599302e+04 4.19794591e+05\n",
      " 4.70233267e+04 3.07555369e+05 5.41236287e+05 6.67754750e+04\n",
      " 8.11568373e+03 1.99275066e+05 2.55230298e+05 5.69549975e+04\n",
      " 3.94542797e+02 1.06461421e+03 4.54675399e+02 2.50799134e+04\n",
      " 1.85025357e+03 5.33818397e+04 4.94362438e+03 2.64589650e+04\n",
      " 8.04863199e+05 1.16195816e+04 1.26935311e+04 1.92012245e+04\n",
      " 4.44119811e+04 1.00956003e+05 1.46342143e+04 6.06188227e+04\n",
      " 1.69471247e+05 2.96473964e+05 2.90623163e+05 3.65919449e+04\n",
      " 3.55279894e+05 2.82489497e+02 2.10395074e+06 4.87649907e+04\n",
      " 9.53570584e+04 2.29131178e+04 9.95672958e+04 1.66956589e+05\n",
      " 3.08292079e+05 2.08823042e+05 6.35027713e+04 1.04438900e+05\n",
      " 6.04000362e+02 1.73644755e+05 4.97565577e+04 1.01986207e+05\n",
      " 5.01041988e+05 1.79906404e+04 1.09052682e+03 2.89975531e+05\n",
      " 2.97922374e+04 3.30819402e+05 2.90361769e+05 1.12186294e+04\n",
      " 1.18182490e+03 5.25155442e+04 4.65161604e+02 6.00061781e+02\n",
      " 1.13678696e+04 4.96394909e+04 6.26394528e+03 7.03460117e+04\n",
      " 4.84573186e+04 2.77611424e+04 1.71256368e+04 2.62871697e+03\n",
      " 7.50586348e+05 2.26310426e+04 7.68312189e+05 2.28233556e+05\n",
      " 9.38859716e+04 1.38299198e+05 1.25899956e+05 3.82116371e+05\n",
      " 3.62172248e+04 3.46155454e+05 2.90959150e+03 8.46751938e+05\n",
      " 2.98193510e+04 2.70758044e+03 2.14650225e+05 3.29537707e+04\n",
      " 5.06624421e+04 6.48424944e+05 2.94664903e+04 9.91385786e+05\n",
      " 3.36185179e+05 1.88347462e+04 1.37455236e+05 3.11055192e+05\n",
      " 3.36753145e+04 3.83776347e+05 1.92150809e+05 2.00871228e+04\n",
      " 1.73570795e+04 1.16261613e+04 5.58586740e+05 3.84069425e+04\n",
      " 8.20811598e+04 6.13766810e+04 8.34505452e+05 2.88958098e+02\n",
      " 2.38194563e+04 2.34336244e+06 7.47536939e+05 1.64686957e+04\n",
      " 2.60157986e+05 2.76970063e+03 2.88912205e+04 1.67871771e+03\n",
      " 4.86595830e+04 4.95202714e+04 3.62212536e+04 3.39386451e+05\n",
      " 2.37982503e+05 6.31640219e+04 1.59257681e+05 1.77049211e+05\n",
      " 6.32654830e+04 8.43248133e+04 3.97334524e+04 5.87900164e+02\n",
      " 5.18600068e+02 5.36280781e+04 1.81817891e+05 8.39671971e+03\n",
      " 2.52326319e+04 3.17347801e+04 1.49959747e+04 3.84117024e+04\n",
      " 6.51606319e+04 1.21146280e+05 1.75901989e+05 1.21242752e+05\n",
      " 5.35539911e+02 1.09607276e+06 3.73948653e+04 6.80130377e+03\n",
      " 1.97918482e+05 1.76783443e+05 3.96814963e+02 1.01471752e+04\n",
      " 7.61691071e+04 6.51912804e+04 1.38047028e+05 1.97781654e+06\n",
      " 9.07750561e+04 4.25384981e+05 4.13059281e+05 1.60866179e+05\n",
      " 3.35637022e+04 1.91402590e+05 2.24656870e+05 2.69146544e+04\n",
      " 1.43018458e+05 2.10836932e+04 3.07740607e+05 1.68696723e+05\n",
      " 4.48525145e+05 2.22203730e+03 2.32496576e+04 5.89490530e+04\n",
      " 4.99505550e+03 8.00818049e+03 2.24615156e+05 3.50935832e+03\n",
      " 4.73227479e+04 2.20239856e+03 5.03239319e+03 2.28901610e+05\n",
      " 3.56263900e+05 3.83188395e+04 8.72211323e+05 4.52577278e+05\n",
      " 1.65275496e+03 4.22946529e+04 1.23496467e+03 7.45710757e+05\n",
      " 1.41621893e+05 1.84952217e+05 1.77804993e+04 2.18888776e+02\n",
      " 2.69015608e+04 9.79185492e+04 5.97605766e+03 9.10996660e+03\n",
      " 3.70214726e+04 1.63866907e+05 3.24493044e+05 2.66584205e+05\n",
      " 4.38987193e+04 3.05350585e+04 7.25915117e+04 4.79348684e+03\n",
      " 7.32941158e+05 1.16945919e+04 2.60426029e+02 3.01079740e+04\n",
      " 7.70591477e+04 1.34173443e+05 4.08293001e+05 4.91449477e+05\n",
      " 1.04988886e+05 2.56867376e+03 7.85209826e+02 6.12625588e+02\n",
      " 1.81194942e+04 6.61835241e+04 1.50511507e+05 2.78248097e+05\n",
      " 8.02589582e+03 2.77055936e+04 3.89618809e+05 5.06888893e+04\n",
      " 5.57290610e+04 1.57978688e+05 5.60226539e+03 2.10680943e+04\n",
      " 1.43921875e+05 3.12038082e+02 1.49676615e+05 1.48641187e+05\n",
      " 8.10341119e+05 2.63257654e+05 1.38643971e+05 2.38884645e+02\n",
      " 1.20624015e+04 8.10458498e+02 2.93733084e+04 2.67180530e+05\n",
      " 5.62016282e+04 5.83554302e+04 2.81886801e+05 7.00768400e+02\n",
      " 3.60510332e+02 1.53393510e+04 4.79295211e+02 3.30294281e+04\n",
      " 3.81361717e+04 1.91990163e+05 5.29330337e+04 4.31669399e+04\n",
      " 2.05184206e+05 6.25762374e+03 2.53678118e+05 3.09791240e+04\n",
      " 2.33103770e+04 2.22153274e+03 1.57754681e+05 3.51842448e+04\n",
      " 1.56532283e+03 2.05709296e+05 5.03116287e+02 1.14098542e+04\n",
      " 1.75380475e+03 5.75091850e+02 1.74520636e+04 3.48794573e+04\n",
      " 1.63186252e+05 1.82162784e+04 2.44965434e+04 4.96295478e+05\n",
      " 1.06268521e+05 5.60035731e+05 4.13240915e+04 6.78567129e+03\n",
      " 4.44681670e+02 5.51842383e+04 2.67694064e+05 7.23890642e+02\n",
      " 1.40282778e+05 4.39687870e+04 6.58746064e+05 3.75679511e+04\n",
      " 2.71949935e+05 7.31220956e+04 2.12779596e+04 2.57003594e+05\n",
      " 1.13235400e+05 5.31802699e+05 1.40075793e+05 2.04504617e+04\n",
      " 3.98269995e+05 3.37853131e+04 1.66227532e+05 2.99657078e+04\n",
      " 3.75101139e+04 1.19981504e+03 1.53167133e+04 6.27520963e+04\n",
      " 3.68584716e+04 1.11703262e+03 2.37125624e+02 6.94536703e+05\n",
      " 9.17193748e+03 4.70744412e+05 6.15701419e+05 3.80099787e+04\n",
      " 3.79185803e+04 2.44314599e+04 5.88856770e+04 2.98729077e+04\n",
      " 1.62482930e+05 4.59840116e+02 4.71546837e+02 5.44269057e+04\n",
      " 4.45253202e+05 9.12384162e+03 8.31009018e+03 5.78310359e+05\n",
      " 3.56037034e+02 5.90878846e+02 3.11615840e+05 2.22830962e+06\n",
      " 2.46341342e+05 4.97304592e+05 5.29178339e+04 2.54185762e+04\n",
      " 3.20044301e+04 5.72551512e+04 1.04335633e+05 4.56695434e+05\n",
      " 1.63683442e+03 1.04894343e+03 4.27445506e+03 2.09043727e+05]\n",
      "[0.8154289757470035]\n",
      "MAPE =  0.8154289757470035\n",
      "Epoch [20/1000], Train Loss: 133886799969.8804, Val Loss: 1332009056072.8003\n",
      "Epoch [40/1000], Train Loss: 917895976374.0542, Val Loss: 1267422291686.7708\n",
      "Epoch [60/1000], Train Loss: 206614327894.7283, Val Loss: 1424050748444.5911\n",
      "Epoch [80/1000], Train Loss: 790885479429.0118, Val Loss: 1382719372210.8708\n",
      "Epoch [100/1000], Train Loss: 647546056734.9351, Val Loss: 1372173461378.9380\n",
      "Epoch [120/1000], Train Loss: 386750483181.9342, Val Loss: 1358631699811.0840\n",
      "Epoch [140/1000], Train Loss: 1151579566110.7158, Val Loss: 1357092134028.9756\n",
      "Early stopping at epoch 156\n",
      "\n",
      "Fold number: 1\n",
      "[2.09140046e+03 2.03620325e+04 6.72631976e+04 1.14509494e+04\n",
      " 8.33718912e+02 5.81075038e+05 5.68955598e+04 3.54382534e+02\n",
      " 6.72503320e+05 4.73945559e+04 5.18804947e+04 1.54672637e+05\n",
      " 1.36884800e+04 1.72184884e+05 2.31926034e+05 1.76872042e+05\n",
      " 1.34829335e+04 3.97889618e+02 6.05517680e+05 7.84318388e+04\n",
      " 9.97994868e+04 5.27217586e+02 2.17253387e+05 2.71904653e+05\n",
      " 1.12024640e+03 5.88789560e+03 5.53184774e+03 4.63095333e+05\n",
      " 3.29921164e+03 7.86558357e+02 1.64613131e+04 9.14138481e+03\n",
      " 3.62710066e+05 3.31864186e+05 4.27968228e+04 4.30021953e+05\n",
      " 4.67295474e+04 5.57347927e+05 5.21694994e+05 6.64220991e+04\n",
      " 8.41567052e+03 2.05751764e+05 3.19136194e+05 5.55148225e+04\n",
      " 4.06366348e+02 1.10769670e+03 4.78047715e+02 2.54065022e+04\n",
      " 1.85136374e+03 5.54389946e+04 4.99718932e+03 1.01311174e+04\n",
      " 8.25479933e+05 1.21449124e+04 1.31052843e+04 1.87415123e+04\n",
      " 2.94228824e+05 1.00116773e+05 1.51907844e+04 6.08286630e+04\n",
      " 1.56998158e+05 3.03817692e+05 2.88216212e+05 3.57867267e+04\n",
      " 3.58131692e+05 2.91842733e+02 2.17276842e+06 4.73084849e+04\n",
      " 9.47917334e+04 2.35644237e+04 1.04465250e+05 1.72655953e+05\n",
      " 4.39392128e+04 2.18410057e+05 6.43334736e+04 1.07143933e+05\n",
      " 6.28886093e+02 2.92236922e+05 4.86166184e+04 1.06426845e+05\n",
      " 4.98176708e+05 1.87436464e+04 1.66766597e+05 2.98088696e+05\n",
      " 2.02336851e+04 3.40542197e+05 2.88586730e+05 7.13907690e+03\n",
      " 1.22852126e+03 5.49478193e+04 4.80167029e+02 6.17086672e+02\n",
      " 1.17535610e+04 5.06984634e+02 6.32416407e+03 7.33331607e+04\n",
      " 4.79604077e+04 2.83646394e+04 1.77374561e+04 3.93744379e+04\n",
      " 7.68426348e+05 2.37711466e+04 7.89021931e+05 2.93150990e+05\n",
      " 9.33612193e+04 1.41824376e+05 1.31623149e+05 1.97845554e+05\n",
      " 3.71130737e+04 3.58221774e+05 6.11529298e+04 8.79238937e+05\n",
      " 3.13378049e+04 2.83527644e+03 1.36844088e+05 3.27876664e+04\n",
      " 3.72713699e+05 6.74881472e+05 3.59482262e+04 1.02168044e+06\n",
      " 3.47562461e+05 1.94870175e+04 1.41471838e+05 3.00070329e+05\n",
      " 1.79547453e+05 3.77508882e+05 1.98605681e+05 2.29010459e+05\n",
      " 1.79469894e+04 1.19902378e+04 5.75449985e+05 3.94372154e+04\n",
      " 8.61030720e+04 6.36558563e+04 4.52860746e+05 2.97664795e+02\n",
      " 1.57193454e+03 2.40423779e+06 7.21108616e+05 1.66950365e+04\n",
      " 2.72718720e+05 2.72064732e+03 2.85464007e+04 1.73540109e+03\n",
      " 4.77309770e+04 4.80516977e+04 3.75617873e+04 3.39052579e+05\n",
      " 2.45511817e+05 6.39480525e+04 1.62822664e+05 1.77377763e+05\n",
      " 6.47083844e+04 8.76706499e+04 2.00980256e+04 6.19905224e+02\n",
      " 5.35926911e+02 4.47187846e+04 1.87017989e+05 8.67432210e+03\n",
      " 1.70022384e+04 3.30172633e+04 1.57098542e+04 3.90216415e+04\n",
      " 6.78324028e+04 1.17814382e+05 1.73828224e+05 1.25096260e+05\n",
      " 5.54344434e+02 1.06256479e+06 2.93472986e+04 6.66255113e+03\n",
      " 2.02377375e+05 1.75884396e+05 4.17893466e+02 1.09161042e+04\n",
      " 8.03063327e+04 6.83049870e+04 1.42783795e+05 2.03065602e+06\n",
      " 8.85102996e+04 4.39903454e+05 4.22555705e+05 1.64524523e+05\n",
      " 3.33569967e+04 1.96382818e+05 2.31077234e+05 2.80656334e+04\n",
      " 7.98962677e+03 2.03780193e+04 3.14860412e+05 1.66662823e+05\n",
      " 4.34663882e+05 2.25661737e+03 2.25091073e+04 6.06447360e+04\n",
      " 5.12259627e+03 8.36300095e+03 2.32894691e+05 3.68223489e+03\n",
      " 4.81990217e+04 2.27212586e+03 4.98943025e+03 2.40131368e+05\n",
      " 3.45974662e+05 3.97731189e+04 9.09944602e+05 4.70783852e+05\n",
      " 1.71849468e+03 4.12305426e+04 1.26298592e+03 7.63733105e+05\n",
      " 1.39956117e+05 1.90018568e+05 1.84244522e+04 2.26273014e+02\n",
      " 2.78496486e+04 9.79085831e+04 6.18046533e+03 9.38691029e+03\n",
      " 3.63522357e+04 1.63256103e+05 3.39786716e+05 3.66443237e+04\n",
      " 4.25739134e+04 1.49092796e+05 7.58594592e+04 4.98130854e+03\n",
      " 7.40079013e+05 1.20402094e+04 2.69127504e+02 3.05867876e+04\n",
      " 8.05174489e+04 1.40159008e+05 1.55921606e+05 5.05508004e+05\n",
      " 1.03277856e+05 2.62736331e+03 8.13212269e+02 6.05084396e+02\n",
      " 2.76650483e+05 6.93021466e+04 1.55165195e+05 2.79648836e+05\n",
      " 8.19892456e+03 1.73490399e+03 4.08266448e+05 2.83761718e+05\n",
      " 5.75972704e+04 1.52850551e+05 5.79994441e+03 2.13346597e+04\n",
      " 1.45215266e+05 3.24685319e+02 1.47904140e+05 1.53456033e+05\n",
      " 8.11056535e+05 1.22710886e+05 1.43270446e+05 2.52055601e+02\n",
      " 1.24682352e+04 8.40399354e+02 1.08357021e+04 2.59633854e+05\n",
      " 5.79392350e+04 3.94837012e+05 2.76185769e+05 7.37021785e+02\n",
      " 3.71579657e+02 1.56519926e+04 4.95977361e+02 3.45301153e+04\n",
      " 3.93329826e+04 1.87696660e+05 5.52020145e+04 2.15870723e+04\n",
      " 2.11671624e+05 6.51674335e+03 2.48226250e+05 3.21327119e+04\n",
      " 2.29729040e+04 2.29736393e+03 1.56496812e+05 3.53911083e+04\n",
      " 1.53239978e+03 1.33047943e+05 5.21960616e+02 1.19954373e+04\n",
      " 1.71648308e+03 6.06365624e+02 1.71017120e+04 3.44321770e+04\n",
      " 1.69906345e+05 1.84330803e+04 2.53403857e+04 5.18140218e+05\n",
      " 1.10214823e+05 5.74823999e+05 6.20346633e+04 7.07922075e+03\n",
      " 4.67652462e+02 3.23341083e+04 2.68525063e+05 7.49075167e+02\n",
      " 1.37035264e+05 4.44796686e+04 6.75015562e+05 3.92804071e+04\n",
      " 2.83714844e+05 7.24408733e+04 2.08955740e+04 2.64194879e+05\n",
      " 1.16028439e+05 5.49800354e+05 1.40190911e+05 2.03536208e+04\n",
      " 8.76822748e+03 3.25353627e+04 1.72157664e+05 3.10471216e+04\n",
      " 3.71751953e+04 1.25549737e+03 1.51299213e+04 6.19566346e+04\n",
      " 3.62490543e+04 1.15388181e+03 2.33132178e+02 6.80810080e+05\n",
      " 9.09329869e+03 4.80236536e+05 6.28592890e+05 3.96459852e+04\n",
      " 1.51084903e+04 1.93755265e+04 6.14952726e+04 3.00196196e+04\n",
      " 1.69727688e+05 4.76313718e+02 4.88101345e+02 5.31419369e+04\n",
      " 4.37833616e+05 9.44142046e+03 8.69981898e+03 5.97680191e+05\n",
      " 2.11283998e+04 5.76392823e+02 2.95593464e+05 2.30908625e+06\n",
      " 2.54358718e+05 4.85773501e+05 5.29418047e+04 2.64731945e+04\n",
      " 3.31473808e+04 6.00437307e+04 1.08699306e+05 4.73711183e+05\n",
      " 8.34295618e+04 1.07815613e+03 2.12538815e+05 2.16243847e+05]\n",
      "[0.796677257054114]\n",
      "MAPE =  0.796677257054114\n",
      "Epoch [20/1000], Train Loss: 209458464412.6108, Val Loss: 315695212684.0118\n",
      "Epoch [40/1000], Train Loss: 964437370255.9961, Val Loss: 293598435896.9260\n",
      "Epoch [60/1000], Train Loss: 203922869921.4112, Val Loss: 283240681819.0677\n",
      "Epoch [80/1000], Train Loss: 2590824559571.3306, Val Loss: 283345572535.1950\n",
      "Epoch [100/1000], Train Loss: 736064126368.2554, Val Loss: 262240839488.4088\n",
      "Epoch [120/1000], Train Loss: 352458213914.7231, Val Loss: 261765318637.7933\n",
      "Epoch [140/1000], Train Loss: 1367066008692.1038, Val Loss: 260280475920.5774\n",
      "Epoch [160/1000], Train Loss: 708229761297.4139, Val Loss: 262274490831.3792\n",
      "Epoch [180/1000], Train Loss: 340843408344.5350, Val Loss: 262670117879.8914\n",
      "Epoch [200/1000], Train Loss: 627409548796.7977, Val Loss: 261760990557.9779\n",
      "Epoch [220/1000], Train Loss: 514363207541.9922, Val Loss: 261574517073.4622\n",
      "Epoch [240/1000], Train Loss: 381697016420.2838, Val Loss: 261427837225.9060\n",
      "Epoch [260/1000], Train Loss: 754230178076.7057, Val Loss: 246653162103.6564\n",
      "Epoch [280/1000], Train Loss: 432238694860.9283, Val Loss: 251288146650.4564\n",
      "Epoch [300/1000], Train Loss: 154779337766.1994, Val Loss: 250261224839.2771\n",
      "Epoch [320/1000], Train Loss: 462404371398.7524, Val Loss: 250800233167.5208\n",
      "Epoch [340/1000], Train Loss: 201916928414.9867, Val Loss: 235551819219.6802\n",
      "Epoch [360/1000], Train Loss: 217685088775.3224, Val Loss: 236753877417.2598\n",
      "Epoch [380/1000], Train Loss: 237930275847.6792, Val Loss: 237105177599.0346\n",
      "Epoch [400/1000], Train Loss: 755821797561.6942, Val Loss: 236873800485.7599\n",
      "Epoch [420/1000], Train Loss: 440996434742.2188, Val Loss: 240536470340.8208\n",
      "Epoch [440/1000], Train Loss: 434406650435.4400, Val Loss: 238944237310.9085\n",
      "Early stopping at epoch 444\n",
      "\n",
      "Fold number: 2\n",
      "[2.15441026e+03 2.02840502e+04 5.41101976e+04 1.08932274e+04\n",
      " 8.57502379e+02 5.92097721e+05 4.43795194e+03 3.63407009e+02\n",
      " 5.03655237e+05 4.85562199e+04 1.76602460e+03 1.43328101e+05\n",
      " 1.30117823e+04 1.72642213e+05 2.35539905e+05 1.77510039e+05\n",
      " 1.27931790e+04 3.96873481e+02 2.70677829e+04 7.66838438e+04\n",
      " 6.56695690e+05 5.17340233e+02 5.84013106e+04 2.40793177e+05\n",
      " 1.15272070e+03 6.05977961e+03 5.70636273e+03 2.49942010e+05\n",
      " 3.41937515e+03 7.72295271e+02 2.34945370e+04 3.05078632e+05\n",
      " 3.31725003e+05 3.08210815e+05 4.39952283e+04 4.65756721e+05\n",
      " 4.69741661e+04 5.76082480e+05 4.80599890e+05 6.72827550e+04\n",
      " 9.95741422e+04 2.11719220e+05 3.30398584e+05 2.23876533e+04\n",
      " 3.84325267e+02 1.08070726e+03 4.81126800e+02 2.59514577e+04\n",
      " 1.90933868e+03 4.54196950e+05 5.15690115e+03 1.04425087e+04\n",
      " 4.49667211e+05 1.20897485e+04 1.29540882e+04 2.78675026e+05\n",
      " 2.97526952e+05 1.01259007e+05 1.45475549e+04 6.22934376e+04\n",
      " 1.56595236e+05 1.85327777e+05 2.90925373e+05 3.54081381e+04\n",
      " 3.94201174e+05 2.99609119e+02 2.14657324e+06 3.38187062e+04\n",
      " 1.25761311e+05 9.39268958e+02 1.03106464e+05 1.89053547e+05\n",
      " 3.45018779e+05 8.05494678e+04 6.63981033e+04 2.39637246e+05\n",
      " 6.03518953e+02 1.68760017e+05 1.08413886e+05 8.48952031e+03\n",
      " 4.96699591e+05 1.84840989e+04 1.16496986e+03 3.02532986e+05\n",
      " 1.90641108e+04 3.42392904e+05 2.90005692e+05 2.84947179e+04\n",
      " 1.16788443e+03 6.26867437e+05 4.93916215e+02 5.80735689e+02\n",
      " 1.21034544e+04 5.24021231e+02 6.51716880e+03 2.91218265e+05\n",
      " 8.28111569e+03 2.55281650e+04 1.83312514e+04 2.89637179e+04\n",
      " 7.63211811e+05 2.36036795e+04 7.34710487e+05 3.01858449e+05\n",
      " 8.67588554e+04 1.36905187e+05 1.24786896e+05 1.94439176e+05\n",
      " 3.48898532e+04 7.67413099e+05 5.89836303e+04 3.03868410e+05\n",
      " 3.57463090e+04 2.91674637e+03 1.41293427e+05 3.27631048e+04\n",
      " 3.84573472e+05 6.20994650e+05 3.39506661e+04 1.05951682e+06\n",
      " 4.49439185e+05 9.39489904e+04 1.44045700e+05 3.06825458e+05\n",
      " 1.82183065e+05 3.64822003e+05 2.03999054e+05 2.36578120e+05\n",
      " 4.89293338e+04 1.24268320e+04 7.43629958e+05 8.86654336e+04\n",
      " 1.00385164e+05 1.95367270e+05 4.39651105e+05 2.77130770e+02\n",
      " 1.62210920e+03 1.90485021e+06 3.50031663e+05 9.25703538e+03\n",
      " 2.67294744e+05 2.79599814e+03 2.91382785e+04 8.32897963e+05\n",
      " 5.83196972e+03 4.61302582e+04 1.54144450e+05 6.77666906e+05\n",
      " 2.33894699e+05 6.58476806e+04 1.47730079e+05 1.82803190e+05\n",
      " 6.66549981e+04 1.51399963e+05 1.25731001e+05 6.30775856e+02\n",
      " 5.51111269e+02 4.22695598e+05 2.52053639e+02 4.38004532e+05\n",
      " 1.73451146e+04 3.17431147e+04 2.49038335e+04 4.04000867e+04\n",
      " 6.86883880e+04 8.96577344e+04 1.95714004e+04 2.36682076e+04\n",
      " 5.74116491e+02 1.23608151e+05 3.01913749e+04 6.61860697e+03\n",
      " 1.80268067e+05 4.16619169e+04 4.14091139e+02 1.08005693e+04\n",
      " 8.05821018e+04 7.95181883e+04 1.03448961e+04 1.82885374e+06\n",
      " 5.67973198e+03 4.52297839e+05 3.92122698e+05 1.50893821e+05\n",
      " 8.61383784e+03 1.92806767e+05 2.38913523e+05 2.35348854e+04\n",
      " 8.09042889e+03 1.41879119e+04 2.05336190e+05 1.62439006e+05\n",
      " 1.00975107e+06 2.31572395e+03 2.16551358e+04 4.82606048e+04\n",
      " 5.27097428e+03 8.24000332e+03 1.76359577e+05 3.77184149e+03\n",
      " 4.96998627e+04 2.34965858e+03 5.13397278e+03 2.40574320e+05\n",
      " 3.16427149e+05 2.56334037e+05 8.62973186e+05 4.85292914e+05\n",
      " 1.64760841e+03 2.31681658e+05 1.15284136e+03 7.08894812e+05\n",
      " 1.41659061e+05 4.12536188e+04 1.00937440e+04 2.32226361e+02\n",
      " 2.87727467e+04 9.69903470e+04 8.86883508e+02 8.73213833e+03\n",
      " 3.57044193e+04 1.67852850e+05 5.75105335e+05 3.74181272e+04\n",
      " 3.52819684e+04 1.52747172e+05 6.10611698e+04 8.24330141e+03\n",
      " 7.59309646e+05 1.14210484e+04 2.75662725e+02 3.12177901e+04\n",
      " 9.48801644e+04 1.33998907e+05 2.85909036e+05 4.82193017e+05\n",
      " 1.00217453e+05 2.42325577e+03 8.43065673e+02 6.21283263e+02\n",
      " 2.67311907e+05 5.20430718e+04 1.45023666e+05 7.73849700e+05\n",
      " 7.51172810e+03 1.00459394e+04 4.18969678e+05 2.68984523e+05\n",
      " 7.76899397e+04 1.90680675e+04 5.99639972e+03 3.98705783e+05\n",
      " 1.27697391e+05 3.11377445e+02 1.47470679e+05 1.52864873e+05\n",
      " 4.00915936e+05 1.26438937e+05 1.54127834e+05 1.68390441e+05\n",
      " 1.28644653e+04 7.98697249e+02 1.08404135e+04 2.49759806e+05\n",
      " 5.11985558e+04 3.97547730e+05 2.67592774e+05 7.38800174e+02\n",
      " 3.48607667e+02 1.36048064e+04 5.08680792e+02 2.47485636e+04\n",
      " 4.94861640e+05 1.81845370e+05 5.42852948e+04 3.28300021e+04\n",
      " 1.83328761e+05 1.76187231e+05 2.44685743e+05 2.06315927e+04\n",
      " 2.31867713e+04 4.06186439e+05 1.54427685e+05 2.76050190e+05\n",
      " 8.76875491e+02 1.33720001e+05 4.96917033e+02 1.21085202e+04\n",
      " 1.69769304e+03 6.15207295e+02 1.70059930e+04 3.46538550e+04\n",
      " 8.49123960e+05 4.92064107e+04 5.54127011e+02 2.39228248e+05\n",
      " 4.73678050e+03 3.54394338e+05 4.05516945e+04 1.07446091e+03\n",
      " 4.71677926e+02 3.06957113e+04 1.27022785e+06 7.76218719e+02\n",
      " 2.40530113e+04 4.60198601e+04 6.26694047e+05 1.57113151e+05\n",
      " 2.92600749e+05 2.31318771e+03 2.52532393e+05 2.60155728e+04\n",
      " 1.91327784e+05 5.27685861e+05 2.70894443e+05 2.10256865e+04\n",
      " 8.27941587e+03 2.51703297e+04 1.68664395e+05 2.43518967e+04\n",
      " 3.63193465e+04 1.25705915e+03 3.26791594e+04 6.10694816e+04\n",
      " 7.90474428e+04 1.18936570e+03 2.38443336e+02 6.78018148e+05\n",
      " 9.39224914e+03 3.62753352e+04 5.44393037e+05 3.96228438e+04\n",
      " 1.52519776e+04 1.67515251e+03 6.06370824e+04 1.53867611e+04\n",
      " 1.00479566e+04 4.47081114e+02 4.58924880e+02 4.51855910e+04\n",
      " 4.47643807e+05 9.74841605e+03 1.57534407e+05 6.19534523e+05\n",
      " 2.14701851e+04 5.65917637e+02 2.96588361e+05 2.20259810e+06\n",
      " 1.65627289e+05 1.76304380e+05 5.44600893e+04 1.01278170e+05\n",
      " 3.42679961e+04 5.79091229e+04 1.23614884e+05 3.19612643e+05\n",
      " 9.84286184e+04 1.01659062e+03 2.18022620e+05 1.92299830e+04]\n",
      "[0.808936270581803]\n",
      "MAPE =  0.808936270581803\n",
      "Epoch [20/1000], Train Loss: 285427994687.6373, Val Loss: 320765285113.5222\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your PyTorch model is defined as before\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # stock_info = sys.argv[0]\n",
    "    # lag_bin = int(sys.argv[1])\n",
    "    # lag_day = int(sys.argv[2])\n",
    "    # bin_num = int(sys.argv[3])\n",
    "    # random_state_here = int(sys.argv[4])\n",
    "    # test_set_size = float(sys.argv[5])\n",
    "    lag_bin = 3\n",
    "    lag_day = 3\n",
    "    num_nodes = (int(lag_bin)+1)*(int(lag_day)+1)\n",
    "    forecast_days = 15\n",
    "    bin_num=24\n",
    "    random_state_here = 88\n",
    "    mape_list = []\n",
    "    data_dir = './data/volume/0308/'\n",
    "    files =file_name('./data/')\n",
    "    stocks_info = sorted(list(set(s.split('_25')[0] for s in files)))\n",
    "    print(stocks_info)\n",
    "    for stock_info in stocks_info[0:1]:\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>{stock_info}>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        data_dir1 = f'{data_dir}{stock_info}_{lag_bin}_{lag_day}'\n",
    "        test_set_size = bin_num*forecast_days\n",
    "        K = 5\n",
    "        inputs_data = np.load(f'{data_dir1}_inputs.npy', allow_pickle=True)\n",
    "        inputs_data = [[[torch.tensor(x, dtype=torch.float64) for x in sublist] for sublist in list1] for list1 in inputs_data]\n",
    "        array_data = np.array(inputs_data)\n",
    "        inputs = np.reshape(array_data, (len(inputs_data), num_nodes,-1))\n",
    "        targets = np.load(f'{data_dir1}_output.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.load(f'{data_dir1}_graph_input.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_input = np.array([graph_input] * inputs.shape[0])\n",
    "        graph_features = np.load(f'{data_dir1}_graph_coords.npy', allow_pickle=True).astype(np.float64)\n",
    "        graph_features = np.array([graph_features] * inputs.shape[0])\n",
    "\n",
    "        trainInputs, testInputs, traingraphInput, testgraphInput, traingraphFeature, testgraphFeature, trainTargets, testTargets = train_test_split(inputs, graph_input, graph_features, targets, test_size=test_set_size, \n",
    "                                                     random_state=random_state_here)\n",
    "        testInputs = normalize(testInputs)\n",
    "        # testInputs = test_inputs\n",
    "        inputsK, targetsK = k_fold_split(trainInputs, trainTargets, K)\n",
    "\n",
    "        mape_list = []\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(testInputs), torch.tensor(testgraphInput), torch.tensor(testTargets))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "        K = 5  # Number of folds\n",
    "        for k in range(K):\n",
    "            torch.manual_seed(0)  # Set a random seed for reproducibility\n",
    "\n",
    "            trainInputsAll, trainTargets, valInputsAll, valTargets = merge_splits(inputsK, targetsK, k, K)\n",
    "\n",
    "            trainGraphInput = traingraphInput[0:trainInputsAll.shape[0], :]\n",
    "            trainGraphFeatureInput = traingraphFeature[0:trainInputsAll.shape[0], :]\n",
    "\n",
    "            valGraphInput = traingraphInput[0:valInputsAll.shape[0], :]\n",
    "            valGraphFeatureInput = traingraphFeature[0:valInputsAll.shape[0], :]\n",
    "\n",
    "            trainInputs = normalize(trainInputsAll[:, :])\n",
    "            valInputs = normalize(valInputsAll[:, :])\n",
    "\n",
    "            # Assuming trainInputs, trainGraphInput, trainGraphFeatureInput, trainTargets are PyTorch tensors\n",
    "            train_dataset = TensorDataset(torch.tensor(trainInputs), torch.tensor(trainGraphInput),torch.tensor(trainTargets))\n",
    "            val_dataset = TensorDataset(torch.tensor(valInputs), torch.tensor(valGraphInput),torch.tensor(valTargets))\n",
    "\n",
    "            # train_dataset = TensorDataset(torch.tensor(trainInputs, dtype=torch.float32), torch.tensor(trainGraphInput, dtype=torch.float32), torch.tensor(trainGraphFeatureInput, dtype=torch.float32), torch.tensor(trainTargets, dtype=torch.float32))\n",
    "            # val_dataset = TensorDataset(torch.tensor(valInputs, dtype=torch.float32), torch.tensor(valGraphInput, dtype=torch.float32), torch.tensor(valGraphFeatureInput, dtype=torch.float32), torch.tensor(valTargets, dtype=torch.float32))\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "        \n",
    "            model = GATModel(7,8,1,0.3,0.2,3)\n",
    "            model.fit(train_loader, val_loader)\n",
    "            predictions=model.test(test_loader)\n",
    "            torch.save(model.state_dict(), f'models/gat_{stock_info}_{lag_bin}_{lag_day}_gcn_model_iteration_{k}.pt')\n",
    "    \n",
    "            print()\n",
    "            print('Fold number:', k)\n",
    "\n",
    "            new_predictions = np.array([item.detach().numpy() for item in predictions]).flatten()\n",
    "            MAPE = []\n",
    "\n",
    "            MAPE.append(mean_absolute_percentage_error(testTargets[:], new_predictions[:]))\n",
    "            print(new_predictions)\n",
    "            print(MAPE)\n",
    "            testTargets0 = list(testTargets)\n",
    "\n",
    "            res = {\n",
    "                'testTargets': testTargets0,\n",
    "                'new_predictions': new_predictions\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame(res)\n",
    "            res_df.to_csv(f'./result/gat_{stock_info}_{lag_bin}_{lag_day}_res_test_MAPE{k}.csv', index=False)\n",
    "\n",
    "            print('MAPE = ', np.array(MAPE).mean())\n",
    "            MAPE_mean = np.array(MAPE).mean()\n",
    "            mape_list.append(MAPE)\n",
    "\n",
    "        print('-')\n",
    "        print('mape score = ', mape_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8eb41-44fb-4c32-ab31-5e5eeef960db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe661f3-83b3-4f01-8f8b-d7bb51f9bd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
